<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Thoughts - Ambarish</title>
    <link>/posts/</link>
    <description>Recent content in Posts on Thoughts - Ambarish</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>© Copyright notice</copyright>
    <lastBuildDate>Fri, 10 Sep 2021 10:00:00 +0030</lastBuildDate><atom:link href="/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Binomial Distributions</title>
      <link>/posts/binomial-distributions/</link>
      <pubDate>Fri, 10 Sep 2021 10:00:00 +0030</pubDate>
      
      <guid>/posts/binomial-distributions/</guid>
      <description>Binomial Probability distribution has the following characteristics:
  A fixed number of trials n. e.g., 20 coin toss , 40 students
  A binary outcome, a success and a failure. Probability of success is p, probability of failure is 1-p. e.g., head or tail in a coin toss , pass or fail of sttudents in an exam, positive and negative results of a test of a patient.
  Constant probability for each trial.</description>
    </item>
    
    <item>
      <title>Probability Distributions</title>
      <link>/posts/probability-distributions/</link>
      <pubDate>Thu, 09 Sep 2021 00:00:00 +0030</pubDate>
      
      <guid>/posts/probability-distributions/</guid>
      <description>Probability distribution is also known as Probability function.
 This gives the probabilities of all possible outcomes A mathematical function which maps each possible outcome x to the probability p(x) The probabilities must all sum or integrate to 1  Discrete and Continous Probability distribution Discrete probability functions can take only discrete values. Examples include Dead/ Alive , numbers obtained by rolling a die, treatment / placebo, whole numbers
Continuous probability functions can take any value within a range.</description>
    </item>
    
    <item>
      <title>Recommendation series in Microsoft Reactor</title>
      <link>/posts/reactor-recommendation-series/</link>
      <pubDate>Mon, 21 Jun 2021 01:00:00 +0030</pubDate>
      
      <guid>/posts/reactor-recommendation-series/</guid>
      <description>We will adopt a case study approach in understanding the concepts and build a recommendation engine from scratch using TF-IDF , cosine similarity, word embeddings and deploy it in Azure ML. The series will span 5 Fridays 5PM - 6PM IST time zone.
  Introduction to Azure ML - Friday, June 25, 2021 
  Recommendation engine using Text data ,Cosine Similarity and TFIDF technique -Friday, July 2, 2021</description>
    </item>
    
    <item>
      <title>Roadmap of the Recommendation series Reading</title>
      <link>/posts/roadmap-recommendation-series/</link>
      <pubDate>Tue, 18 May 2021 01:00:00 +0030</pubDate>
      
      <guid>/posts/roadmap-recommendation-series/</guid>
      <description>A roadmap of reading the Recommendation series involving TF-IDF , Cosine Similarity , Word Embeddings and deploying into AzureML
  Recommendation engine using Text data ,Cosine Similarity and TFIDF technique
  Recommendation engine using Text data ,Cosine Similarity and TFIDF technique , Azure ML
  Word Embeddings
  Recommendation engine using Text data ,Cosine Similarity and word embeddings technique
  Recommendation engine using Text data ,Cosine Similarity and word embeddings technique, , Azure ML</description>
    </item>
    
    <item>
      <title>Word embeddings</title>
      <link>/posts/word-embeddings/</link>
      <pubDate>Tue, 18 May 2021 00:00:00 +0030</pubDate>
      
      <guid>/posts/word-embeddings/</guid>
      <description>From the TensorFlow documentation word embeddings documentation
 Word embeddings give us a way to use an efficient, dense representation in which similar words have a similar encoding.
  Importantly, you do not have to specify this encoding by hand. An embedding is a dense vector of floating point values (the length of the vector is a parameter you specify).
  Instead of specifying the values for the embedding manually, they are trainable parameters (weights learned by the model during training, in the same way a model learns weights for a dense layer).</description>
    </item>
    
    <item>
      <title>Recommendation engine using Text data ,Cosine Similarity and Word Embeddings , Azure ML</title>
      <link>/posts/recommender-career-spacy-azure/</link>
      <pubDate>Sat, 15 May 2021 01:00:00 +0030</pubDate>
      
      <guid>/posts/recommender-career-spacy-azure/</guid>
      <description>What are we trying to do We will build a very simple recommendation engine using Text Data. To demostrate this we would use a case study approach and build a recommendation engine for a non profit organization Career Village. I have detailed post on the methodology of the recommendation engine in the post here. In this post we will show of how we train, infer and deploy the solution in Azure.</description>
    </item>
    
    <item>
      <title>Recommendation engine using Text data ,Cosine Similarity and word embeddings technique</title>
      <link>/posts/recommender-career-spacy/</link>
      <pubDate>Sat, 15 May 2021 00:00:00 +0030</pubDate>
      
      <guid>/posts/recommender-career-spacy/</guid>
      <description>What are we trying to do We will build a very simple recommendation engine using Text Data. To demostrate this we would use a case study approach and build a recommendation engine for a non profit organization Career Village.
CareerVillage.org is a nonprofit that crowdsources career advice for underserved youth. Founded in 2011 in four classrooms in New York City, the platform has now served career advice from 25,000 volunteer professionals to over 3.</description>
    </item>
    
    <item>
      <title>Recommendation engine using Text data ,Cosine Similarity and TFIDF technique , Azure ML</title>
      <link>/posts/recommender-career-tfidf-azure/</link>
      <pubDate>Fri, 14 May 2021 01:00:00 +0030</pubDate>
      
      <guid>/posts/recommender-career-tfidf-azure/</guid>
      <description>What are we trying to do We will build a very simple recommendation engine using Text Data. To demostrate this we would use a case study approach and build a recommendation engine for a non profit organization Career Village. I have detailed post on the methodology of the recommendation engine in the post here. In this post we will show of how we train, infer and deploy the solution in Azure.</description>
    </item>
    
    <item>
      <title>Recommendation engine using Text data ,Cosine Similarity and TFIDF technique</title>
      <link>/posts/recommender-career-tfidf/</link>
      <pubDate>Thu, 13 May 2021 00:00:00 +0030</pubDate>
      
      <guid>/posts/recommender-career-tfidf/</guid>
      <description>What are we trying to do We will build a very simple recommendation engine using Text Data. To demostrate this we would use a case study approach and build a recommendation engine for a non profit organization Career Village.
CareerVillage.org is a nonprofit that crowdsources career advice for underserved youth. Founded in 2011 in four classrooms in New York City, the platform has now served career advice from 25,000 volunteer professionals to over 3.</description>
    </item>
    
    <item>
      <title>Latex  Cheatsheet</title>
      <link>/posts/matrix-helper/</link>
      <pubDate>Tue, 04 May 2021 00:00:00 +0030</pubDate>
      
      <guid>/posts/matrix-helper/</guid>
      <description>Plain Matrix $ \\begin{matrix} 1 &amp;amp; 2 &amp;amp; 3\\\\ a &amp;amp; b &amp;amp; c \\end{matrix} $ $ \begin{matrix} 1 &amp;amp; 2 &amp;amp; 3\\
a &amp;amp; b &amp;amp; c \end{matrix} $
Square Bracket Matrix $ \\begin{bmatrix} 1 &amp;amp; 2 &amp;amp; 3\\\\ a &amp;amp; b &amp;amp; c \\end{bmatrix} $ $ \begin{bmatrix} 1 &amp;amp; 2 &amp;amp; 3\\
a &amp;amp; b &amp;amp; c \end{bmatrix} $
Parentheses;round brackets Matrix $ \\begin{pmatrix} 1 &amp;amp; 2 &amp;amp; 3\\\\ a &amp;amp; b &amp;amp; c \\end{pmatrix} $ $ \begin{pmatrix} 1 &amp;amp; 2 &amp;amp; 3\\</description>
    </item>
    
    <item>
      <title>Convolutions in 1 dimension</title>
      <link>/posts/conv1d-pytorch/</link>
      <pubDate>Mon, 26 Apr 2021 00:00:00 +0030</pubDate>
      
      <guid>/posts/conv1d-pytorch/</guid>
      <description>Basics The Convolutional block is one of the basic building blocks used in deep learning. We go in-depth with Convolution in 1 dimension and understand the basics of convolution, strides, and padding. We explain visually and also through PyTorch code to verify our concepts.
The Kernel takes an Input and provides an output which is sometimes referred to as a feature map
The Kernel is made up of many things .</description>
    </item>
    
    <item>
      <title>Red Wine Quality prediction using AzureML, AKS with TensorFlow Keras</title>
      <link>/posts/red-wine-quality-azureml-keras/</link>
      <pubDate>Sat, 24 Apr 2021 00:00:00 +0030</pubDate>
      
      <guid>/posts/red-wine-quality-azureml-keras/</guid>
      <description>What are we trying to do Predict the Quality of Red Wine using Tensorflow Keras deep learning framework given certain attributes such as fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates, and alcohol
We divide our approach into 2 major blocks:
 Building the Model in Azure ML Inference from the Model in Azure ML  Building the model in Azure ML has the following steps:</description>
    </item>
    
    <item>
      <title>AdaptiveAvgPool2d in PyTorch</title>
      <link>/posts/adaptiveavgpool2d-pytorch/</link>
      <pubDate>Sun, 18 Apr 2021 00:00:00 +0030</pubDate>
      
      <guid>/posts/adaptiveavgpool2d-pytorch/</guid>
      <description>I had trouble understanding the AdaptiveAvgPool2d function in PyTorch. The following examples helped me to teach myself better. Hopefully, somebody may benefit from this.
Example 1 import torch import torch.nn as nn import numpy as np m = nn.AdaptiveAvgPool2d((1)) x = np.array( [ [ 2. , 3.], [ 4. , 1.], ]) input = torch.tensor(x) print(input) output = m(input) print(output) print(torch.mean(input)) The output will be equal to torch.mean(input)
Example 2 with a 3 x 3 x 3 tensor x = np.</description>
    </item>
    
    <item>
      <title>Bees Health detection using Azure Custom Vision Service</title>
      <link>/posts/bees-health-detection-azure-cognitive-services/</link>
      <pubDate>Wed, 31 Mar 2021 00:00:00 +0030</pubDate>
      
      <guid>/posts/bees-health-detection-azure-cognitive-services/</guid>
      <description>Every third bite of food relies on pollination by bees. Honey beehive losses are quite prevalent due to the diseased bees.
While many indications of hive strength and health are visible on the inside of the hive, frequent check-ups on the hive are time-consuming and disruptive to the bees&#39; workflow and hive in general. By investigating the bees that leave the hive, we can gain a more complete understanding of the hive itself.</description>
    </item>
    
    <item>
      <title>Cross Entropy Loss</title>
      <link>/posts/2021-03-27-cross-entropy-loss/</link>
      <pubDate>Sat, 27 Mar 2021 00:00:00 +0030</pubDate>
      
      <guid>/posts/2021-03-27-cross-entropy-loss/</guid>
      <description>Cross Entropy Loss In a supervised learning problem for predicting classes, we predict probabilities for the classes. To determine how successful we are predicting the classes, we require a loss function.
Cross-Entropy loss function provides a loss function to calculate the loss between the actual classes and the predicted probabilities
This can be represented as
 Get the predicted probability of the class Get the actual label of the class ( 1 or 0 ) Multiply the actual label with the log of the predicted probability of the class Sum the values obtained from each of the classes Multiply by -1 the value obtained in Step 4  This is the loss for a single observation.</description>
    </item>
    
    <item>
      <title>Azure ML DataStores and Datasets</title>
      <link>/posts/2021-03-15-azure-ml-datastores-datasets-basics/</link>
      <pubDate>Mon, 15 Mar 2021 00:00:00 +0030</pubDate>
      
      <guid>/posts/2021-03-15-azure-ml-datastores-datasets-basics/</guid>
      <description>DataStores
In Azure ML, datastores are references to storage locations, such as Azure Storage blob containers. Every workspace has a default datastore - usually the Azure storage blob container that was created with the workspace.
When data is uploaded into the datastore through the following code
default_ds.upload_files(files=[&amp;#39;data/diabetes.csv&amp;#39;, &amp;#39;data/diabetes2.csv&amp;#39;], # Upload the diabetes csv files in /data target_path=&amp;#39;diabetes-data/&amp;#39;, # Put it in a folder path in the datastore overwrite=True, # Replace existing files of the same name show_progress=True) we can see the files in the Azure Storage Account &amp;gt; Containers &amp;gt; Blob Stores</description>
    </item>
    
    <item>
      <title>Azure ML Experiments and Runs basics</title>
      <link>/posts/2021-03-13-azure-ml-experiments-runs-basics/</link>
      <pubDate>Sat, 13 Mar 2021 00:00:00 +0030</pubDate>
      
      <guid>/posts/2021-03-13-azure-ml-experiments-runs-basics/</guid>
      <description>An experiment is a grouping of many runs from a specified script. It always belongs to a workspace. When we submit a run, we provide an experiment name. Information for the run is stored under that experiment. If the name doesn&amp;rsquo;t exist when we submit an experiment, a new experiment is automatically created.
  A run is a single execution of a training script. An experiment will typically contain multiple runs.</description>
    </item>
    
    <item>
      <title>2 Stories, Azure ML, Azure Kubernetes Service, Model deployment</title>
      <link>/posts/2021-03-11-two-stories-azureml-aks/</link>
      <pubDate>Thu, 11 Mar 2021 00:00:00 +0030</pubDate>
      
      <guid>/posts/2021-03-11-two-stories-azureml-aks/</guid>
      <description>Let&amp;rsquo;s start with 2 of my experiences with machine learning in data science competitions and writing a research paper. This will give us enough motivation to understand why platforms such as AzureML are required.
Story 1 - A data science competition  I participated in a data science competition that required predicting time series data. For this, I tried 5 different algorithms Arima, ETS, Facebook Prophet, XGBoost, and Random Forest. I also used several imputation techniques which also involved a number of hyperparameters.</description>
    </item>
    
    <item>
      <title>Azure ML HyperParameters</title>
      <link>/posts/2020-11-08-azure-hyperparameters-basics/</link>
      <pubDate>Sun, 08 Nov 2020 00:00:00 +0030</pubDate>
      
      <guid>/posts/2020-11-08-azure-hyperparameters-basics/</guid>
      <description>Parameter
A variable of a model that the machine learning system trains on its own. For example, weights are parameters whose values the machine learning system gradually learns through successive training iterations.
Hyperparameter
The &amp;ldquo;knobs&amp;rdquo; that we tweak during successive runs of training a model. For example, learning rate is a hyperparameter.
Hyperparameters have the following properties
  Search space - The set of hyperparameter values tried during hyperparameter tuning is known as the search space.</description>
    </item>
    
    <item>
      <title>Azure ML Environments</title>
      <link>/posts/2020-11-04-azure-ml-environments-runconfigs-basics/</link>
      <pubDate>Wed, 04 Nov 2020 00:00:00 +0030</pubDate>
      
      <guid>/posts/2020-11-04-azure-ml-environments-runconfigs-basics/</guid>
      <description>An environment is the encapsulation of the environment where training or scoring of the machine learning model happens. The environment specifies
 the Python packages, environment variables, and software settings around your training and scoring scripts.  Environments are of three types curated, user-managed, and system-managed.
Curated environments are provided by Azure Machine Learning and are available in the workspace by default.
In user-managed environments, the person is responsible for setting up the environment and installing every package that the training script needs on the compute target.</description>
    </item>
    
    <item>
      <title>Azure ML workspace basics</title>
      <link>/posts/2020-11-01-azure-ml-workspace-basics/</link>
      <pubDate>Sun, 01 Nov 2020 00:00:00 +0030</pubDate>
      
      <guid>/posts/2020-11-01-azure-ml-workspace-basics/</guid>
      <description>Azure ML has a top level component which is the Workspace. The workspace contains all the components of the Azure Machine Learning space.
The workspace is associated with
 Azure subscription Azure Key Vault Azure Application Insights  It is associated with the following assets
 Datasets Experiments Pipelines Models EndPoints  The workspace manages the following
 Datastores Compute  The workspace can be used for authoring
 Notebooks Automated ML Designer  You can implement via code</description>
    </item>
    
    <item>
      <title>Tensorflow basics</title>
      <link>/posts/2020-10-07-tensorflow-basics/</link>
      <pubDate>Wed, 07 Oct 2020 00:00:00 +0030</pubDate>
      
      <guid>/posts/2020-10-07-tensorflow-basics/</guid>
      <description>Sequential API
Validation Regularization Callback
Saving Models</description>
    </item>
    
    <item>
      <title>Random walk with Penguins</title>
      <link>/posts/2018-01-01-random-walk-with-penguins/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0030</pubDate>
      
      <guid>/posts/2018-01-01-random-walk-with-penguins/</guid>
      <description>Honoured to win the 1st prize in a Data Science competition Random Walk of Penguins hosted by DrivenData. Sharing the winning approach here.
Introduction Penguins are among the most charismatic animals in the world and have captured the imaginations of news-makers, scientists, film producers, and the general public. Beyond their general intrinsic value, they are considered important ecosystem indicators. In other words, monitoring these beautiful species can tell us a lot about the general health of the Antarctic because penguins are important krill and fish predators, and changes (natural or anthropogenic) that influence prey abundance and environmental conditions will ultimately be detected through changes in distribution or population size.</description>
    </item>
    
    <item>
      <title>Chicago Restaurants Inspections Text Mining</title>
      <link>/posts/2017-10-19-chicago-restaurants-inspections/</link>
      <pubDate>Thu, 19 Oct 2017 00:00:00 +0030</pubDate>
      
      <guid>/posts/2017-10-19-chicago-restaurants-inspections/</guid>
      <description>Restaurant inspections ensure that food served to the public at licensed food establishments follows food safety guidelines. The Food Protection Division of the Chicago Department of Public Health (CDPH) is committed to maintaining the safety of food bought, sold, or prepared for public consumption in Chicago by carrying out science-based inspections of all retail food establishments. These inspections promote public health in areas of food safety and sanitation and prevent the occurrence of food-borne illness.</description>
    </item>
    
    <item>
      <title>African Conflicts</title>
      <link>/posts/2017-10-17-african-conflicts/</link>
      <pubDate>Tue, 17 Oct 2017 00:00:00 +0030</pubDate>
      
      <guid>/posts/2017-10-17-african-conflicts/</guid>
      <description>We examine a dataset from ACLED(Armed Conflict Location and Event Project).The Armed Conflict Location and Event Data Project is designed for disaggregated conflict analysis and crisis mapping. This dataset codes the dates and locations of all reported political violence and protest events in dozens of developing countries in Africa. Political violence and protest includes events that occur within civil wars and periods of instability, public protest and regime breakdown. The project covers all African countries from 1997 to the present.</description>
    </item>
    
    <item>
      <title>Austin Bike Share EDA with Maps and TimeSeries</title>
      <link>/posts/2017-10-07-austin-bikeshare/</link>
      <pubDate>Sat, 07 Oct 2017 00:00:00 +0030</pubDate>
      
      <guid>/posts/2017-10-07-austin-bikeshare/</guid>
      <description>Catching a show on Red River? Eating on the East Side? Paddleboarding at Zilker? All of the above?
Hop on a B-cycle and coast your way through the People’s Republic of Austin
We explore the Austin Bike Share data for the following topics :
 Top 10 Common Start Stations Top 10 Common Destination Stations Get Top 10 Most Popular Routes Map of Bike Stations Month , Day , Time Analysis Types of Subscriber Month , Day , Time Analysis for Local365 Duration of Trips Trips for Station ID #2575 BikeTrips for Station ID #2575 (Week wise) SXSW 2017 SXSW 2016  A detailed report Austin Bike Share Analysis has the complete Data Visualization with Maps.</description>
    </item>
    
    <item>
      <title>Fun in TextMining with Simpsons</title>
      <link>/posts/2017-10-07-fun-in-textmining-with-simpsons/</link>
      <pubDate>Sat, 07 Oct 2017 00:00:00 +0030</pubDate>
      
      <guid>/posts/2017-10-07-fun-in-textmining-with-simpsons/</guid>
      <description>This dataset contains the characters, locations, episode details, and script lines for approximately 600 Simpsons episodes, dating back to 1989.
I had fun learning Text Mining with Simpsons.Hope you will also enjoy the same while I had writing the text mining and data visualization code.
A detailed report Fun in Text Mining with Simpsons has the complete Data Visualization with Text Mining and Modelling.</description>
    </item>
    
    <item>
      <title>Meteorite Landings</title>
      <link>/posts/2017-10-05-meteorite-landings/</link>
      <pubDate>Thu, 05 Oct 2017 00:00:00 +0030</pubDate>
      
      <guid>/posts/2017-10-05-meteorite-landings/</guid>
      <description>We examine a dataset from NASA Meteorite Landings and do a complete Exploratory Data Analysis.
The Meteoritical Society collects data on meteorites that have fallen to Earth from outer space. This dataset includes the location, mass, composition, and fall year for over 45,000 meteorites that have struck our planet.
A detailed report Fun with Meteorite Landings has the complete Exploratory Data Analysis.
A book on this report can be found in Little Book on Exploratory Data Analysis.</description>
    </item>
    
    <item>
      <title>Crime in Los Angeles</title>
      <link>/posts/2017-09-25-la-crime/</link>
      <pubDate>Mon, 25 Sep 2017 00:00:00 +0030</pubDate>
      
      <guid>/posts/2017-09-25-la-crime/</guid>
      <description>We investigate the Crime in LA. We do Exploratory Data analysis on various topics such as the Month of Crime ,Day of Crime,Time of Crime. We also explore how the Crimes are related to the Sex and Age of the Victims, Crime Types, Crime Types of persons aged 70 and above. We explore on the Crimes at different Premises,Types of Crimes at different Premises,victim Descent Analysis and an analysis of Weapons used in Crime.</description>
    </item>
    
    <item>
      <title>activation functions</title>
      <link>/posts/2017-07-06-activation-functions/</link>
      <pubDate>Thu, 06 Jul 2017 00:00:00 +0030</pubDate>
      
      <guid>/posts/2017-07-06-activation-functions/</guid>
      <description>Sigmoid function takes values between 0 and 1
$ {sigmoid} = \frac {e^x} {( 1+ e^x )} $
This is used in logistic regression.
The sigmoid graph is generated by the R code provided below.
Softmax function is used to calculate the probablities in multiple classes. Suppose we have a image of a digit. We want to calculate whether it is a 1, 2, 3, and so on. Softmax would provide the probablity for each of the classes.</description>
    </item>
    
    <item>
      <title>Installing XGBoost on Anaconda on Windows</title>
      <link>/posts/2017-07-06-installing-xgboost-anaconda-windows/</link>
      <pubDate>Thu, 06 Jul 2017 00:00:00 +0030</pubDate>
      
      <guid>/posts/2017-07-06-installing-xgboost-anaconda-windows/</guid>
      <description>The installation instructions are exactly the same as in the Installing XGBoost For Anaconda on Windows except Step 10 since the name of the DLL created is libxgboost.dll but the Python Module expects the dll of the name xgboost.dll.
Step 1 : Install Anaconda
Step 2 : Install Git on Windows
Step 3 : Launch Git Bash window
Step 4 : The directory in which the code is to be installed in my case is D:\XGBoostCode</description>
    </item>
    
    <item>
      <title>California WireTapping</title>
      <link>/posts/2017-06-17-california-wiretapping/</link>
      <pubDate>Sat, 17 Jun 2017 00:00:00 +0030</pubDate>
      
      <guid>/posts/2017-06-17-california-wiretapping/</guid>
      <description>In 2016, California investigators used state wiretapping laws 563 times to capture 7.8 million communications from 181,000 people, and only 19% of these communications were incriminating. The year&amp;rsquo;s wiretaps cost nearly $30 million.
We know this, and much more, now that the California Department of Justice (CADOJ) for the first time has released to EFF the dataset underlying its annual wiretap report to the state legislature.
The California WireTappinghas a detailed analysis of the same.</description>
    </item>
    
    <item>
      <title>Uttar Pradesh Assembly Elections 2017 Data Visualization</title>
      <link>/posts/2017-06-15-up-elections-2017/</link>
      <pubDate>Thu, 15 Jun 2017 00:00:00 +0030</pubDate>
      
      <guid>/posts/2017-06-15-up-elections-2017/</guid>
      <description>The assembly election results for Uttar Pradesh(UP) ,largest state in India were surprising to say the least. Never in the past has any single party secured a similar mandate. UP with a population of around 220 million is as big as the whole of United States. It has 403 constituencies each having its own demographic breakup. The election was conducted in 7 phases.
The Exploratory Analysis UP Elections 2017has a detailed analysis of the same.</description>
    </item>
    
    <item>
      <title>Adverse Food Events</title>
      <link>/posts/2017-10-07-adverse-food-events/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/posts/2017-10-07-adverse-food-events/</guid>
      <description>The CFSAN Adverse Event Reporting System (CAERS) is a database that contains information on adverse event and product complaint reports submitted to FDA for foods, dietary supplements, and cosmetics. The database is designed to support CFSAN’s safety surveillance program.
We explore the data as outlined below :
 Ten Most Common Symptoms Ten Most Common Product Name Ten Most Common Industry Name Ages Commonly Affected Distribution of Ages Ten Most Common Outcomes Outcomes and Products associated with Adverse Events Symptoms and the Products Causing it Time Analysis of Adverse Events Products in Adverse Events for January Gender and Adverse Events  A detailed report Adverse Food Events EDA has the complete Data Visualization.</description>
    </item>
    
    <item>
      <title>Azure ML Pipelines</title>
      <link>/posts/2020-11-05-azure-pipelines-basics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/posts/2020-11-05-azure-pipelines-basics/</guid>
      <description>Different types of pipelines are
In Azure Machine Learning, a pipeline is a workflow of machine learning tasks in which each task is implemented as a step.
 Steps can be arranged sequentially or in parallel Each step can be run on a specific compute target, making it possible to combine different types of processing as required to achieve an overall goal A pipeline can be executed as a process by running the pipeline as an experiment Each step in the pipeline runs on its allocated compute target as part of the overall experiment run.</description>
    </item>
    
    <item>
      <title>Melbourne Housing Market Data Visualization</title>
      <link>/posts/2017-06-16-melb-housing-market/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/posts/2017-06-16-melb-housing-market/</guid>
      <description>Melbourne is currently experiencing a housing bubble (some experts say it may burst soon). Maybe someone can find a trend or give a prediction? Which suburbs are the best to buy in? Which ones are value for money? Where&amp;rsquo;s the expensive side of town? And more importantly where should your distant cousin :-) buy a 2 bedroom unit?
The Melbourne Housing Market has a detailed analysis of the same.The analysis provides insights into the following</description>
    </item>
    
  </channel>
</rss>

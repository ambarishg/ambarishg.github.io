<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge"><title>Word embeddings - Thoughts - Ambarish</title><meta name="viewport" content="width=device-width, initial-scale=1">
	<meta itemprop="name" content="Word embeddings">
<meta itemprop="description" content="Word embeddings concept"><meta itemprop="datePublished" content="2021-05-18T00:00:00&#43;00:30" />
<meta itemprop="dateModified" content="2021-05-18T00:00:00&#43;00:30" />
<meta itemprop="wordCount" content="344">
<meta itemprop="keywords" content="word embeddings,nlp," /><meta property="og:title" content="Word embeddings" />
<meta property="og:description" content="Word embeddings concept" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/word-embeddings/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-05-18T00:00:00&#43;00:30" />
<meta property="article:modified_time" content="2021-05-18T00:00:00&#43;00:30" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Word embeddings"/>
<meta name="twitter:description" content="Word embeddings concept"/>
<link href='https://fonts.googleapis.com/css?family=Playfair+Display:700' rel='stylesheet' type='text/css'>
	<link rel="stylesheet" type="text/css" media="screen" href="/css/normalize.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="/css/main.css" />

        <link id="dark-scheme" rel="stylesheet" type="text/css" href="/css/dark.css" />

	<script src="https://cdn.jsdelivr.net/npm/feather-icons/dist/feather.min.js"></script>
		<script src="/js/main.js"></script>
</head>

<body>
	<div class="container wrapper">
		<div class="header">
	
	<h1 class="site-title"><a href="/">Thoughts - Ambarish</a></h1>
	<div class="site-description"><p></p><nav class="nav social">
			<ul class="flat"><li><a href="mailto:ambarish.ganguly@gmail.com" title="Email"><i data-feather="mail"></i></a></li><li><a href="https://www.linkedin.com/in/ambarish-ganguly/" title="LinkedIn"><i data-feather="linkedin"></i></a></li><li><a href="https://twitter.com/a_ganguly/" title="Twitter"><i data-feather="twitter"></i></a></li><li><a href="https://github.com/ambarishg" title="Github"><i data-feather="github"></i></a></li><li><a href="/index.xml" title="RSS"><i data-feather="rss"></i></a></li><li><a href="#" class="scheme-toggle" id="scheme-toggle"></a></li></ul>
		</nav>
	</div>

	<nav class="nav">
		<ul class="flat">
			
			<li>
				<a href="/">Home</a>
			</li>
			
			<li>
				<a href="/posts">All posts</a>
			</li>
			
			<li>
				<a href="/about">About</a>
			</li>
			
			<li>
				<a href="/tags">Tags</a>
			</li>
			
			<li>
				<a href="/books">Books</a>
			</li>
			
			<li>
				<a href="/awards/">Awards</a>
			</li>
			
			<li>
				<a href="/testimonials/">Testimonials</a>
			</li>
			
		</ul>
	</nav>
</div>


		<div class="post">
			<div class="post-header">
				
					<div class="meta">
						<div class="date">
							<span class="day">18</span>
							<span class="rest">May 2021</span>
						</div>
					</div>
				
				<div class="matter">
					<h1 class="title">Word embeddings</h1>
				</div>
			</div>
					
			<div class="markdown">
				<p>From the TensorFlow documentation <a href="https://www.tensorflow.org/tutorials/text/word_embeddings">word embeddings documentation</a></p>
<blockquote>
<p>Word embeddings give us a way to use an <strong>efficient, dense</strong> representation in which similar words have a similar encoding.</p>
</blockquote>
<blockquote>
<p>Importantly, you do not have to specify this encoding by hand. An embedding is a dense vector of floating point values (the length of the vector is a parameter you specify).</p>
</blockquote>
<blockquote>
<p>Instead of specifying the values for the embedding manually, they are trainable parameters (weights learned by the model during training, in the same way a model learns weights for a dense layer).</p>
</blockquote>
<blockquote>
<p>It is common to see word embeddings that are 8-dimensional (for small datasets), up to 1024-dimensions when working with large datasets. A higher dimensional embedding can capture fine-grained relationships between words, but takes more data to learn.</p>
</blockquote>
<p><img src="../../img/azureml/embedding2.png" alt="word embedding example"></p>
<p>Let us explore the word embedding with some examples. We will use <strong>spacy</strong> for demonstration.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">import numpy as np
import spacy
from sklearn.metrics.pairwise import cosine_similarity
# Need to load the large model to get the vectors
nlp = spacy.load(&#39;en_core_web_lg&#39;)

nlp(&#34;queen&#34;).vector.shape
</code></pre></div><p>We find the word embedding of a  single word <strong>queen</strong> and find that we have a vector with 1 row and 300 columns. Therefore a single word is converted to 300 numerical values.</p>
<p>We find the similarity between the words using cosine similarity</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">cosine_similarity([nlp(&#34;queen&#34;).vector],[nlp(&#34;king&#34;).vector])
</code></pre></div><blockquote>
<p>0.725261</p>
</blockquote>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">cosine_similarity([nlp(&#34;queen&#34;).vector],[nlp(&#34;mother&#34;).vector])
</code></pre></div><blockquote>
<p>0.44720313</p>
</blockquote>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">cosine_similarity([nlp(&#34;queen&#34;).vector],[nlp(&#34;princess&#34;).vector])
</code></pre></div><blockquote>
<p>0.6578181</p>
</blockquote>
<p>We observe that the similarity between queen and king is the highest , followed by princess and mother</p>
<p>We will see that how we can use the similarity between sentences</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">x1 = nlp(&#34;I am a software consultant&#34;).vector
x2 = nlp(&#34;Hey ,me  data guy&#34;).vector
x3 = nlp(&#34;Hey ,me  plumber&#34;).vector

</code></pre></div><div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">x1.shape , x2.shape , x3.shape
</code></pre></div><blockquote>
<p>((300,), (300,), (300,))</p>
</blockquote>
<p>We find that the shape of the sentence vectors are also 1 x 300. The individual words also have shape  1 x 300 . But for a sentence , we average the vectors so as to get the shape also as 1 x 300.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">cosine_similarity([nlp(&#34;x1&#34;).vector],[nlp(&#34;x2&#34;).vector])
</code></pre></div><blockquote>
<p>0.7383951</p>
</blockquote>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">cosine_similarity([nlp(&#34;x1&#34;).vector],[nlp(&#34;x3&#34;).vector])
</code></pre></div><blockquote>
<p>0.64217263</p>
</blockquote>
<p>We see that the similarity between the senctence with software consultant and data guy is higher than the sentence with software consultant and plumber</p>

			</div>

			<div class="tags">
				
					
						<ul class="flat">
							
							<li><a href="/tags/word-embeddings">word embeddings</a></li>
							
							<li><a href="/tags/nlp">nlp</a></li>
							
						</ul>
					
				
			</div>

			<script src="https://utteranc.es/client.js"
        repo="ambarishg/ambarishg.github.io"
        issue-term="pathname"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>
		</div>
	</div>
	<div class="footer wrapper">
	<nav class="nav">
		<div>2021  Â© Copyright notice |  <a href="https://github.com/knadh/hugo-ink">Ink</a> theme on <a href="https://gohugo.io">Hugo</a></div>
	</nav>
</div>


<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-113191180-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>
<script>feather.replace()</script>
</body>
</html>

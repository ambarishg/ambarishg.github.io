[
["index.html", "Little Book on Text Mining Chapter 1 Introduction 1.1 Spooky Author Identification Dataset", " Little Book on Text Mining Ambarish Ganguly Chapter 1 Introduction This Little Book on Text Mining provides a gentle and hands on introduction to Text Mining. If you are tired of reading through pages of text and would like to get your hands dirty and experience on how to do a quick and detailed text mining, then you are in the right place. This book does a detailed Text Mining and Modelling on the following datasets Spooky Author Identification dataset from Kaggle Yelp Data Reviews dataset from Kaggle Simpsons dataset from Kaggle Chicago Inspections dataset from Kaggle The book focuses on Three main areas Exploratory Data Analysis , TF IDF concept and the application of it ,Bigrams ,Trigrams ,Relationship among various words ( Word Clouds and Bar Plots ) Detailed Sentiment Analysis and insights from it using different Sentiment Analysis lexicons such as AFINN , NRC Modelling using feature engineering and supervised learning techniques such as XGBoost and Multinomial Logistic Regression.Modelling using unsupervised learning techniques such as Topic Modelling. 1.1 Spooky Author Identification Dataset The Spooky Author Identification dataset from Kaggle has excerpts from horror stories by Edgar Allan Poe, Mary Shelley, and HP Lovecraft. The competition challenges to predict the author of excerpts. The dataset can be found in Kaggle. This chapter focuses on the following topics Word Length comparison among the various authors Common words used by the authors TF IDF concept and the application of it Bigrams Trigrams Relationship among various words Sentiment Analysis NRC Sentiment Analysis Building features using the Sentiment Score Building features using the NRC Sentiment Score Words Commonly used by Males and Females in Authors’ text Evaluation of the Document Term Matrix Topic Modelling Predictions using the XGBoost Model Predictions using the GLMnet Model "],
["read-the-data.html", "Chapter 2 Read the Data", " Chapter 2 Read the Data We read the data from the train and the test datasets. library(tidyverse) library(tidytext) library(stringr) library(knitr) library(kableExtra) library(&#39;wordcloud&#39;) library(igraph) library(ggraph) library(tm) library(topicmodels) library(caret) library(syuzhet) library(text2vec) library(data.table) library(&quot;readr&quot;) library(glmnet) library(topicmodels) # for LDA topic modelling library(tm) # general text mining functions, making document term matrixes library(SnowballC) # for stemming rm(list=ls()) fillColor = &quot;#FFA07A&quot; fillColor2 = &quot;#F1C40F&quot; train = read_csv(&quot;input/train.csv&quot;) test = read_csv(&quot;input/test.csv&quot;) "],
["add-feature-number-of-words.html", "Chapter 3 Add Feature Number of Words", " Chapter 3 Add Feature Number of Words We add a Feature, Number of Words for each line to the Train and Test data sets train$len = str_count(train$text) test$len = str_count(test$text) "],
["peek-into-the-data.html", "Chapter 4 Peek into the Data", " Chapter 4 Peek into the Data We peek into the train data in the table below. kable(head(train),&quot;html&quot;) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;, &quot;responsive&quot;)) %&gt;% scroll_box(width = &quot;800px&quot;) id text author len id26305 This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall. EAP 231 id17569 It never once occurred to me that the fumbling might be a mere mistake. HPL 71 id11008 In his left hand was a gold snuff box, from which, as he capered down the hill, cutting all manner of fantastic steps, he took snuff incessantly with an air of the greatest possible self satisfaction. EAP 200 id27763 How lovely is spring As we looked from Windsor Terrace on the sixteen fertile counties spread beneath, speckled by happy cottages and wealthier towns, all looked as in former years, heart cheering and fair. MWS 206 id12958 Finding nothing else, not even gold, the Superintendent abandoned his attempts; but a perplexed look occasionally steals over his countenance as he sits thinking at his desk. HPL 174 id22965 A youth passed in solitude, my best years spent under your gentle and feminine fosterage, has so refined the groundwork of my character that I cannot overcome an intense distaste to the usual brutality exercised on board ship: I have never believed it to be necessary, and when I heard of a mariner equally noted for his kindliness of heart and the respect and obedience paid to him by his crew, I felt myself peculiarly fortunate in being able to secure his services. MWS 468 "],
["length-comparison.html", "Chapter 5 Length Comparison", " Chapter 5 Length Comparison We examine the median length of the sentences by the different authors and plot in a flipped bar plot. HP Lovecraft writes long sentences with the highest number of words per sentence.Edgar Allen Poe writes short sentences compared to the other Two authors. train %&gt;% group_by(author) %&gt;% summarise(CountMedian = median(len,na.rm = TRUE)) %&gt;% ungroup() %&gt;% mutate(author = reorder(author,CountMedian)) %&gt;% ggplot(aes(x = author,y = CountMedian)) + geom_bar(stat=&#39;identity&#39;,colour=&quot;white&quot;, fill = fillColor2) + geom_text(aes(x = author, y = 1, label = paste0(&quot;(&quot;,CountMedian,&quot;)&quot;,sep=&quot;&quot;)), hjust=0, vjust=.5, size = 4, colour = &#39;black&#39;, fontface = &#39;bold&#39;) + labs(x = &#39;author&#39;, y = &#39;Count&#39;, title = &#39;author and Count&#39;) + coord_flip() + theme_bw() "],
["words-length-distribution.html", "Chapter 6 Words Length Distribution 6.1 Words Length Distribution Plot 2", " Chapter 6 Words Length Distribution We examine the number of words written by the author in a single sentence with the histogram. Unfortunately the plot does not reveal much. Therefore we would like to change the x-axis so that we can have a better plot. train %&gt;% ggplot(aes(x = len, fill = author)) + geom_histogram() + scale_fill_manual( values = c(&quot;red&quot;,&quot;blue&quot;,&quot;orange&quot;) ) + facet_wrap(~author) + labs(x= &#39;Word Length&#39;,y = &#39;Count&#39;, title = paste(&quot;Distribution of&quot;, &#39; Word Length &#39;)) + theme_bw() 6.1 Words Length Distribution Plot 2 We limit the word length to 100 and investigate the distribution.We notice that HP Lovecraft and Mary Wollstonecraft Shelley have a lot of sentences with word length in the range 75 - 100. train %&gt;% ggplot(aes(x = len, fill = author)) + geom_histogram() + scale_x_continuous(limits = c(15,100)) + scale_fill_manual( values = c(&quot;red&quot;,&quot;blue&quot;,&quot;orange&quot;) ) + facet_wrap(~author) + labs(x= &#39;Word Length&#39;,y = &#39;Count&#39;, title = paste(&quot;Distribution of&quot;, &#39; Word Length &#39;)) + theme_bw() "],
["tokensiation.html", "Chapter 7 Tokensiation 7.1 Removing the Stop words", " Chapter 7 Tokensiation We break the text into individual tokens which are simply individual words. This process is called tokenisation. This is accomplished through the unnest_tokens function. train %&gt;% unnest_tokens(word, text) %&gt;% head(10) ## # A tibble: 10 x 4 ## id author len word ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 id26305 EAP 231 this ## 2 id26305 EAP 231 process ## 3 id26305 EAP 231 however ## 4 id26305 EAP 231 afforded ## 5 id26305 EAP 231 me ## 6 id26305 EAP 231 no ## 7 id26305 EAP 231 means ## 8 id26305 EAP 231 of ## 9 id26305 EAP 231 ascertaining ## 10 id26305 EAP 231 the 7.1 Removing the Stop words We seperate the words in the train dataset and remove the most commonly occuring words train %&gt;% unnest_tokens(word, text) %&gt;% filter(!word %in% stop_words$word) %&gt;% head(10) ## # A tibble: 10 x 4 ## id author len word ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 id26305 EAP 231 process ## 2 id26305 EAP 231 afforded ## 3 id26305 EAP 231 means ## 4 id26305 EAP 231 ascertaining ## 5 id26305 EAP 231 dimensions ## 6 id26305 EAP 231 dungeon ## 7 id26305 EAP 231 circuit ## 8 id26305 EAP 231 return ## 9 id26305 EAP 231 set ## 10 id26305 EAP 231 aware "],
["top-twenty-most-common-words.html", "Chapter 8 Top Twenty most Common Words 8.1 WordCloud of the Common Words 8.2 WordCloud of HPL 8.3 BarPlot for words of HPL 8.4 WordCloud of MWS 8.5 BarPlot for words of MWS 8.6 WordCloud of EAP 8.7 BarPlot for words of EAP", " Chapter 8 Top Twenty most Common Words We examine the Top Twenty Most Common words and show them in a bar graph. Several words which occur a lot are time , life , found , night, eyes, day , death, mind , heard createBarPlotCommonWords = function(train,title) { train %&gt;% unnest_tokens(word, text) %&gt;% filter(!word %in% stop_words$word) %&gt;% count(word,sort = TRUE) %&gt;% ungroup() %&gt;% mutate(word = factor(word, levels = rev(unique(word)))) %&gt;% head(10) %&gt;% ggplot(aes(x = word,y = n)) + geom_bar(stat=&#39;identity&#39;,colour=&quot;white&quot;, fill =fillColor) + geom_text(aes(x = word, y = 1, label = paste0(&quot;(&quot;,n,&quot;)&quot;,sep=&quot;&quot;)), hjust=0, vjust=.5, size = 4, colour = &#39;black&#39;, fontface = &#39;bold&#39;) + labs(x = &#39;Word&#39;, y = &#39;Word Count&#39;, title = title) + coord_flip() + theme_bw() } createBarPlotCommonWords(train,&#39;Top 10 most Common Words&#39;) 8.1 WordCloud of the Common Words A word cloud is a graphical representation of frequently used words in the text. The height of each word in this picture is an indication of frequency of occurrence of the word in the entire text. createWordCloud = function(train) { train %&gt;% unnest_tokens(word, text) %&gt;% filter(!word %in% stop_words$word) %&gt;% count(word,sort = TRUE) %&gt;% ungroup() %&gt;% head(30) %&gt;% with(wordcloud(word, n, max.words = 30,colors=brewer.pal(8, &quot;Dark2&quot;))) } createWordCloud(train) 8.2 WordCloud of HPL Time , night, strange, found, house are the most common words written by HP Lovecraft. createWordCloud(train %&gt;% filter(author == &#39;HPL&#39;)) 8.3 BarPlot for words of HPL createBarPlotCommonWords(train %&gt;% filter(author == &#39;HPL&#39;),&#39;Top 10 most Common Words of HPL&#39;) 8.4 WordCloud of MWS Raymond,heart, love, time and eyes are the most common words written by Mary Wollstonecraft Shelley createWordCloud(train %&gt;% filter(author == &#39;MWS&#39;)) 8.5 BarPlot for words of MWS createBarPlotCommonWords(train %&gt;% filter(author == &#39;MWS&#39;),&#39;Top 10 most Common Words of MWS&#39;) 8.6 WordCloud of EAP Found, time, eyes, length, head, day are the most common words written by Edgar Allan Poe. createWordCloud(train %&gt;% filter(author == &#39;EAP&#39;)) 8.7 BarPlot for words of EAP createBarPlotCommonWords(train %&gt;% filter(author == &#39;EAP&#39;),&#39;Top 10 most Common Words of EAP&#39;) "],
["tf-idf.html", "Chapter 9 TF-IDF 9.1 The Math 9.2 Twenty Most Important words 9.3 Word Cloud for the Most Important Words", " Chapter 9 TF-IDF We wish to find out the important words which are written by the authors. Example for your young child , the most important word is mom. Example for a bar tender , important words would be related to drinks. We would explore this using a fascinating concept known as Term Frequency - Inverse Document Frequency. Quite a mouthful, but we will unpack it and clarify each and every term. A document in this case is the set of lines written by an author. Therefore we have different documents for each Author. From the book 5 Algorithms Every Web Developer Can Use and Understand TF-IDF computes a weight which represents the importance of a term inside a document. It does this by comparing the frequency of usage inside an individual document as opposed to the entire data set (a collection of documents). The importance increases proportionally to the number of times a word appears in the individual document itself–this is called Term Frequency. However, if multiple documents contain the same word many times then you run into a problem. That’s why TF-IDF also offsets this value by the frequency of the term in the entire document set, a value called Inverse Document Frequency. 9.1 The Math TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document) IDF(t) = log_e(Total number of documents / Number of documents with term t in it). Value = TF * IDF 9.2 Twenty Most Important words Here using TF-IDF , we investigate the Twenty Most Important words trainWords &lt;- train %&gt;% unnest_tokens(word, text) %&gt;% count(author, word, sort = TRUE) %&gt;% ungroup() total_words &lt;- trainWords %&gt;% group_by(author) %&gt;% summarize(total = sum(n)) trainWords &lt;- left_join(trainWords, total_words) #Now we are ready to use the bind_tf_idf which computes the tf-idf for each term. trainWords &lt;- trainWords %&gt;% filter(!is.na(author)) %&gt;% bind_tf_idf(word, author, n) plot_trainWords &lt;- trainWords %&gt;% arrange(desc(tf_idf)) %&gt;% mutate(word = factor(word, levels = rev(unique(word)))) plot_trainWords %&gt;% top_n(20) %&gt;% ggplot(aes(word, tf_idf)) + geom_col(fill = fillColor) + labs(x = NULL, y = &quot;tf-idf&quot;) + coord_flip() + theme_bw() 9.2.1 Twenty most important words HPL plot_trainWords %&gt;% filter(author == &#39;HPL&#39;) %&gt;% top_n(20) %&gt;% ggplot(aes(word, tf_idf)) + geom_col(fill = fillColor2) + labs(x = NULL, y = &quot;tf-idf&quot;) + coord_flip() + theme_bw() 9.2.2 Twenty most important words EAP plot_trainWords %&gt;% filter(author == &#39;EAP&#39;) %&gt;% top_n(20) %&gt;% ggplot(aes(word, tf_idf)) + geom_col(fill = fillColor) + labs(x = NULL, y = &quot;tf-idf&quot;) + coord_flip() + theme_bw() 9.2.3 Twenty most important words MWS plot_trainWords %&gt;% filter(author == &#39;MWS&#39;) %&gt;% top_n(20) %&gt;% ggplot(aes(word, tf_idf, fill = author)) + geom_col() + labs(x = NULL, y = &quot;tf-idf&quot;) + coord_flip() + theme_bw() 9.3 Word Cloud for the Most Important Words We show the Hundred most important words. This Word Cloud is based on the TF- IDF scores. Higher the score, bigger is the size of the text. plot_trainWords %&gt;% with(wordcloud(word, tf_idf, max.words = 100,colors=brewer.pal(8, &quot;Dark2&quot;))) "],
["most-common-bigrams.html", "Chapter 10 Most Common Bigrams", " Chapter 10 Most Common Bigrams A Bigram is a collection of Two words. We examine the most common Bigrams and plot them in a bar plot. count_bigrams &lt;- function(dataset) { dataset %&gt;% unnest_tokens(bigram, text, token = &quot;ngrams&quot;, n = 2) %&gt;% separate(bigram, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) %&gt;% filter(!word1 %in% stop_words$word, !word2 %in% stop_words$word) %&gt;% count(word1, word2, sort = TRUE) } visualize_bigrams &lt;- function(bigrams) { set.seed(2016) a &lt;- grid::arrow(type = &quot;closed&quot;, length = unit(.15, &quot;inches&quot;)) bigrams %&gt;% graph_from_data_frame() %&gt;% ggraph(layout = &quot;fr&quot;) + geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a) + geom_node_point(color = &quot;lightblue&quot;, size = 5) + geom_node_text(aes(label = name), vjust = 1, hjust = 1) + theme_void() } visualize_bigrams_individual &lt;- function(bigrams) { set.seed(2016) a &lt;- grid::arrow(type = &quot;closed&quot;, length = unit(.15, &quot;inches&quot;)) bigrams %&gt;% graph_from_data_frame() %&gt;% ggraph(layout = &quot;fr&quot;) + geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a,end_cap = circle(.07, &#39;inches&#39;)) + geom_node_point(color = &quot;lightblue&quot;, size = 5) + geom_node_text(aes(label = name), vjust = 1, hjust = 1) + theme_void() } train %&gt;% unnest_tokens(bigram, text, token = &quot;ngrams&quot;, n = 2) %&gt;% separate(bigram, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) %&gt;% filter(!word1 %in% stop_words$word, !word2 %in% stop_words$word) %&gt;% unite(bigramWord, word1, word2, sep = &quot; &quot;) %&gt;% group_by(bigramWord) %&gt;% tally() %&gt;% ungroup() %&gt;% arrange(desc(n)) %&gt;% mutate(bigramWord = reorder(bigramWord,n)) %&gt;% head(10) %&gt;% ggplot(aes(x = bigramWord,y = n)) + geom_bar(stat=&#39;identity&#39;,colour=&quot;white&quot;, fill = fillColor2) + geom_text(aes(x = bigramWord, y = 1, label = paste0(&quot;(&quot;,n,&quot;)&quot;,sep=&quot;&quot;)), hjust=0, vjust=.5, size = 4, colour = &#39;black&#39;, fontface = &#39;bold&#39;) + labs(x = &#39;Bigram&#39;, y = &#39;Count&#39;, title = &#39;Bigram and Count&#39;) + coord_flip() + theme_bw() "],
["most-common-trigrams.html", "Chapter 11 Most Common Trigrams", " Chapter 11 Most Common Trigrams A Trigram is a collection of Three words. We examine the most common Trigrams and plot them in a bar plot. train %&gt;% unnest_tokens(trigram, text, token = &quot;ngrams&quot;, n = 3) %&gt;% separate(trigram, c(&quot;word1&quot;, &quot;word2&quot;,&quot;word3&quot;), sep = &quot; &quot;) %&gt;% filter(!word1 %in% stop_words$word, !word2 %in% stop_words$word, !word3 %in% stop_words$word) %&gt;% unite(trigramWord, word1, word2, word3,sep = &quot; &quot;) %&gt;% group_by(trigramWord) %&gt;% tally() %&gt;% ungroup() %&gt;% arrange(desc(n)) %&gt;% mutate(trigramWord = reorder(trigramWord,n)) %&gt;% head(10) %&gt;% ggplot(aes(x = trigramWord,y = n)) + geom_bar(stat=&#39;identity&#39;,colour=&quot;white&quot;, fill = fillColor2) + geom_text(aes(x = trigramWord, y = 1, label = paste0(&quot;(&quot;,n,&quot;)&quot;,sep=&quot;&quot;)), hjust=0, vjust=.5, size = 4, colour = &#39;black&#39;, fontface = &#39;bold&#39;) + labs(x = &#39;Trigram&#39;, y = &#39;Count&#39;, title = &#39;Trigram and Count&#39;) + coord_flip() + theme_bw() "],
["relationship-among-words.html", "Chapter 12 Relationship among words", " Chapter 12 Relationship among words Til now, we have explored the most important words for a author. Now, we will explore the relationship between words. trainWords &lt;- train %&gt;% count_bigrams() trainWords %&gt;% filter(n &gt; 10) %&gt;% visualize_bigrams() The above infographic shows the words which follow another word. "],
["sentiment-analysis.html", "Chapter 13 Sentiment Analysis 13.1 Postive Authors and Not so Positive Authors 13.2 Postive and Not So Postive Words of Authors 13.3 Postive and Not So Postive Words of Author HPL 13.4 Postive and Not So Postive Words of Author EAP 13.5 Postive and Not So Postive Words of Author MWS", " Chapter 13 Sentiment Analysis 13.1 Postive Authors and Not so Positive Authors We investigate how often positive and negative words occurred in the text written by the authors. Which author was the most positive or negative overall? We will use the AFINN sentiment lexicon, which provides numeric positivity scores for each word, and visualize it with a bar plot. Edgar Allen Poe and Mary Wollstonecraft Shelley are positive authors HP Lovecraft is unfortunately a negative author as explained through a bar plot. We need to go into detail why HP Lovecraft is a negative author. visualize_sentiments &lt;- function(SCWords) { SCWords_sentiments &lt;- SCWords %&gt;% inner_join(get_sentiments(&quot;afinn&quot;), by = &quot;word&quot;) %&gt;% group_by(author) %&gt;% summarize(score = sum(score * n) / sum(n)) %&gt;% arrange(desc(score)) SCWords_sentiments %&gt;% mutate(author = reorder(author, score)) %&gt;% ggplot(aes(author, score, fill = score &gt; 0)) + geom_col(show.legend = TRUE) + coord_flip() + ylab(&quot;Average sentiment score&quot;) + theme_bw() } trainWords &lt;- train %&gt;% unnest_tokens(word, text) %&gt;% count(author, word, sort = TRUE) %&gt;% ungroup() visualize_sentiments(trainWords) 13.2 Postive and Not So Postive Words of Authors The following graph shows the Twenty high positive and the negative words positiveWordsBarGraph &lt;- function(SC) { contributions &lt;- SC %&gt;% unnest_tokens(word, text) %&gt;% count(author, word, sort = TRUE) %&gt;% ungroup() %&gt;% inner_join(get_sentiments(&quot;afinn&quot;), by = &quot;word&quot;) %&gt;% group_by(word) %&gt;% summarize(occurences = n(), contribution = sum(score)) contributions %&gt;% top_n(20, abs(contribution)) %&gt;% mutate(word = reorder(word, contribution)) %&gt;% head(20) %&gt;% ggplot(aes(word, contribution, fill = contribution &gt; 0)) + geom_col(show.legend = FALSE) + coord_flip() + theme_bw() } positiveWordsBarGraph(train) 13.3 Postive and Not So Postive Words of Author HPL trainHPL = train %&gt;% filter(author == &#39;HPL&#39;) positiveWordsBarGraph(trainHPL) 13.4 Postive and Not So Postive Words of Author EAP trainEAP = train %&gt;% filter(author == &#39;EAP&#39;) positiveWordsBarGraph(trainEAP) 13.5 Postive and Not So Postive Words of Author MWS trainMWS = train %&gt;% filter(author == &#39;MWS&#39;) positiveWordsBarGraph(trainMWS) "],
["sentiment-analysis-using-nrc-sentiment-lexicon.html", "Chapter 14 Sentiment Analysis using NRC Sentiment lexicon 14.1 Sentiment Analysis Words - Fear 14.2 Fear Word Cloud - MWP 14.3 Sentiment Analysis Words - Surprise 14.4 Surprise Word Cloud - MWP 14.5 Sentiment Analysis Words - Joy 14.6 Joy Word Cloud - MWP", " Chapter 14 Sentiment Analysis using NRC Sentiment lexicon We examine the following sentiments using NRC Sentiment lexicon Fear Surprise Joy Mary Wollstonecraft Shelley is the most Fearful and most Surprising and most Joyful author. Edgar Allen Poe is the least Fearful author. HP Lovecraft is the least Surprising author. HP Lovecraft is the least Joyful author. 14.1 Sentiment Analysis Words - Fear The plot shows the authors with the Count of fear words. plotEmotions = function(emotion,fillColor = fillColor2) { nrcEmotions = get_sentiments(&quot;nrc&quot;) %&gt;% filter(sentiment == emotion) train %&gt;% unnest_tokens(word, text) %&gt;% filter(!word %in% stop_words$word) %&gt;% inner_join(nrcEmotions) %&gt;% group_by(author) %&gt;% summarise(Count = n()) %&gt;% ungroup() %&gt;% mutate(author = reorder(author,Count)) %&gt;% ggplot(aes(x = author,y = Count)) + geom_bar(stat=&#39;identity&#39;,colour=&quot;white&quot;, fill =fillColor) + geom_text(aes(x = author, y = 1, label = paste0(&quot;(&quot;,Count,&quot;)&quot;,sep=&quot;&quot;)), hjust=0, vjust=.5, size = 4, colour = &#39;black&#39;, fontface = &#39;bold&#39;) + labs(x = &#39;author&#39;, y = &#39;Count&#39;, title = paste0(&#39;Author and &#39;,emotion,&#39; Words &#39;)) + coord_flip() + theme_bw() } plotEmotions(&quot;fear&quot;) 14.2 Fear Word Cloud - MWP The following table and word cloud shows the fear words written by MWP , the most fearful author. getEmotionalWords = function(emotion,author) { nrcEmotions = get_sentiments(&quot;nrc&quot;) %&gt;% filter(sentiment == emotion) emotionalWords = train %&gt;% unnest_tokens(word, text) %&gt;% filter(!word %in% stop_words$word) %&gt;% filter(author == author) %&gt;% inner_join(nrcEmotions) %&gt;% group_by(word) %&gt;% summarise(Count = n()) %&gt;% arrange(desc(Count)) return(emotionalWords) } FearWordsMWS = getEmotionalWords(&#39;fear&#39;,&#39;MWS&#39;) kable(head(FearWordsMWS),&quot;html&quot;) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;, &quot;responsive&quot;)) %&gt;% scroll_box(width = &quot;800px&quot;) word Count death 380 fear 240 horror 198 doubt 159 terrible 146 change 136 wordcloud(FearWordsMWS$word, FearWordsMWS$Count, max.words = 30,colors=brewer.pal(8, &quot;Dark2&quot;)) 14.3 Sentiment Analysis Words - Surprise The plot shows the authors with the Count of Surprise words. plotEmotions(&quot;surprise&quot;,fillColor) 14.4 Surprise Word Cloud - MWP The following table and word cloud shows the surprising words written by MWP , the most surprising author. SurpriseWordsMWS = getEmotionalWords(&#39;surprise&#39;,&#39;MWS&#39;) kable(head(SurpriseWordsMWS),&quot;html&quot;) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;, &quot;responsive&quot;)) %&gt;% scroll_box(width = &quot;800px&quot;) word Count death 380 horror 198 hope 195 sun 167 wild 157 suddenly 151 wordcloud(SurpriseWordsMWS$word, SurpriseWordsMWS$Count, max.words = 30,colors=brewer.pal(8, &quot;Dark2&quot;)) 14.5 Sentiment Analysis Words - Joy The plot shows the authors with the Count of Joy words. plotEmotions(&quot;joy&quot;,fillColor2) 14.6 Joy Word Cloud - MWP The following table and word cloud shows the joy words written by MWP , the most joy author. JoyWordsMWS = getEmotionalWords(&#39;joy&#39;,&#39;MWS&#39;) kable(head(JoyWordsMWS),&quot;html&quot;) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;, &quot;responsive&quot;)) %&gt;% scroll_box(width = &quot;800px&quot;) word Count found 559 love 331 friend 270 hope 195 sun 167 beauty 154 wordcloud(JoyWordsMWS$word, JoyWordsMWS$Count, max.words = 30,colors=brewer.pal(8, &quot;Dark2&quot;)) "],
["positive-and-not-so-postive-lines.html", "Chapter 15 Positive and Not So Postive Lines", " Chapter 15 Positive and Not So Postive Lines sentiment_lines = train %&gt;% unnest_tokens(word, text) %&gt;% inner_join(get_sentiments(&quot;afinn&quot;), by = &quot;word&quot;) %&gt;% group_by(id) %&gt;% summarize(sentiment = mean(score), words = n()) The sentences having top Ten positive sentiments are id sentiment words text author len id05517 5 1 The apartment was superb. EAP 25 id07548 5 1 My original soul seemed, at once, to take its flight from my body and a more than fiendish malevolence, gin nurtured, thrilled every fibre of my frame. EAP 151 id10196 5 1 “I thought so I knew it hurrah” vociferated Legrand, letting the negro go, and executing a series of curvets and caracols, much to the astonishment of his valet, who, arising from his knees, looked, mutely, from his master to myself, and then from myself to his master. EAP 269 id25394 5 1 “Superb physiologist” said the Westminster. EAP 43 id00752 4 1 Then I would hasten to my desk, weave the new found web of mind in firm texture and brilliant colours, leaving the fashioning of the material to a calmer moment. MWS 161 id00880 4 1 The Automaton does not invariably win the game. EAP 47 id01194 4 1 “Wonderful genius” said the Quarterly. EAP 38 id01391 4 1 With the quick sensibility peculiar to his temperament, he perceived his power in the brilliant circle to be on the wane. MWS 121 id01674 4 1 Ibid’s masterpiece, on the other hand, was the famous Op. HPL 57 id01902 4 1 The ‘Oil of Bob’ is the title of this masterpiece of eloquence and art. EAP 71 id01957 4 1 As soon as I sufficiently recovered my senses to comprehend the terrific predicament in which I stood or rather hung, I exerted all the power of my lungs to make that predicament known to the æronaut overhead. EAP 209 id02055 4 1 “Astonishingly,” said the second; “still quite a brilliant air, but art will do wonders. EAP 88 id02141 4 1 And now for the first time my memory records verbal discourse, Warren addressing me at length in his mellow tenor voice; a voice singularly unperturbed by our awesome surroundings. HPL 180 id02150 4 1 The general burst of terrific grandeur was all that I beheld. EAP 61 id03133 4 1 It appears to me a miracle of miracles that our enormous bulk is not swallowed up at once and forever. EAP 102 id04370 4 1 Phantasies such as these, presenting themselves at night, extended their terrific influence far into my waking hours. EAP 117 id04796 4 1 But here’s a funny thing. HPL 25 id04987 4 1 But his wife had said she found a funny tin thing in one of the beds when she fixed the rooms at noon, and maybe that was it. HPL 125 id05161 4 1 Some miracle might have produced it, yet the stages of the discovery were distinct and probable. MWS 96 id05414 4 1 Here he pointed to a fabulous creature of the artist, which one might describe as a sort of dragon with the head of an alligator. HPL 129 id05512 4 1 The starry sky, the sea, and every sight afforded by these wonderful regions seem still to have the power of elevating his soul from earth. MWS 139 id05994 4 1 Still others, including Joe himself, have theories too wild and fantastic for sober credence. HPL 93 id06517 4 1 The galvanic battery was applied, and he suddenly expired in one of those ecstatic paroxysms which, occasionally, it superinduces. EAP 130 id07409 4 1 Gilman came from Haverhill, but it was only after he had entered college in Arkham that he began to connect his mathematics with the fantastic legends of elder magic. HPL 166 id07919 4 1 There were secrets, said the peasants, which must not be uncovered; secrets that had lain hidden since the plague came to the children of Partholan in the fabulous years beyond history. HPL 185 id08642 4 1 But for one thing he would have been completely triumphant. MWS 59 id09430 4 1 At a terrific height directly above us, and upon the very verge of the precipitous descent, hovered a gigantic ship of, perhaps, four thousand tons. EAP 148 id10453 4 1 It is of a brilliant gold color about the size of a large hickory nut with two jet black spots near one extremity of the back, and another, somewhat longer, at the other. EAP 170 id10612 4 1 I heard of the slothful Asiatics, of the stupendous genius and mental activity of the Grecians, of the wars and wonderful virtue of the early Romans of their subsequent degenerating of the decline of that mighty empire, of chivalry, Christianity, and kings. MWS 257 id10741 4 1 I could scarcely contain my feelings of triumph. EAP 48 id11420 4 1 And thus were produced a multitude of gaudy and fantastic appearances. EAP 70 id12199 4 1 You never saw a more brilliant metallic lustre than the scales emit but of this you cannot judge till tomorrow. EAP 111 id12895 4 1 When rocked by the waves of the lake my spirits rose in triumph as a horseman feels with pride the motions of his high fed steed. MWS 129 id13372 4 2 The principle being discovered by which a machine can be made to play a game of chess, an extension of the same principle would enable it to win a game a farther extension would enable it to win all games that is, to beat any possible game of an antagonist. EAP 257 id14028 4 1 And at such moments was her beauty in my heated fancy thus it appeared perhaps the beauty of beings either above or apart from the earth the beauty of the fabulous Houri of the Turk. EAP 182 id14063 4 1 It is rumoured in Ulthar, beyond the river Skai, that a new king reigns on the opal throne in Ilek Vad, that fabulous town of turrets atop the hollow cliffs of glass overlooking the twilight sea wherein the bearded and finny Gnorri build their singular labyrinths, and I believe I know how to interpret this rumour. HPL 315 id14303 4 1 I displayed a peculiar erudition utterly unlike the fantastic, monkish lore over which I had pored in youth; and covered the flyleaves of my books with facile impromptu epigrams which brought up suggestions of Gay, Prior, and the sprightliest of the Augustan wits and rimesters. HPL 278 id14573 4 1 For your life you could not have found a fault with its wonderful proportion. EAP 77 id17723 4 1 Even Perdita will rejoice. MWS 26 id17854 4 1 In a moment of fantastic whim I whispered questions to the reddening ears; questions of other worlds of which the memory might still be present. HPL 144 id18191 4 1 In the midst of these reflections, as if dramatically arranged to intensify them, there fell near by a terrific bolt of lightning followed by the sound of sliding earth. HPL 169 id18900 4 1 Once a terrific flash and peal shook the frail house to its foundations, but the whisperer seemed not to notice it. HPL 115 id18936 4 1 Its productions and features may be without example, as the phenomena of the heavenly bodies undoubtedly are in those undiscovered solitudes. MWS 141 id19544 4 1 As he walked among other men he seemed encompassed with a heavenly halo that divided him from and lifted him above them. MWS 120 id19712 4 1 Thus it was that, by a master stroke of genius, I at length consummated my triumphs by “putting money in my purse,” and thus may be said really and fairly to have commenced that brilliant and eventful career which rendered me illustrious, and which now enables me to say, with Chateaubriand, “I have made history” “I’ai fait l’histoire.” EAP 337 id19994 4 1 The resources of his mind on this occasion were truly astonishing: his conversation was full of imagination; and very often, in imitation of the Persian and Arabic writers, he invented tales of wonderful fancy and passion. MWS 222 id20925 4 1 Wonderful likewise were the gardens made by Zokkar the olden king. HPL 66 id20938 4 1 A moment more and the old walls again met my sight, while over them hovered a murky cloud; fragments of buildings whirled above, half seen in smoke, while flames burst out beneath, and continued explosions filled the air with terrific thunders. MWS 244 id21073 4 1 The whole cohort now remained at a standstill, and as the torches faded I watched what I thought were fantastic shadows outlined in the sky by the spectral luminosity of the Via Lactea as it flowed through Perseus, Cassiopeia, Cepheus, and Cygnus. HPL 247 id21163 4 1 Then they all sprang at him and tore him to pieces before my eyes, bearing the fragments away into that subterranean vault of fabulous abominations. HPL 148 id21978 4 1 But more wonderful than the lore of old men and the lore of books is the secret lore of ocean. HPL 94 id22192 4 1 Overjoyed at this discovery, he hastened to the house, which was situated in a mean street near the Reuss. MWS 106 id22670 4 1 Only at the twelfth was the triumph complete. EAP 45 id22754 4 1 While they were talking Desrochers dropped in to say that he had heard a terrific clattering overhead in the dark small hours. HPL 126 id23837 4 1 I frequently engaged him in play, and contrived, with the gambler’s usual art, to let him win considerable sums, the more effectually to entangle him in my snares. EAP 163 id24090 4 1 Curtis Whateley was only just regaining consciousness when the Arkham men came slowly down the mountain in the beams of a sunlight once more brilliant and untainted. HPL 165 id24735 4 1 And yet I saw them in a limitless stream flopping, hopping, croaking, bleating surging inhumanly through the spectral moonlight in a grotesque, malignant saraband of fantastic nightmare. HPL 186 id27441 4 1 I will therefore guess even;’ he guesses even, and wins. EAP 56 id27698 4 1 Yet there have been many and wonderful automata. EAP 48 The sentences having top Ten NOT so positive sentiments are ## Joining, by = &quot;id&quot; id sentiment words text author len id05489 -5 1 We shall see that at which dogs howl in the dark, and that at which cats prick up their ears after midnight. HPL 108 id16045 -5 1 How did he know the time when Nahab and her acolyte were due to bear the brimming bowl which would follow the black cock and the black goat? HPL 140 id00307 -4 1 It rang on my ears long and heavily; the mountains re echoed it, and I felt as if all hell surrounded me with mockery and laughter. MWS 131 id00476 -4 1 and was he not consequently damned? EAP 35 id02649 -4 1 You have given me new wants and now your trifle with me as if my heart were as whole as yours, as if I were not in truth a shorn lamb thrust out on the bleak hill side, tortured by every blast. MWS 193 id06874 -4 1 The very beauty of the Grecian climate, during the season of spring, added torture to her sensations. MWS 101 id08709 -4 1 “Ass” said the fourth. EAP 22 id09729 -4 1 There was a secret which even torture could not extract. HPL 56 id11422 -4 1 My companion looked eagerly from one bed to the other, till at the end of the ward she espied, on a wretched bed, a squalid, haggard creature, writhing under the torture of disease. MWS 181 id11965 -4 1 Perdita, who then resided with Evadne, saw the torture that Adrian endured. MWS 75 id15587 -4 1 Raymond staggered forth from this scene, as a man might do, who had been just put to the torture, and looked forward to when it would be again inflicted. MWS 153 id16281 -4 1 I heard many things in hell. EAP 28 id16535 -4 1 I might be driven into the wide Atlantic and feel all the tortures of starvation or be swallowed up in the immeasurable waters that roared and buffeted around me. MWS 162 id16780 -4 1 The tortures endured, however, were indubitably quite equal for the time, to those of actual sepulture. EAP 103 id21653 -4 1 I don’t believe anybody since Goya could put so much of sheer hell into a set of features or a twist of expression. HPL 115 id21796 -4 1 This idea was torture to him. MWS 29 id22475 -4 1 In the former, the torture of meditation was excessive in the latter, supreme. EAP 78 id22542 -4 1 Molehills . . . the damned place must be honeycombed . . . HPL 58 id23474 -4 1 “The full moon damn ye ye . . . HPL 31 "],
["feature-sentiment-score.html", "Chapter 16 Feature Sentiment Score", " Chapter 16 Feature Sentiment Score We calculate the sentiment score for each line thru the following code. getSentimentScore = function(train) { sentiment_lines = train %&gt;% unnest_tokens(word, text) %&gt;% inner_join(get_sentiments(&quot;afinn&quot;), by = &quot;word&quot;) %&gt;% group_by(id) %&gt;% summarize(sentiment = mean(score)) sentiment_lines = sentiment_lines %&gt;% right_join(train, by = &quot;id&quot;) sentiment_lines = sentiment_lines %&gt;% mutate(sentiment = ifelse(is.na(sentiment),0,sentiment)) return(sentiment_lines$sentiment) } "],
["feature-nrc-sentiments.html", "Chapter 17 Feature NRC Sentiments", " Chapter 17 Feature NRC Sentiments We create the NRC Sentiments in the following section. We display the sentiment scores for Six lines of the train dataset. sentimentsTrain = get_nrc_sentiment(train$text) sentimentsTest = get_nrc_sentiment(test$text) kable(head(sentimentsTrain),&quot;html&quot;) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;, &quot;responsive&quot;)) %&gt;% scroll_box(width = &quot;800px&quot;) anger anticipation disgust fear joy sadness surprise trust negative positive 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 1 0 2 0 0 3 1 1 2 0 5 1 0 0 1 0 1 0 0 2 1 3 4 2 3 3 0 2 6 4 9 "],
["he-or-she-analysis.html", "Chapter 18 He or She Analysis 18.1 Gender associated verbs", " Chapter 18 He or She Analysis We examine the words which start with he or she. This section draws inspiration from the blog post by David Robinson in his writeup train %&gt;% unnest_tokens(bigram, text, token = &quot;ngrams&quot;, n = 2) %&gt;% separate(bigram, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) %&gt;% filter(word1 %in% c(&quot;he&quot;, &quot;she&quot;)) ## # A tibble: 5,680 x 5 ## id author len word1 word2 ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 id00004 EAP 134 he might ## 2 id00004 EAP 134 he necessarily ## 3 id00017 EAP 469 he makes ## 4 id00029 MWS 115 he entered ## 5 id00035 HPL 75 he was ## 6 id00036 HPL 201 he had ## 7 id00037 MWS 274 he owned ## 8 id00037 MWS 274 he the ## 9 id00043 HPL 167 he absorbed ## 10 id00045 MWS 237 he found ## # ... with 5,670 more rows 18.1 Gender associated verbs Which words were most shifted towards occurring after “he” or “she”? We’ll filter for words that appeared at least 20 times. train %&gt;% unnest_tokens(bigram, text, token = &quot;ngrams&quot;, n = 2) %&gt;% separate(bigram, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) %&gt;% filter(word1 %in% c(&quot;he&quot;, &quot;she&quot;)) %&gt;% count(word1,word2) %&gt;% spread(word1, n, fill = 0) %&gt;% mutate(total = he + she, he = (he + 1) / sum(he + 1), she = (she + 1) / sum(she + 1), log_ratio = log2(she / he), abs_ratio = abs(log_ratio)) %&gt;% arrange(desc(log_ratio)) %&gt;% filter(!word2 %in% c(&quot;himself&quot;, &quot;herself&quot;), !word2 %in% stop_words$word, total&gt;= 20) %&gt;% group_by(direction = ifelse(log_ratio &gt; 0, &#39;More &quot;she&quot;&#39;, &quot;More &#39;he&#39;&quot;)) %&gt;% top_n(15, abs_ratio) %&gt;% ungroup() %&gt;% mutate(word2 = reorder(word2, log_ratio)) %&gt;% ggplot(aes(word2, log_ratio, fill = direction)) + geom_col() + coord_flip() + labs(x = &quot;&quot;, y = &#39;Relative appearance after &quot;she&quot; compared to &quot;he&quot;&#39;, fill = &quot;&quot;, title = &quot;Gender associated with Verbs &quot;) + scale_y_continuous(labels = c(&quot;4X&quot;, &quot;2X&quot;, &quot;Same&quot;, &quot;2X&quot;), breaks = seq(-2, 1)) + guides(fill = guide_legend(reverse = TRUE)) + theme_bw() She cried , She loved , She died ,She heard is common while He told, He spoke, He sat, He wished , He found are common "],
["document-term-matrix.html", "Chapter 19 Document Term Matrix", " Chapter 19 Document Term Matrix Document Term Matrix creates a Bag of Words and does the following operations * make the words lower remove punctuation remove stopwords of English stem the bag of Words. The tm package provides the stemDocument() function to get to a word’s root makeDTM &lt;- function(train) { corpus = Corpus(VectorSource(train$text)) # Pre-process data corpus &lt;- tm_map(corpus, tolower) corpus &lt;- tm_map(corpus, removePunctuation) corpus &lt;- tm_map(corpus, removeWords, stopwords(&quot;english&quot;)) corpus &lt;- tm_map(corpus, stemDocument) dtm = DocumentTermMatrix(corpus) # Remove sparse terms dtm = removeSparseTerms(dtm, 0.997) # Create data frame labeledTerms = as.data.frame(as.matrix(dtm)) return(labeledTerms) } Topic modeling is a method for unsupervised classification of documents, similar to clustering on numeric data, which finds natural groups of items even when we’re not sure what we’re looking for. Latent Dirichlet allocation (LDA) is a particularly popular method for fitting a topic model. It treats each document as a mixture of topics, and each topic as a mixture of words. This allows documents to “overlap” each other in terms of content, rather than being separated into discrete groups, in a way that mirrors typical use of natural language. labeledTerms4LDA = makeDTM(train) labeledTerms4LDA = labeledTerms4LDA[rowSums(abs(labeledTerms4LDA)) != 0,] spooky_lda &lt;- LDA(labeledTerms4LDA, k = 9, control = list(seed = 13)) spooky_lda ## A LDA_VEM topic model with 9 topics. #The tidytext package provides this method for extracting the per-topic-per-word probabilities, # called β (“beta”), from the model spooky_topics &lt;- tidy(spooky_lda, matrix = &quot;beta&quot;) spooky_top_terms &lt;- spooky_topics %&gt;% group_by(topic) %&gt;% top_n(10, beta) %&gt;% ungroup() %&gt;% arrange(topic, -beta) spooky_top_terms %&gt;% mutate(term = reorder(term, beta)) %&gt;% ggplot(aes(term, beta, fill = factor(topic))) + geom_col(show.legend = FALSE) + facet_wrap(~ topic, scales = &quot;free&quot;) + coord_flip() + theme_bw() This visualization lets us understand the Nine topics that were extracted from the Words. Some of the words occur in more than one topic. "],
["modelling-with-xgboost.html", "Chapter 20 Modelling with XGBoost 20.1 Add features 20.2 Creating the XGBoost Model", " Chapter 20 Modelling with XGBoost We try to predict whether the lines are written by a specific author. We do Cross Validation using Caret package.Lastly we wish to examine the feature importance of the variables. This is shown in the flipped bar chart. We then use the model to predict the authors. makeFeatures &lt;- function(train) { labeledTerms = makeDTM(train) ## Preparing the features for the XGBoost Model features &lt;- colnames(labeledTerms) for (f in features) { if ((class(labeledTerms[[f]])==&quot;factor&quot;) || (class(labeledTerms[[f]])==&quot;character&quot;)) { levels &lt;- unique(labeledTerms[[f]]) labeledTerms[[f]] &lt;- as.numeric(factor(labeledTerms[[f]], levels=levels)) } } return(labeledTerms) } labeledTerms = makeFeatures(train) labeledTermsTest = makeFeatures(test) colnamesSame = intersect(colnames(labeledTerms),colnames(labeledTermsTest)) labeledTerms = labeledTerms[ , (colnames(labeledTerms) %in% colnamesSame)] labeledTermsTest = labeledTermsTest[ , (colnames(labeledTermsTest) %in% colnamesSame)] 20.1 Add features We add the following features to the model Number of words in the line Sentiment Score per line labeledTerms$len = train$len labeledTermsTest$len = test$len labeledTerms$sentiScore = getSentimentScore(train) labeledTermsTest$sentiScore = getSentimentScore(test) 20.2 Creating the XGBoost Model labeledTerms$author = as.factor(train$author) levels(labeledTerms$author) = make.names(unique(labeledTerms$author)) formula = author ~ . #Please uncomment if you want to do Cross Validation # fitControl &lt;- trainControl(method=&quot;cv&quot;,number = 5,classProbs=TRUE, summaryFunction=mnLogLoss) # # xgbGrid &lt;- expand.grid(nrounds = 500, # max_depth = 3, # eta = .05, # gamma = 0, # colsample_bytree = .8, # min_child_weight = 1, # subsample = 1) fitControl &lt;- trainControl(method=&quot;none&quot;,classProbs=TRUE, summaryFunction=mnLogLoss) xgbGrid &lt;- expand.grid(nrounds = 500, max_depth = 3, eta = .05, gamma = 0, colsample_bytree = .8, min_child_weight = 1, subsample = 1) set.seed(13) AuthorXGB = train(formula, data = labeledTerms, method = &quot;xgbTree&quot;,trControl = fitControl, tuneGrid = xgbGrid,na.action = na.pass,metric=&quot;LogLoss&quot;, maximize=FALSE) importance = varImp(AuthorXGB) varImportance &lt;- data.frame(Variables = row.names(importance[[1]]), Importance = round(importance[[1]]$Overall,2)) # Create a rank variable based on importance rankImportance &lt;- varImportance %&gt;% mutate(Rank = paste0(&#39;#&#39;,dense_rank(desc(Importance)))) %&gt;% head(20) rankImportancefull = rankImportance ggplot(rankImportance, aes(x = reorder(Variables, Importance), y = Importance)) + geom_bar(stat=&#39;identity&#39;,colour=&quot;white&quot;, fill = fillColor) + geom_text(aes(x = Variables, y = 1, label = Rank), hjust=0, vjust=.5, size = 4, colour = &#39;black&#39;, fontface = &#39;bold&#39;) + labs(x = &#39;Variables&#39;, title = &#39;Relative Variable Importance&#39;) + coord_flip() + theme_bw() AuthorXGB ## eXtreme Gradient Boosting ## ## 19579 samples ## 850 predictor ## 3 classes: &#39;EAP&#39;, &#39;HPL&#39;, &#39;MWS&#39; ## ## No pre-processing ## Resampling: None "],
["predictions-using-the-xgb-model.html", "Chapter 21 Predictions using the XGB Model", " Chapter 21 Predictions using the XGB Model predictions = predict(AuthorXGB,labeledTermsTest,type = &#39;prob&#39;) # Save the solution to a dataframe solution &lt;- data.frame(&#39;id&#39; = test$id, predictions) head(solution) ## id EAP HPL MWS ## 1 id02310 0.2020102 0.2493993 0.5485905 ## 2 id24541 0.4381515 0.3032377 0.2586108 ## 3 id00134 0.3396536 0.4587254 0.2016211 ## 4 id27757 0.4239962 0.3229545 0.2530494 ## 5 id04081 0.5903386 0.1773055 0.2323559 ## 6 id27337 0.3906093 0.3939233 0.2154674 # Write it to file write.csv(solution, &#39;XGBEDASpooky26Oct2017.csv&#39;, row.names = F) "],
["predictions-using-glmnet-model.html", "Chapter 22 Predictions using glmnet Model", " Chapter 22 Predictions using glmnet Model We predict using the glmnet model. AuthorGLM = train(formula, data = labeledTerms, method = &quot;glmnet&quot;,trControl = fitControl, na.action = na.pass,metric=&quot;LogLoss&quot;, maximize=FALSE) predictions = predict(AuthorGLM,labeledTermsTest,type = &#39;prob&#39;) # Save the solution to a dataframe solution &lt;- data.frame(&#39;id&#39; = test$id, predictions) head(solution) ## id EAP HPL MWS ## 1 id02310 0.1483739 0.05823763 0.793388478 ## 2 id24541 0.1367529 0.84022349 0.023023574 ## 3 id00134 0.4635034 0.53113157 0.005365013 ## 4 id27757 0.1716008 0.82497320 0.003425992 ## 5 id04081 0.7116491 0.10344372 0.184907176 ## 6 id27337 0.4591286 0.53811284 0.002758600 # Write it to file write.csv(solution, &#39;GLMNetEDASpooky29Oct2017.csv&#39;, row.names = F) "],
["modelling-using-the-text2vec-package.html", "Chapter 23 Modelling using the text2vec package 23.1 Inspect the vocabulary 23.2 Inspect the Document Term Matrix 23.3 Build the Multinomial Logistic Regression Model 23.4 Predict using the Multinomial Logistic Regression Model", " Chapter 23 Modelling using the text2vec package We create a vocabulary-based DTM. Here we collect unique terms from all documents and mark each of them with a unique ID using the create_vocabulary() function. We use an iterator to create the vocabulary. We also prune the vocabulary to reduce the terms in the matrix. prep_fun = function(x) { stringr::str_replace_all(tolower(x), &quot;[^[:alpha:]]&quot;, &quot; &quot;) } tok_fun = word_tokenizer it_train = itoken(train$text, preprocessor = prep_fun, tokenizer = tok_fun, ids = train$id, progressbar = FALSE) it_test = test$text %&gt;% prep_fun %&gt;% tok_fun %&gt;% itoken(ids = test$id, progressbar = FALSE) NFOLDS = 4 vocab = create_vocabulary(it_train, ngram = c(1L, 3L)) vocab = vocab %&gt;% prune_vocabulary(term_count_min = 10, doc_proportion_max = 0.5, doc_proportion_min = 0.01) trigram_vectorizer = vocab_vectorizer(vocab) dtm_train = create_dtm(it_train, trigram_vectorizer) dtm_test = create_dtm(it_test, trigram_vectorizer) 23.1 Inspect the vocabulary vocab ## Number of docs: 19579 ## 0 stopwords: ... ## ngram_min = 1; ngram_max = 3 ## Vocabulary: ## term term_count doc_count ## 1: black 198 196 ## 2: until 200 200 ## 3: over_the 201 198 ## 4: spirit 202 198 ## 5: itself 203 202 ## --- ## 325: was 6647 5493 ## 326: in 9458 7101 ## 327: a 10750 7507 ## 328: i 10811 7075 ## 329: to 12843 8665 23.2 Inspect the Document Term Matrix dim(dtm_train) ## [1] 19579 329 23.3 Build the Multinomial Logistic Regression Model dtm_train &lt;- cBind(train$len, dtm_train) dtm_test &lt;- cBind(test$len, dtm_test) glmnet_classifier = cv.glmnet(x = dtm_train, y = train[[&#39;author&#39;]], family = &#39;multinomial&#39;, alpha = 1, type.measure = &quot;class&quot;, nfolds = NFOLDS, thresh = 1e-3, maxit = 1e3) 23.4 Predict using the Multinomial Logistic Regression Model preds = data.frame(id=test$id,predict(glmnet_classifier, dtm_test, type = &#39;response&#39;)) names(preds)[2] &lt;- &quot;EAP&quot; names(preds)[3] &lt;- &quot;HPL&quot; names(preds)[4] &lt;- &quot;MWS&quot; write_csv(preds, &quot;glmnet_benchmark_vocab_3N-grams.csv&quot;) "],
["introduction-1.html", "Chapter 24 Introduction", " Chapter 24 Introduction This dataset is a subset of Yelp’s businesses, reviews, and user data. It was originally put together for the Yelp Dataset Challenge which is a chance for students to conduct research or analysis on Yelp’s data and share their discoveries. In the dataset you will find information about businesses across 11 metropolitan areas in four countries.We will do a detailed EDA, Text Mining ,Network Analysis and Geospatial Analysis on this dataset.The dataset can be found in Kaggle For a city we spot the most popular business and also provide a map of the city of Las vegas with the business identified as dots in the map. We have analysed Las Vegas , Toronto and Phoenix.For Phoenix city, we also do Word Cloud, detailed Sentiment Analysis and Topic Modelling. For a business we do the following analysis Word Cloud of the reviews of the business Top Ten most common Words reviews of the business Sentiment Analysis - Postive and Not So Postive Words of reviews Calculate Sentiment for the reviews Negative Reviews Positive Reviews Most Common Bigrams (a collection of Two words) in the review text Relationship among words Relationship of words with an important word in the review such as steak, crab, food Topic Modelling of the reviews The business that we are analysing are Mon Ami Gabi , a Las Vegas Restaurant , the most popular and highly rated restaurants Bacchanal Buffet , the Second most popular and highly rated Las Vegas Restaurant Pai Northern Thai Kitchen , the most popular Toronto restaurant Chipotle Business in Yonge Street Toronto You guessed it right I like Chipotle :-) How Sentiment Analysis can help your business ? For a business, the Sentiment Analysis is very important. If the business owners can just see the Top Ten negative reviews, they can easily find out which aspect of the business they need to improve. For example, the Pai Northern Thai Kitchen, the complaints were about Service. Now when we go deeper into the Service complaints, we can find out various aspects of the service complaints such as why our waitress seemed to be in such a hurry to get us out of the place. This restaurant was crowded and noisy. The tables were packed so closely that I was falling over other diners while maneuvering to my seat but their service was God-awful. They rarely attended our table, It took 55 minutes for our food to arrive. They took our drink orders and did not deliver them Another interesting complaint for the Chipotle Business in Yonge Street Toronto was that they did not accept Interac , a standard payment method in Canada Examples involving it are as follows Not complying with customers' choice to pay with Interac, a standard payment method in Canada, is also a nuisance Only reason it got a 4 star is because they don't accept interac which is my go to. How Topic Modelling can help understand your business and city ? Topic modelling helps to pick specific topics from the huge volume of text. Topic Modelling on the Three popular restaurants and also on Phoenix City helps us to understand that complaints regarding restaurants and business is around Service "],
["preparation.html", "Chapter 25 Preparation 25.1 Load Libraries 25.2 Read the data", " Chapter 25 Preparation 25.1 Load Libraries library(textcat) 25.2 Read the data rm(list=ls()) fillColor = &quot;#FFA07A&quot; fillColor2 = &quot;#F1C40F&quot; reviews &lt;- read_csv(&#39;../input/yelp_review.csv&#39;) business &lt;- read_csv(&quot;../input/yelp_business.csv&quot;) "],
["business-data.html", "Chapter 26 Business data", " Chapter 26 Business data datatable(head(business), style=&quot;bootstrap&quot;, class=&quot;table-condensed&quot;, options = list(dom = &#39;tp&#39;,scrollX = TRUE)) "],
["reviews-data.html", "Chapter 27 Reviews data", " Chapter 27 Reviews data A glimpse of the reviews data glimpse(reviews) "],
["detecting-the-language-of-the-reviews.html", "Chapter 28 Detecting the language of the reviews", " Chapter 28 Detecting the language of the reviews Detecting the language of the first Ten reviews. textcat(reviews[1:10,]$text) "],
["most-popular-categories.html", "Chapter 29 Most Popular Categories", " Chapter 29 Most Popular Categories The most popular categories of business are plotted in the bar plot categories = str_split(business$categories,&quot;;&quot;) categories = as.data.frame(unlist(categories)) colnames(categories) = c(&quot;Name&quot;) categories %&gt;% group_by(Name) %&gt;% summarise(Count = n()) %&gt;% arrange(desc(Count)) %&gt;% ungroup() %&gt;% mutate(Name = reorder(Name,Count)) %&gt;% head(10) %&gt;% ggplot(aes(x = Name,y = Count)) + geom_bar(stat=&#39;identity&#39;,colour=&quot;white&quot;, fill =fillColor2) + geom_text(aes(x = Name, y = 1, label = paste0(&quot;(&quot;,Count,&quot;)&quot;,sep=&quot;&quot;)), hjust=0, vjust=.5, size = 4, colour = &#39;black&#39;, fontface = &#39;bold&#39;) + labs(x = &#39;Name of Category&#39;, y = &#39;Count&#39;, title = &#39;Top 10 Categories of Business&#39;) + coord_flip() + theme_bw() "],
["top-ten-cities-with-the-most-business-parties-mentioned-in-yelp.html", "Chapter 30 Top Ten Cities with the most Business parties mentioned in Yelp", " Chapter 30 Top Ten Cities with the most Business parties mentioned in Yelp We show the Top Ten Cities which has the most Business parties mentioned in Yelp business %&gt;% group_by(city) %&gt;% summarise(Count = n()) %&gt;% arrange(desc(Count)) %&gt;% ungroup() %&gt;% mutate(City = reorder(city,Count)) %&gt;% head(10) %&gt;% ggplot(aes(x = City,y = Count)) + geom_bar(stat=&#39;identity&#39;,colour=&quot;white&quot;, fill =fillColor) + geom_text(aes(x = City, y = 1, label = paste0(&quot;(&quot;,round(Count/1e3),&quot; K )&quot;,sep=&quot;&quot;)), hjust=0, vjust=.5, size = 4, colour = &#39;black&#39;, fontface = &#39;bold&#39;) + labs(x = &#39;City&#39;, y = &#39;Count of Reviews&#39;, title = &#39;Top Ten Cities with the most Business parties in Yelp&#39;) + coord_flip() + theme_bw() "],
["map-of-the-business-parties-in-las-vegas.html", "Chapter 31 Map of the business parties in Las vegas", " Chapter 31 Map of the business parties in Las vegas Seems from the map that most of the business is in the neighborhood of The Strip in Las Vagas. From Wikipedia The Las Vegas Strip is a stretch of South Las Vegas Boulevard in Clark County, Nevada that is known for its concentration of resort hotels and casinos. The Strip is approximately 4.2 miles (6.8 km) in length,[1] located immediately south of the Las Vegas city limits in the unincorporated towns of Paradise and Winchester. LasvegasCoords = business %&gt;% filter(city == &quot;Las Vegas&quot;) center_lon = median(LasvegasCoords$longitude,na.rm = TRUE) center_lat = median(LasvegasCoords$latitude,na.rm = TRUE) leaflet(LasvegasCoords) %&gt;% addProviderTiles(&quot;Esri.NatGeoWorldMap&quot;) %&gt;% addCircles(lng = ~longitude, lat = ~latitude,radius = ~sqrt(review_count)) %&gt;% # controls setView(lng=center_lon, lat=center_lat,zoom = 13) "],
["business-with-most-five-star-reviews-from-users.html", "Chapter 32 Business with most Five Star Reviews from Users", " Chapter 32 Business with most Five Star Reviews from Users The following plot shows the names of business with the most Five Star Reviews.Mon Ami Gabi and Bacchanal Buffet are the Two most popular restaurants from the Yelp reviews with Five Star ratings. We will do a deep dive for these restaurants. most5StarsReviews = reviews %&gt;% filter(stars == 5) %&gt;% group_by(business_id) %&gt;% summarise(Count = n()) %&gt;% arrange(desc(Count)) %&gt;% ungroup() %&gt;% mutate(BusinessID = reorder(business_id,Count)) %&gt;% head(10) most5StarsReviews = inner_join(most5StarsReviews,business) most5StarsReviews %&gt;% mutate(name = reorder(name,Count)) %&gt;% ggplot(aes(x = name,y = Count)) + geom_bar(stat=&#39;identity&#39;,colour=&quot;white&quot;, fill = fillColor) + geom_text(aes(x = name, y = 1, label = paste0(&quot;(&quot;,Count,&quot;)&quot;,sep=&quot;&quot;)), hjust=0, vjust=.5, size = 4, colour = &#39;black&#39;, fontface = &#39;bold&#39;) + labs(x = &#39;Name of the Business&#39;, y = &#39;Count&#39;, title = &#39;Name of the Business and Count&#39;) + coord_flip() + theme_bw() "],
["mon-ami-gabi.html", "Chapter 33 “Mon Ami Gabi” 33.1 Useful,funny,cool reviews 33.2 Word Cloud of Mon Ami Gabi 33.3 Top Ten most common Words of the business “Mon Ami Gabi” 33.4 Sentiment Analysis - Postive and Not So Postive Words of “Mon Ami Gabi” 33.5 Calculate Sentiment for the reviews 33.6 Negative Reviews 33.7 Positive Reviews 33.8 Most Common Bigrams of “Mon Ami Gabi” 33.9 Relationship among words", " Chapter 33 “Mon Ami Gabi” The location and category of the most liked business Mon Ami Gabi is shown below mon_ami_gabi = business %&gt;% filter(business_id == &quot;4JNXUYY8wbaaDmk3BPzlWw&quot;) %&gt;% select(name,neighborhood,city,state,postal_code,categories) datatable(head(mon_ami_gabi), style=&quot;bootstrap&quot;, class=&quot;table-condensed&quot;, options = list(dom = &#39;tp&#39;,scrollX = TRUE)) 33.1 Useful,funny,cool reviews The following plot describes the number of Useful, Funny and Cool reviews.Most of the reviews are NOT useful , funny or cool. mon_ami_gabi_reviews = reviews %&gt;% filter(business_id == &quot;4JNXUYY8wbaaDmk3BPzlWw&quot;) mon_ami_gabi_reviews %&gt;% group_by(useful) %&gt;% summarise(Count = n()) %&gt;% arrange(desc(Count)) %&gt;% ungroup() %&gt;% mutate(useful = reorder(useful,Count)) %&gt;% head(10) %&gt;% ggplot(aes(x = useful,y = Count)) + geom_bar(stat=&#39;identity&#39;,colour=&quot;white&quot;, fill = fillColor) + geom_text(aes(x = useful, y = 1, label = paste0(&quot;(&quot;,Count,&quot;)&quot;,sep=&quot;&quot;)), hjust=0, vjust=.5, size = 4, colour = &#39;black&#39;, fontface = &#39;bold&#39;) + labs(x = &#39;Useful Reviews&#39;, y = &#39;Count&#39;, title = &#39;Useful Reviews and Count&#39;) + coord_flip() + theme_bw() mon_ami_gabi_reviews %&gt;% group_by(funny) %&gt;% summarise(Count = n()) %&gt;% arrange(desc(Count)) %&gt;% ungroup() %&gt;% mutate(funny = reorder(funny,Count)) %&gt;% head(10) %&gt;% ggplot(aes(x = funny,y = Count)) + geom_bar(stat=&#39;identity&#39;,colour=&quot;white&quot;, fill = fillColor2) + geom_text(aes(x = funny, y = 1, label = paste0(&quot;(&quot;,Count,&quot;)&quot;,sep=&quot;&quot;)), hjust=0, vjust=.5, size = 4, colour = &#39;black&#39;, fontface = &#39;bold&#39;) + labs(x = &#39;Funny Reviews&#39;, y = &#39;Count&#39;, title = &#39;Funny Reviews and Count&#39;) + coord_flip() + theme_bw() mon_ami_gabi_reviews %&gt;% group_by(cool) %&gt;% summarise(Count = n()) %&gt;% arrange(desc(Count)) %&gt;% ungroup() %&gt;% mutate(cool = reorder(cool,Count)) %&gt;% head(10) %&gt;% ggplot(aes(x = cool,y = Count)) + geom_bar(stat=&#39;identity&#39;,colour=&quot;white&quot;, fill = fillColor) + geom_text(aes(x = cool, y = 1, label = paste0(&quot;(&quot;,Count,&quot;)&quot;,sep=&quot;&quot;)), hjust=0, vjust=.5, size = 4, colour = &#39;black&#39;, fontface = &#39;bold&#39;) + labs(x = &#39;Cool Reviews&#39;, y = &#39;Count&#39;, title = &#39;Cool Reviews and Count&#39;) + coord_flip() + theme_bw() 33.2 Word Cloud of Mon Ami Gabi A word cloud is a graphical representation of frequently used words in the text. The height of each word in this picture is an indication of frequency of occurrence of the word in the entire text. The words steak, service, vegas,french,patio,bellagio,delicious, nice are the words which have been used very frequently in the reviews.Note that if we choose a word which is not food related , it is Service and we will see in the subsequent sections of sentiment analysis and topic modelling , why this keyword is important. createWordCloud = function(train) { train %&gt;% unnest_tokens(word, text) %&gt;% filter(!word %in% stop_words$word) %&gt;% count(word,sort = TRUE) %&gt;% ungroup() %&gt;% head(30) %&gt;% with(wordcloud(word, n, max.words = 30,colors=brewer.pal(8, &quot;Dark2&quot;))) } createWordCloud(reviews %&gt;% filter(business_id == &quot;4JNXUYY8wbaaDmk3BPzlWw&quot;)) 33.3 Top Ten most common Words of the business “Mon Ami Gabi” We examine the Top Ten Most Common words and show them in a bar graph. The words steak, service, vegas,french,patio,bellagio,delicious, nice are the words which have been used very frequently in the reviews. reviews %&gt;% filter(business_id == &quot;4JNXUYY8wbaaDmk3BPzlWw&quot;) %&gt;% unnest_tokens(word, text) %&gt;% filter(!word %in% stop_words$word) %&gt;% filter(!word %in% c(&#39;food&#39;,&#39;restaurant&#39;)) %&gt;% count(word,sort = TRUE) %&gt;% ungroup() %&gt;% mutate(word = factor(word, levels = rev(unique(word)))) %&gt;% head(10) %&gt;% ggplot(aes(x = word,y = n)) + geom_bar(stat=&#39;identity&#39;,colour=&quot;white&quot;, fill =fillColor) + geom_text(aes(x = word, y = 1, label = paste0(&quot;(&quot;,n,&quot;)&quot;,sep=&quot;&quot;)), hjust=0, vjust=.5, size = 4, colour = &#39;black&#39;, fontface = &#39;bold&#39;) + labs(x = &#39;Word&#39;, y = &#39;Word Count&#39;, title = &#39;Word Count&#39;) + coord_flip() + theme_bw() 33.4 Sentiment Analysis - Postive and Not So Postive Words of “Mon Ami Gabi” We display the Positive and Not So Positive words used by reviewers for the business Mon Ami Gabi.We have used the AFINN sentiment lexicon, which provides numeric positivity scores for each word, and visualize it with a bar plot. Breathtaking,funnier,fun,fantastic,fabulous,ecstatic,brilliant,awesome,amazing are some of the postive words that we have seen in the reviews of the business. positiveWordsBarGraph &lt;- function(SC) { contributions &lt;- SC %&gt;% unnest_tokens(word, text) %&gt;% count(word,sort = TRUE) %&gt;% ungroup() %&gt;% inner_join(get_sentiments(&quot;afinn&quot;), by = &quot;word&quot;) %&gt;% group_by(word) %&gt;% summarize(occurences = n(), contribution = sum(score)) contributions %&gt;% top_n(20, abs(contribution)) %&gt;% mutate(word = reorder(word, contribution)) %&gt;% head(20) %&gt;% ggplot(aes(word, contribution, fill = contribution &gt; 0)) + geom_col(show.legend = FALSE) + coord_flip() + theme_bw() } positiveWordsBarGraph(reviews %&gt;% filter(business_id == &quot;4JNXUYY8wbaaDmk3BPzlWw&quot;)) 33.5 Calculate Sentiment for the reviews We calculate the sentiment scores for all the reviews using the AFINN sentiment lexicon. We display the Top Six sentiments here. calculate_sentiment &lt;- function(review_text) { sentiment_lines = review_text %&gt;% filter(textcat(text) == &quot;english&quot;) %&gt;% # considering only English text unnest_tokens(word, text) %&gt;% inner_join(get_sentiments(&quot;afinn&quot;), by = &quot;word&quot;) %&gt;% group_by(review_id) %&gt;% summarize(sentiment = mean(score),words = n()) %&gt;% ungroup() %&gt;% filter(words &gt;= 5) return(sentiment_lines) } sentiment_lines = calculate_sentiment(mon_ami_gabi_reviews) head(sentiment_lines) 33.6 Negative Reviews We examine the Top Ten most negative reviews. The complaints were about Service. An excerpt of the Service Complaints is provided below Worst service ever. Didn't pay attention to our orders at all so we had to send most of the food back The server ignored us twice when we are talking to him. Threw the dishes instead of placing them on the table The service was mediocre and the food was terrible Food was OK, but service was terrible. Our server never came back to our table to check if we need another drink, water, bread, etc. We had to get somebody else's attention for our need. At the end, they included 18% tipping which is their policy for 5 or more people display_neg_sentiments &lt;- function(sentiment_lines,review_text) { neg_sentiment_lines = sentiment_lines %&gt;% arrange(desc(sentiment)) %&gt;% top_n(-10, sentiment) %&gt;% inner_join(review_text, by = &quot;review_id&quot;) %&gt;% select(date,sentiment,text) datatable(neg_sentiment_lines, style=&quot;bootstrap&quot;, class=&quot;table-condensed&quot;, options = list(dom = &#39;tp&#39;,scrollX = TRUE)) } display_neg_sentiments(sentiment_lines,mon_ami_gabi_reviews) 33.7 Positive Reviews We examine the Top Ten most positive reviews. display_pos_sentiments &lt;- function(sentiment_lines,review_text) { pos_sentiment_lines = sentiment_lines %&gt;% arrange(desc(sentiment)) %&gt;% top_n(10, sentiment) %&gt;% inner_join(review_text, by = &quot;review_id&quot;) %&gt;% select(date,sentiment,text) datatable(pos_sentiment_lines, style=&quot;bootstrap&quot;, class=&quot;table-condensed&quot;, options = list(dom = &#39;tp&#39;,scrollX = TRUE)) } display_pos_sentiments(sentiment_lines,mon_ami_gabi_reviews) 33.8 Most Common Bigrams of “Mon Ami Gabi” A Bigram is a collection of Two words. We examine the most common Bigrams and plot them in a bar plot. count_bigrams &lt;- function(dataset) { dataset %&gt;% unnest_tokens(bigram, text, token = &quot;ngrams&quot;, n = 2) %&gt;% separate(bigram, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) %&gt;% filter(!word1 %in% stop_words$word, !word2 %in% stop_words$word) %&gt;% count(word1, word2, sort = TRUE) } visualize_bigrams &lt;- function(bigrams) { set.seed(2016) a &lt;- grid::arrow(type = &quot;closed&quot;, length = unit(.15, &quot;inches&quot;)) bigrams %&gt;% graph_from_data_frame() %&gt;% ggraph(layout = &quot;fr&quot;) + geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a) + geom_node_point(color = &quot;lightblue&quot;, size = 5) + geom_node_text(aes(label = name), vjust = 1, hjust = 1) + theme_void() } visualize_bigrams_individual &lt;- function(bigrams) { set.seed(2016) a &lt;- grid::arrow(type = &quot;closed&quot;, length = unit(.15, &quot;inches&quot;)) bigrams %&gt;% graph_from_data_frame() %&gt;% ggraph(layout = &quot;fr&quot;) + geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a,end_cap = circle(.07, &#39;inches&#39;)) + geom_node_point(color = &quot;lightblue&quot;, size = 5) + geom_node_text(aes(label = name), vjust = 1, hjust = 1) + theme_void() } reviews %&gt;% filter(business_id == &quot;4JNXUYY8wbaaDmk3BPzlWw&quot;) %&gt;% unnest_tokens(bigram, text, token = &quot;ngrams&quot;, n = 2) %&gt;% select(bigram,review_id) %&gt;% head(10) reviews %&gt;% filter(business_id == &quot;4JNXUYY8wbaaDmk3BPzlWw&quot;) %&gt;% unnest_tokens(bigram, text, token = &quot;ngrams&quot;, n = 2) %&gt;% separate(bigram, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) %&gt;% filter(!word1 %in% stop_words$word, !word2 %in% stop_words$word) %&gt;% filter(!word1 %in% c(&quot;mon&quot;,&quot;ami&quot;)) %&gt;% filter(!word2 %in% c(&quot;gabi&quot;)) %&gt;% unite(bigramWord, word1, word2, sep = &quot; &quot;) %&gt;% group_by(bigramWord) %&gt;% tally() %&gt;% ungroup() %&gt;% arrange(desc(n)) %&gt;% mutate(bigramWord = reorder(bigramWord,n)) %&gt;% head(10) %&gt;% ggplot(aes(x = bigramWord,y = n)) + geom_bar(stat=&#39;identity&#39;,colour=&quot;white&quot;, fill = fillColor2) + geom_text(aes(x = bigramWord, y = 1, label = paste0(&quot;(&quot;,n,&quot;)&quot;,sep=&quot;&quot;)), hjust=0, vjust=.5, size = 4, colour = &#39;black&#39;, fontface = &#39;bold&#39;) + labs(x = &#39;Bigram&#39;, y = &#39;Count&#39;, title = &#39;Bigram and Count&#39;) + coord_flip() + theme_bw() 33.9 Relationship among words We explore the different relationship among the various words in Mon Ami Gabi reviews here through a network graph bigramsMonAmiGabi &lt;- reviews %&gt;% filter(business_id == &quot;4JNXUYY8wbaaDmk3BPzlWw&quot;) %&gt;% count_bigrams() bigramsMonAmiGabi %&gt;% filter(n &gt; 50) %&gt;% visualize_bigrams() 33.9.1 Relationship of words with steak The following network diagram shows the words associated with the word steak bigramsMonAmiGabi %&gt;% filter(word1 == &quot;steak&quot; | word2 == &quot;steak&quot;) %&gt;% filter(n &gt; 30) %&gt;% visualize_bigrams() 33.9.2 Relationship of words with french The following network diagram shows the words associated with the word french bigramsMonAmiGabi %&gt;% filter(word1 == &quot;french&quot; | word2 == &quot;french&quot; ) %&gt;% filter(n &gt; 30) %&gt;% visualize_bigrams() "],
["bacchanal-buffet.html", "Chapter 34 Bacchanal Buffet 34.1 Word Cloud of Bacchanal Buffet 34.2 Top Ten most common Words of the business “Bacchanal Buffet” 34.3 Sentiment Analysis - Postive and Not So Postive Words of Bacchanal Buffet 34.4 Calculate Sentiment for the reviews 34.5 Negative Reviews 34.6 Positive Reviews 34.7 Relationship among words in Bacchanal Buffet", " Chapter 34 Bacchanal Buffet The location and category of the most liked business Bacchanal Buffet is shown below bacchanal = business %&gt;% filter(business_id == &quot;RESDUcs7fIiihp38-d6_6g&quot;) %&gt;% select(name,neighborhood,city,state,postal_code,categories) datatable(head(bacchanal), style=&quot;bootstrap&quot;, class=&quot;table-condensed&quot;, options = list(dom = &#39;tp&#39;,scrollX = TRUE)) 34.1 Word Cloud of Bacchanal Buffet bacchanal = reviews %&gt;% filter(business_id == &quot;RESDUcs7fIiihp38-d6_6g&quot;) createWordCloud(bacchanal) 34.2 Top Ten most common Words of the business “Bacchanal Buffet” We examine the Top Ten Most Common words and show them in a bar graph. bacchanal %&gt;% unnest_tokens(word, text) %&gt;% filter(!word %in% stop_words$word) %&gt;% filter(!word %in% c(&#39;food&#39;,&#39;restaurant&#39;)) %&gt;% count(word,sort = TRUE) %&gt;% ungroup() %&gt;% mutate(word = factor(word, levels = rev(unique(word)))) %&gt;% head(10) %&gt;% ggplot(aes(x = word,y = n)) + geom_bar(stat=&#39;identity&#39;,colour=&quot;white&quot;, fill =fillColor) + geom_text(aes(x = word, y = 1, label = paste0(&quot;(&quot;,n,&quot;)&quot;,sep=&quot;&quot;)), hjust=0, vjust=.5, size = 4, colour = &#39;black&#39;, fontface = &#39;bold&#39;) + labs(x = &#39;Word&#39;, y = &#39;Word Count&#39;, title = &#39;Word Count&#39;) + coord_flip() + theme_bw() 34.3 Sentiment Analysis - Postive and Not So Postive Words of Bacchanal Buffet We display the Positive and Not So Positive words used by reviewers for the business Bacchanal Buffet.We have used the AFINN sentiment lexicon, which provides numeric positivity scores for each word, and visualize it with a bar plot. positiveWordsBarGraph(bacchanal) 34.4 Calculate Sentiment for the reviews We calculate the sentiment scores for all the reviews using the AFINN sentiment lexicon. We display the Top Six sentiments here. sentiment_lines = calculate_sentiment(bacchanal) head(sentiment_lines) 34.5 Negative Reviews We examine the Top Ten most negative reviews.We examine the Top Ten most negative reviews. The complaints were about Service,waiting,decor. An excerpt of the Service Complaints is provided below this place sucks so fucking bad. We are waiting in line for almost one hour. They only let VIP members taking the available sits as soon as possible Stupid system!!!! Their ticketing idea sucks and defeats the purpose of having it at all!!! Service sucks! Server didn't even bother to check our table. I have to set a side my dirty plates on the other side of my table to be able have a space on our table display_neg_sentiments(sentiment_lines,bacchanal) 34.6 Positive Reviews We examine the Top Ten most postive reviews. display_pos_sentiments(sentiment_lines,bacchanal) 34.7 Relationship among words in Bacchanal Buffet We explore the different relationship among the various words in Bacchanal Buffet here through a network graph bigrams_bacchanal &lt;- bacchanal %&gt;% count_bigrams() bigrams_bacchanal %&gt;% filter(n &gt; 100) %&gt;% visualize_bigrams() 34.7.1 Relationship of words with crab The following network diagram shows the words associated with the word crab bigramsMonAmiGabi %&gt;% filter(word1 == &quot;crab&quot; | word2 == &quot;crab&quot; ) %&gt;% visualize_bigrams() 34.7.2 Relationship of words with food The following network diagram shows the words associated with the word food bigramsMonAmiGabi %&gt;% filter(word1 == &quot;food&quot; | word2 == &quot;food&quot; ) %&gt;% filter(n &gt; 10) %&gt;% visualize_bigrams() "],
["top-ten-business-in-toronto.html", "Chapter 35 Top Ten Business in Toronto", " Chapter 35 Top Ten Business in Toronto We list the Top Ten business in Toronto giving importance to the number of reviews and then to the number of stars obtained by the business. toronto_biz = business %&gt;% filter(city == &quot;Toronto&quot;) %&gt;% arrange(desc(review_count,stars)) %&gt;% select(name,neighborhood,address,review_count,stars) %&gt;% head(10) datatable(toronto_biz, style=&quot;bootstrap&quot;, class=&quot;table-condensed&quot;, options = list(dom = &#39;tp&#39;,scrollX = TRUE)) "],
["pai-northern-thai-kitchen.html", "Chapter 36 Pai Northern Thai Kitchen 36.1 Word Cloud of business Pai Northern Thai Kitchen 36.2 Ten most common words used in reviews of business Pai Northern Thai Kitchen 36.3 Sentiment Analysis - Postive and Not So Postive Words of Pai Northern Thai Kitchen 36.4 Calculate Sentiment for the reviews 36.5 Negative Reviews 36.6 Positive Reviews 36.7 Relationship among words in Pai Northern Thai Kitchen", " Chapter 36 Pai Northern Thai Kitchen 36.1 Word Cloud of business Pai Northern Thai Kitchen #r_BrIgzYcwo1NAuG9dLbpg createWordCloud(reviews %&gt;% filter(business_id == &quot;r_BrIgzYcwo1NAuG9dLbpg&quot;)) 36.2 Ten most common words used in reviews of business Pai Northern Thai Kitchen We examine the Top Ten Most Common words and show them in a bar graph. reviews %&gt;% filter(business_id == &quot;r_BrIgzYcwo1NAuG9dLbpg&quot;) %&gt;% unnest_tokens(word, text) %&gt;% filter(!word %in% stop_words$word) %&gt;% filter(!word %in% c(&#39;food&#39;,&#39;restaurant&#39;)) %&gt;% count(word,sort = TRUE) %&gt;% ungroup() %&gt;% mutate(word = factor(word, levels = rev(unique(word)))) %&gt;% head(10) %&gt;% ggplot(aes(x = word,y = n)) + geom_bar(stat=&#39;identity&#39;,colour=&quot;white&quot;, fill =fillColor) + geom_text(aes(x = word, y = 1, label = paste0(&quot;(&quot;,n,&quot;)&quot;,sep=&quot;&quot;)), hjust=0, vjust=.5, size = 4, colour = &#39;black&#39;, fontface = &#39;bold&#39;) + labs(x = &#39;Word&#39;, y = &#39;Word Count&#39;, title = &#39;Word Count&#39;) + coord_flip() + theme_bw() 36.3 Sentiment Analysis - Postive and Not So Postive Words of Pai Northern Thai Kitchen We display the Positive and Not So Positive words used by reviewers for the business Pai Northern Thai Kitchen.We have used the AFINN sentiment lexicon, which provides numeric positivity scores for each word, and visualize it with a bar plot. positiveWordsBarGraph(reviews %&gt;% filter(business_id == &quot;r_BrIgzYcwo1NAuG9dLbpg&quot;)) 36.4 Calculate Sentiment for the reviews We calculate the sentiment scores for all the reviews using the AFINN sentiment lexicon. We display the Top Six sentiments here. pai_thai = reviews %&gt;% filter(business_id == &quot;r_BrIgzYcwo1NAuG9dLbpg&quot;) sentiment_lines = calculate_sentiment(pai_thai) head(sentiment_lines) 36.5 Negative Reviews We examine the Top 10 most negative reviews.An analysis of the negative reviews reveals that the complaints were about Service. Now when we go deeper into the Service complaints, we can find out various aspects of the service complaints such as why our waitress seemed to be in such a hurry to get us out of the place. This restaurant was crowded and noisy. The tables were packed so closely that I was falling over other diners while maneuvering to my seat but their service was God-awful. They rarely attended our table, It took 55 minutes for our food to arrive. They took our drink orders and did not deliver them display_neg_sentiments(sentiment_lines,pai_thai) 36.6 Positive Reviews We examine the Top Ten most postive reviews. display_pos_sentiments(sentiment_lines,pai_thai) 36.7 Relationship among words in Pai Northern Thai Kitchen We explore the different relationship among the various words in *Pai Northern Thai Kitchen here through a network graph bigrams_thai &lt;- reviews %&gt;% filter(business_id == &quot;r_BrIgzYcwo1NAuG9dLbpg&quot;) %&gt;% count_bigrams() bigrams_thai %&gt;% filter(n &gt; 50) %&gt;% visualize_bigrams() 36.7.1 Relationship of words with thai The following network diagram shows the words associated with the word thai bigrams_thai %&gt;% filter(word1 == &quot;thai&quot; | word2 == &quot;thai&quot; ) %&gt;% filter(n &gt; 5) %&gt;% visualize_bigrams() "],
["chipotle-business.html", "Chapter 37 Chipotle business", " Chapter 37 Chipotle business We explore the various Chipotle business chipotle_biz = business %&gt;% filter(str_detect(name,&quot;Chipotle&quot;) )%&gt;% arrange(desc(review_count,stars)) datatable(head(chipotle_biz), style=&quot;bootstrap&quot;, class=&quot;table-condensed&quot;, options = list(dom = &#39;tp&#39;,scrollX = TRUE)) "],
["chipotle-business-in-yonge-street-toronto.html", "Chapter 38 Chipotle Business in Yonge Street Toronto 38.1 Word Cloud of business Chipotle Business in Yonge Street Toronto 38.2 Top Ten most common Words of the business “Chipotle Business in Yonge Street Toronto” 38.3 Sentiment Analysis - Postive and Not So Postive Words of Chipotle Business in Yonge Street Toronto 38.4 Calculate Sentiment for the reviews 38.5 Negative Reviews 38.6 Positive Reviews 38.7 Relationship among words in Chipotle Business in Yonge Street Toronto", " Chapter 38 Chipotle Business in Yonge Street Toronto We explore in detail the Chipotle business in Yonge Street Toronto since this has recived the highest number of reviews among the Chipotle business. 38.1 Word Cloud of business Chipotle Business in Yonge Street Toronto #gOBxVkHpqtjRRxHBIrpnMA chioptle_yonge = reviews %&gt;% filter(business_id == &quot;gOBxVkHpqtjRRxHBIrpnMA&quot;) createWordCloud(chioptle_yonge) 38.2 Top Ten most common Words of the business “Chipotle Business in Yonge Street Toronto” We examine the Top Ten Most Common words and show them in a bar graph. chioptle_yonge %&gt;% unnest_tokens(word, text) %&gt;% filter(!word %in% stop_words$word) %&gt;% filter(!word %in% c(&#39;food&#39;,&#39;restaurant&#39;)) %&gt;% count(word,sort = TRUE) %&gt;% ungroup() %&gt;% mutate(word = factor(word, levels = rev(unique(word)))) %&gt;% head(10) %&gt;% ggplot(aes(x = word,y = n)) + geom_bar(stat=&#39;identity&#39;,colour=&quot;white&quot;, fill =fillColor) + geom_text(aes(x = word, y = 1, label = paste0(&quot;(&quot;,n,&quot;)&quot;,sep=&quot;&quot;)), hjust=0, vjust=.5, size = 4, colour = &#39;black&#39;, fontface = &#39;bold&#39;) + labs(x = &#39;Word&#39;, y = &#39;Word Count&#39;, title = &#39;Word Count&#39;) + coord_flip() + theme_bw() 38.3 Sentiment Analysis - Postive and Not So Postive Words of Chipotle Business in Yonge Street Toronto We display the Positive and Not So Positive words used by reviewers for the business Chipotle Business in Yonge Street Toronto.We have used the AFINN sentiment lexicon, which provides numeric positivity scores for each word, and visualize it with a bar plot. positiveWordsBarGraph(chioptle_yonge) 38.4 Calculate Sentiment for the reviews We calculate the sentiment scores for all the reviews using the AFINN sentiment lexicon. We display the Top Six sentiments here. sentiment_lines = calculate_sentiment(chioptle_yonge) head(sentiment_lines) 38.5 Negative Reviews We examine the Top Ten most negative reviews.An interesting complaint for the Chipotle Business in Yonge Street Toronto was that they did not accept Interac , a standard payment method in Canada Examples involving it are as follows Not complying with customers' choice to pay with Interac, a standard payment method in Canada, is also a nuisance Only reason it got a 4 star is because they don't accept interac which is my go to. display_neg_sentiments(sentiment_lines,chioptle_yonge) 38.6 Positive Reviews We examine the Top Ten most postive reviews. display_pos_sentiments(sentiment_lines,chioptle_yonge) 38.7 Relationship among words in Chipotle Business in Yonge Street Toronto We explore the different relationship among the various words in Chipotle Business in Yonge Street Toronto here through a network graph bigrams_chioptle_yonge &lt;- chioptle_yonge %&gt;% count_bigrams() bigrams_chioptle_yonge %&gt;% filter(n &gt; 5) %&gt;% visualize_bigrams() "],
["topic-modelling.html", "Chapter 39 Topic Modelling 39.1 LDA Function 39.2 Topic Modelling for Mon Ami Gabi 39.3 Topic Modelling for Bacchanal Buffet 39.4 Topic Modelling for Pai Northern Thai Kitchen", " Chapter 39 Topic Modelling Topic modeling is a method for unsupervised classification of documents, similar to clustering on numeric data, which finds natural groups of items even when we’re not sure what we’re looking for. Latent Dirichlet allocation (LDA) is a particularly popular method for fitting a topic model. It treats each document as a mixture of topics, and each topic as a mixture of words. This allows documents to “overlap” each other in terms of content, rather than being separated into discrete groups, in a way that mirrors typical use of natural language. 39.1 LDA Function Borrowing an awesome function from Rachael’s Notebook # function to get &amp; plot the most informative terms by a specificed number # of topics, using LDA top_terms_by_topic_LDA &lt;- function(input_text, # should be a columm from a dataframe plot = T, # return a plot? TRUE by defult number_of_topics = 4) # number of topics (4 by default) { # create a corpus (type of object expected by tm) and document term matrix Corpus &lt;- Corpus(VectorSource(input_text)) # make a corpus object DTM &lt;- DocumentTermMatrix(Corpus) # get the count of words/document # remove any empty rows in our document term matrix (if there are any # we&#39;ll get an error when we try to run our LDA) unique_indexes &lt;- unique(DTM$i) # get the index of each unique value DTM &lt;- DTM[unique_indexes,] # get a subset of only those indexes # preform LDA &amp; get the words/topic in a tidy text format lda &lt;- LDA(DTM, k = number_of_topics, control = list(seed = 1234)) topics &lt;- tidy(lda, matrix = &quot;beta&quot;) # get the top ten terms for each topic top_terms &lt;- topics %&gt;% # take the topics data frame and.. group_by(topic) %&gt;% # treat each topic as a different group top_n(10, beta) %&gt;% # get the top 10 most informative words ungroup() %&gt;% # ungroup arrange(topic, -beta) # arrange words in descending informativeness # if the user asks for a plot (TRUE by default) if(plot == T){ # plot the top ten terms for each topic in order top_terms %&gt;% # take the top terms mutate(term = reorder(term, beta)) %&gt;% # sort terms by beta value ggplot(aes(term, beta, fill = factor(topic))) + # plot beta by theme geom_col(show.legend = FALSE) + # as a bar plot facet_wrap(~ topic, scales = &quot;free&quot;) + # which each topic in a seperate plot labs(x = NULL, y = &quot;Beta&quot;) + # no x label, change y label coord_flip() # turn bars sideways }else{ # if the user does not request a plot # return a list of sorted terms instead return(top_terms) } } 39.2 Topic Modelling for Mon Ami Gabi 4 topics for the Mon Ami Gabi create_LDA_topics &lt;- function(business_text,custom_stop_words) { # create a document term matrix to clean reviewsCorpus &lt;- Corpus(VectorSource(business_text$text)) reviewsDTM &lt;- DocumentTermMatrix(reviewsCorpus) # convert the document term matrix to a tidytext corpus reviewsDTM_tidy &lt;- tidy(reviewsDTM) # remove stopwords reviewsDTM_tidy_cleaned &lt;- reviewsDTM_tidy %&gt;% # take our tidy dtm and... anti_join(stop_words, by = c(&quot;term&quot; = &quot;word&quot;)) %&gt;% # remove English stopwords and... anti_join(custom_stop_words, by = c(&quot;term&quot; = &quot;word&quot;)) # remove my custom stopwords top_terms_by_topic_LDA(reviewsDTM_tidy_cleaned$term, number_of_topics = 4) } monamigabi = reviews %&gt;% filter(business_id == &quot;4JNXUYY8wbaaDmk3BPzlWw&quot;) custom_stop_words &lt;- tibble(word = c(&quot;mon&quot;,&quot;ami&quot;,&quot;gabi&quot;,&quot;restaurant&quot;,&quot;food&quot;,&quot;vegas&quot;)) create_LDA_topics(monamigabi,custom_stop_words) 39.3 Topic Modelling for Bacchanal Buffet 4 topics for the Bacchanal Buffet custom_stop_words &lt;- tibble(word = c(&quot;restaurant&quot;,&quot;food&quot;)) create_LDA_topics(bacchanal,custom_stop_words) 39.4 Topic Modelling for Pai Northern Thai Kitchen 4 topics for the Pai Northern Thai Kitchen custom_stop_words &lt;- tibble(word = c(&quot;thai&quot;,&quot;restaurant&quot;,&quot;food&quot;)) create_LDA_topics(pai_thai,custom_stop_words) We observe a common theme which appears across topics across the Three restaurants is service.The theme of service complaints was also very evident when we did the sentiment analysis "],
["phoenix-city-analysis.html", "Chapter 40 Phoenix City Analysis 40.1 Top Ten Business in Phoenix 40.2 Topic Modelling for Phoenix City 40.3 Word Cloud of Phoenix City 40.4 Top Ten most common Words of the business Phoenix City 40.5 Sentiment Analysis - Postive and Not So Postive Words of Phoenix City 40.6 Calculate Sentiment for the reviews 40.7 Negative Reviews 40.8 Positive Reviews", " Chapter 40 Phoenix City Analysis 40.1 Top Ten Business in Phoenix We list the Top Ten business in Toronto giving importance to the number of reviews and then to the number of stars obtained by the business. city_biz = business %&gt;% filter(city == &quot;Phoenix&quot;) %&gt;% arrange(desc(review_count,stars)) %&gt;% select(name,neighborhood,address,review_count,stars) %&gt;% head(10) datatable(city_biz, style=&quot;bootstrap&quot;, class=&quot;table-condensed&quot;, options = list(dom = &#39;tp&#39;,scrollX = TRUE)) 40.2 Topic Modelling for Phoenix City We do a Topic Modelling on the reviews of a sample of Ten Thousand Words of Phoenix City. CityCoords = business %&gt;% filter(city == &quot;Phoenix&quot;) city_words = inner_join(CityCoords,reviews) %&gt;% select(date,text,review_id) %&gt;% sample_n(10000) custom_stop_words &lt;- tibble(word = c(&quot;restaurant&quot;,&quot;food&quot;)) create_LDA_topics(city_words,custom_stop_words) We observe the themes of Service and time being very dominant. The occurence of the word chicken among food items is present. 40.3 Word Cloud of Phoenix City createWordCloud(city_words) 40.4 Top Ten most common Words of the business Phoenix City We examine the Top Ten Most Common words and show them in a bar graph. city_words %&gt;% unnest_tokens(word, text) %&gt;% filter(!word %in% stop_words$word) %&gt;% filter(!word %in% c(&#39;food&#39;,&#39;restaurant&#39;)) %&gt;% count(word,sort = TRUE) %&gt;% ungroup() %&gt;% mutate(word = factor(word, levels = rev(unique(word)))) %&gt;% head(10) %&gt;% ggplot(aes(x = word,y = n)) + geom_bar(stat=&#39;identity&#39;,colour=&quot;white&quot;, fill =fillColor) + geom_text(aes(x = word, y = 1, label = paste0(&quot;(&quot;,n,&quot;)&quot;,sep=&quot;&quot;)), hjust=0, vjust=.5, size = 4, colour = &#39;black&#39;, fontface = &#39;bold&#39;) + labs(x = &#39;Word&#39;, y = &#39;Word Count&#39;, title = &#39;Word Count&#39;) + coord_flip() + theme_bw() 40.5 Sentiment Analysis - Postive and Not So Postive Words of Phoenix City We display the Positive and Not So Positive words used by reviewers for Phoenix City.We have used the AFINN sentiment lexicon, which provides numeric positivity scores for each word, and visualize it with a bar plot. positiveWordsBarGraph(city_words) 40.6 Calculate Sentiment for the reviews We calculate the sentiment scores for all the reviews using the AFINN sentiment lexicon. We display the Top Six sentiments here. sentiment_lines = calculate_sentiment(city_words) head(sentiment_lines) 40.7 Negative Reviews We examine the Top Ten most negative reviews. display_neg_sentiments(sentiment_lines,city_words) 40.8 Positive Reviews We examine the Top Ten most postive reviews. display_pos_sentiments(sentiment_lines,city_words) "],
["introduction-2.html", "Chapter 41 Introduction", " Chapter 41 Introduction This dataset contains the characters, locations, episode details, and script lines for approximately 600 Simpsons episodes, dating back to 1989. Simpson’s image from Wikipedia "],
["ten-most-active-characters.html", "Chapter 42 Ten Most Active Characters", " Chapter 42 Ten Most Active Characters We want to find out which characters have spoken the most in the episodes. Here the top Ten are being displayed. detach(package:plyr) rm(list=ls()) fillColor = &quot;#FFA07A&quot; fillColor2 = &quot;#F1C40F&quot; Scripts = read_csv(&quot;input/simpsons_script_lines.csv&quot;) Characters = read_csv(&quot;input/simpsons_characters.csv&quot;) Scripts$character_id = as.integer(Scripts$character_id) ScriptsCharacters = left_join(Scripts,Characters, by = c(&quot;character_id&quot; = &quot;id&quot;) ) ScriptsCharacters = ScriptsCharacters %&gt;% filter(!is.na(name)) TopCharacters = ScriptsCharacters %&gt;% group_by(name) %&gt;% tally(sort = TRUE) ggplot(head(TopCharacters,10), aes(x = reorder(name, n), y = n)) + geom_bar(stat=&#39;identity&#39;,colour=&quot;white&quot;, fill = fillColor) + geom_text(aes(x = name, y = 1, label = paste0(&quot;(&quot;,n,&quot;)&quot;,sep=&quot;&quot;)), hjust=0, vjust=.5, size = 4, colour = &#39;black&#39;, fontface = &#39;bold&#39;) + labs(x = &#39;name&#39;, y = &#39;Count Of Sentences&#39;, title = &#39;Ten Most Active Characters&#39;) + coord_flip() + theme_bw() If you are not familiar with the characters like me , Wikipedia is there to help. Lets hop to Wikipedia and get the details. I have the following information from Wikipedia. Homer Jay Simpson is a fictional character and the main protagonist of the American animated television series The Simpsons as the patriarch of the eponymous family. Homer and his wife Marge have three children: Bart, Lisa, and Maggie. As the family’s provider, he works at the Springfield Nuclear Power Plant as safety inspector. Charles Montgomery Burns, known as C. Montgomery Burns and Monty Burns, but usually referred to simply as Mr. Burns, occasionally as Mr Snrub, is a recurring character in the animated television series The Simpsons, and is voiced by Harry Shearer. Mr. Burns is the evil owner of the Springfield Nuclear Power Plant and is also Homer Simpson’s boss. He is assisted at almost all times by Waylon Smithers, his loyal and sycophantic aide, adviser, confidant and secret admirer. Moe Szyslak is the proprietor and bartender of Moe’s Tavern, a Springfield bar frequented by Homer Simpson, Barney Gumble, Carl Carlson, Lenny Leonard, Sam, Larry, and others. "],
["next-ten-most-active-characters.html", "Chapter 43 Next Ten Most Active Characters", " Chapter 43 Next Ten Most Active Characters Here the next 10 active characters are being displayed. Their ranking is from the 11th position to the 20th position. ggplot(TopCharacters[10:20,], aes(x = reorder(name, n), y = n)) + geom_bar(stat=&#39;identity&#39;,colour=&quot;white&quot;, fill = fillColor2) + geom_text(aes(x = name, y = 1, label = paste0(&quot;(&quot;,n,&quot;)&quot;,sep=&quot;&quot;)), hjust=0, vjust=.5, size = 4, colour = &#39;black&#39;, fontface = &#39;bold&#39;) + labs(x = &#39;name&#39;, y = &#39;Count Of Sentences&#39;, title = &#39;Next Ten Most Active Characters&#39;) + coord_flip() + theme_bw() "],
["top-twenty-most-common-words-1.html", "Chapter 44 Top Twenty most Common Words 44.1 WordCloud of the Common Words", " Chapter 44 Top Twenty most Common Words We examine the Top Twenty Most Common words and show them in a bar graph. SC = ScriptsCharacters %&gt;% select(id,name,normalized_text) SC %&gt;% unnest_tokens(word, normalized_text) %&gt;% filter(!word %in% stop_words$word) %&gt;% dplyr::count(word,sort = TRUE) %&gt;% ungroup() %&gt;% mutate(word = factor(word, levels = rev(unique(word)))) %&gt;% head(20) %&gt;% ggplot(aes(x = word,y = n)) + geom_bar(stat=&#39;identity&#39;,colour=&quot;white&quot;, fill =fillColor) + geom_text(aes(x = word, y = 1, label = paste0(&quot;(&quot;,n,&quot;)&quot;,sep=&quot;&quot;)), hjust=0, vjust=.5, size = 4, colour = &#39;black&#39;, fontface = &#39;bold&#39;) + labs(x = &#39;Word&#39;, y = &#39;Word Count&#39;, title = &#39;Top 20 most Common Words&#39;) + coord_flip() + theme_bw() 44.1 WordCloud of the Common Words A word cloud is a graphical representation of frequently used words in the Normalized text. The height of each word in this picture is an indication of frequency of occurrence of the word in the entire text. im , dont , hey and homer are some of the most commonly occuring terms. SC %&gt;% unnest_tokens(word, normalized_text) %&gt;% filter(!word %in% stop_words$word) %&gt;% dplyr::count(word,sort = TRUE) %&gt;% ungroup() %&gt;% head(50) %&gt;% with(wordcloud(word, n, max.words = 50,colors=brewer.pal(8, &quot;Dark2&quot;))) "],
["tf-idf-1.html", "Chapter 45 TF-IDF 45.1 The Math 45.2 Twenty Most Important words for the Twenty Most Active Characters 45.3 Word Cloud for the Twenty most important characters 45.4 Marge Important Words 45.5 Moe Important Words", " Chapter 45 TF-IDF We wish to find out the important words which are spoken by the characters. Example for your young child , the most important word is mom. Example for a bar tender , important words would be related to drinks. We would explore this using a fascinating concept known as Term Frequency - Inverse Document Frequency. Quite a mouthful, but we will unpack it and clarify each and every term. A document in this case is the set of lines spoken by a character. E.g. The words spoken by Bart is a single document.The words spoken by Marge is a another document. Therefore we have different documents for each Character. From the book 5 Algorithms Every Web Developer Can Use and Understand TF-IDF computes a weight which represents the importance of a term inside a document. e.g. The term mom might be an important term for both Lisa and Bart since they would say this a lot of times to Marge but not any other character would say this term. It does this by comparing the frequency of usage inside an individual document as opposed to the entire data set (a collection of documents). The importance increases proportionally to the number of times a word appears in the individual document itself–this is called Term Frequency. However, if multiple documents contain the same word many times then you run into a problem. That’s why TF-IDF also offsets this value by the frequency of the term in the entire document set, a value called Inverse Document Frequency. 45.1 The Math TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document) IDF(t) = log_e(Total number of documents / Number of documents with term t in it). Value = TF * IDF 45.2 Twenty Most Important words for the Twenty Most Active Characters Here using TF-IDF , we investigate the Twenty Most Important words for the Twenty Most Active Characters. #Get the Top 20 Characters Top20Characters = head(TopCharacters)$name ################################################################################## # Prepare for the bind_tf_idf function ################################################################################## SCWords &lt;- SC %&gt;% unnest_tokens(word, normalized_text) %&gt;% dplyr::count(name, word, sort = TRUE) %&gt;% ungroup() total_words &lt;- SCWords %&gt;% group_by(name) %&gt;% summarize(total = sum(n)) SCWords &lt;- left_join(SCWords, total_words) SCWordsFull &lt;- SCWords %&gt;% filter(!is.na(name)) %&gt;% bind_tf_idf(word, name, n) #Now we are ready to use the bind_tf_idf which computes the tf-idf for each term. SCWords &lt;- SCWords %&gt;% filter( name %in% Top20Characters) %&gt;% bind_tf_idf(word, name, n) plot_SCWords &lt;- SCWords %&gt;% arrange(desc(tf_idf)) %&gt;% mutate(word = factor(word, levels = rev(unique(word)))) plot_SCWords %&gt;% top_n(20) %&gt;% ggplot(aes(word, tf_idf, fill = name)) + geom_col() + labs(x = NULL, y = &quot;tf-idf&quot;) + coord_flip() + theme_bw() #Choose words with low IDF SCWords2 &lt;- SCWords %&gt;% bind_tf_idf(word, name, n) LowIDF = SCWords2 %&gt;% arrange((idf)) %&gt;% select(word,idf) #Get the Unique Words with LowIDF UniqueLowIDF = unique(LowIDF$word) We observe that the most important word for Bart and Lisa is mom. This is obvious since both Bart and Lisa are children of Marge and Homer. 45.3 Word Cloud for the Twenty most important characters We show the Hundred most important words for the Twenty most important characters. This Word Cloud is based on the TF- IDF scores. Higher the score, bigger is the size of the text. plot_SCWords %&gt;% with(wordcloud(word, tf_idf, max.words = 100,colors=brewer.pal(8, &quot;Dark2&quot;))) 45.4 Marge Important Words We investigate here the most important words spoken by Marge.The conversations of Marge with the word homie is provided below. We observe that all of it is addressed to her husband Homer . keywordHomie = &#39;homie&#39; ScriptsCharactersMarge = ScriptsCharacters %&gt;% filter(name == &#39;Marge Simpson&#39;) %&gt;% filter(str_detect(normalized_text,keywordHomie) ) MargeAdressesTo &lt;- data.frame(Name = character(), Text = character()) for(i in 1: 5) { MargeNextSenceAfterHomie = ScriptsCharacters %&gt;% filter(id &gt; ScriptsCharactersMarge[i,]$id - 1) %&gt;% filter(id &lt; (ScriptsCharactersMarge[i,]$id +2) ) %&gt;% select(name,raw_text) MargeAdressesTo = rbind(MargeAdressesTo,MargeNextSenceAfterHomie) } name raw_text Marge Simpson Marge Simpson: Homie, did you straighten everything out…? Homer Simpson Homer Simpson: (HAPPILY) Up… up… up… up… up… up. Don’t say anything, Marge. Let’s just go to bed. I’m on the biggest roll of my life. Marge Simpson Marge Simpson: Oh, Homie, you have lots of hair… Why did you want to know your blood type? Homer Simpson Homer Simpson: Aw, old man Burns is gonna kick off if he doesn’t get some Double-O-Negative blood, but nobody at the plant has it. Marge Simpson Marge Simpson: Please, Homie? For me? Homer Simpson Homer Simpson: (SOFTENING) Oh, all right. (GRUMBLING) You always do that hand thing! And it usually works. Marge Simpson Marge Simpson: Oh, Homie. Psychiatrist Psychiatrist: Mr. Simpson, after talking to your wife, we believe you’re no threat to yourself or others. Marge Simpson Marge Simpson: (AMOROUSLY) Homie… Put down your magazine for a minute… Homer Simpson Homer Simpson: Hmph? 45.5 Moe Important Words We investigate here the most important words spoken by Moe. 45.5.1 Word focus - “Midge” keywordMidge = &#39;midge&#39; ScriptsCharactersMoe = ScriptsCharacters %&gt;% filter(name == &#39;Moe Szyslak&#39;) %&gt;% filter(str_detect(normalized_text,keywordMidge) ) MoeAdressesTo &lt;- data.frame(Name = character(), Text = character()) for(i in 1: 5) { MoeNextSenceAfterMidge = ScriptsCharacters %&gt;% filter(id &gt; ScriptsCharactersMoe[i,]$id - 1) %&gt;% filter(id &lt; (ScriptsCharactersMoe[i,]$id +2) ) %&gt;% select(name,raw_text) MoeAdressesTo = rbind(MoeAdressesTo,MoeNextSenceAfterMidge) } name raw_text Moe Szyslak Moe Szyslak: What? What? A bartender can’t come by and say “hi” to his best customer? (TO MARGE) Hey, hey there, Midge! Oh gee I like what you done to your hair. Marge Simpson Marge Simpson: You caught me at a real bad time, Moe. I hope you understand I’m too tense to pretend I like you. Moe Szyslak Moe Szyslak: Can somebody tell me what the hell is goin’ on? Midge, help me out here. Homer Simpson Homer Simpson: Quiet! You’re missing the jokes! Moe Szyslak Moe Szyslak: Outta the way, Midge! Marge Simpson Marge Simpson: (PHONY) Oh, am I in the way? Moe Szyslak Moe Szyslak: Ah, remember, Midge. You feel the need to rage, you call me, right? I won’t even get sexual or nothin’. Unless that’s what you want. (SHORT AWKWARD BEAT) That, that’s not what you want, right? Marge Simpson Marge Simpson: (FIRMLY) No thanks. (UPBEAT) But thanks. Moe Szyslak Moe Szyslak: Okay, Midge. You made us feel bad about what we done to your boy. But what can we do about it now? It’s not like we can play the game over again. Lisa Simpson Lisa Simpson: (SLY) Can’t we? "],
["relationship-among-words-2.html", "Chapter 46 Relationship among words 46.1 Dont word network graph", " Chapter 46 Relationship among words Til now, we have explored the most important words for a character. Now, we will explore the relationship between words. count_bigrams &lt;- function(dataset) { dataset %&gt;% unnest_tokens(bigram, normalized_text, token = &quot;ngrams&quot;, n = 2) %&gt;% separate(bigram, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) %&gt;% filter(!word1 %in% stop_words$word, !word2 %in% stop_words$word) %&gt;% dplyr::count(word1, word2, sort = TRUE) } visualize_bigrams &lt;- function(bigrams) { set.seed(2016) a &lt;- grid::arrow(type = &quot;closed&quot;, length = unit(.15, &quot;inches&quot;)) bigrams %&gt;% graph_from_data_frame() %&gt;% ggraph(layout = &quot;fr&quot;) + geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a) + geom_node_point(color = &quot;lightblue&quot;, size = 5) + geom_node_text(aes(label = name), vjust = 1, hjust = 1) + theme_void() } visualize_bigrams_individual &lt;- function(bigrams) { set.seed(2016) a &lt;- grid::arrow(type = &quot;closed&quot;, length = unit(.15, &quot;inches&quot;)) bigrams %&gt;% graph_from_data_frame() %&gt;% ggraph(layout = &quot;fr&quot;) + geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a,end_cap = circle(.07, &#39;inches&#39;)) + geom_node_point(color = &quot;lightblue&quot;, size = 5) + geom_node_text(aes(label = name), vjust = 1, hjust = 1) + theme_void() } SCWords &lt;- SC %&gt;% count_bigrams() SCWords %&gt;% filter(n &gt; 50) %&gt;% visualize_bigrams() The above infographic shows the words which follow another word. E.g. the word dont is preceded by many words such as care forget understand worry mind wanna There are certain words which follows itself such as bye , hee since we have script lines which would be bye bye and hee hee respectively. 46.1 Dont word network graph individual_words_bigrams &lt;- function(SC, word1Value, word2Value) { x_Words1 &lt;- SC %&gt;% count_bigrams() %&gt;% filter(word1 == word1Value) x_Words2 &lt;- SC %&gt;% count_bigrams() %&gt;% filter(word2 == word2Value) x_full = rbind(x_Words1,x_Words2) } individual_words_bigrams(SC,&quot;dont&quot;,&quot;dont&quot;) %&gt;% filter(n &gt; 20) %&gt;% visualize_bigrams_individual() "],
["sentiment-analysis-1.html", "Chapter 47 Sentiment Analysis 47.1 Postive Characters and Not so Positive Characters 47.2 Postive and Not So Postive Words 47.3 Postive and Not So Postive Script Lines", " Chapter 47 Sentiment Analysis 47.1 Postive Characters and Not so Positive Characters We investigate how often positive and negative words occurred in these Simpsons script lines. Which characters were the most positive or negative overall? We will use the AFINN sentiment lexicon, which provides numeric positivity scores for each word, and visualize it with a bar plot.We limit the sentiment analysis to the Top 20 characters who have spoken the most in the episodes. visualize_sentiments &lt;- function(SCWords) { SCWords_sentiments &lt;- SCWords %&gt;% inner_join(get_sentiments(&quot;afinn&quot;), by = &quot;word&quot;) %&gt;% group_by(name) %&gt;% summarize(score = sum(score * n) / sum(n)) %&gt;% arrange(desc(score)) SCWords_sentiments %&gt;% mutate(name = reorder(name, score)) %&gt;% ggplot(aes(name, score, fill = score &gt; 0)) + geom_col(show.legend = TRUE) + coord_flip() + ylab(&quot;Average sentiment score&quot;) + theme_bw() } Top20Characters = head(TopCharacters,20)$name SCWordsTop20Characters &lt;- SC %&gt;% unnest_tokens(word, normalized_text) %&gt;% filter(name != &quot;NA&quot;) %&gt;% filter( name %in% Top20Characters) %&gt;% dplyr::count(name, word, sort = TRUE) %&gt;% ungroup() visualize_sentiments(SCWordsTop20Characters) We observe the following Edna Flanders has the most positive sentiment. Marge Simpson is the most postive among the Simpsons. Bart Simpson is the most negative among the Simpsons. Dr. Julius Hilbert is the most negative among the Top 20 characters. 47.2 Postive and Not So Postive Words The following graph shows the Twenty high positive and the negative words. contributions &lt;- SC %&gt;% unnest_tokens(word, normalized_text) %&gt;% filter(name != &quot;NA&quot;) %&gt;% dplyr::count(name, word, sort = TRUE) %&gt;% ungroup() %&gt;% inner_join(get_sentiments(&quot;afinn&quot;), by = &quot;word&quot;) %&gt;% group_by(word) %&gt;% summarize(occurences = n(), contribution = sum(score)) contributions %&gt;% top_n(20, abs(contribution)) %&gt;% mutate(word = reorder(word, contribution)) %&gt;% ggplot(aes(word, contribution, fill = contribution &gt; 0)) + geom_col(show.legend = FALSE) + coord_flip() + theme_bw() 47.3 Postive and Not So Postive Script Lines We examine the positive and the negative Script Lines.We filtered out messages that had fewer than five words that contributed to sentiment. sentiment_lines = SC %&gt;% unnest_tokens(word, normalized_text) %&gt;% filter(name != &quot;NA&quot;) %&gt;% inner_join(get_sentiments(&quot;afinn&quot;), by = &quot;word&quot;) %&gt;% group_by(id,word) %&gt;% summarize(sentiment = mean(score), words = n()) %&gt;% ungroup() %&gt;% filter(words &gt;= 5) The scriptlines having top Ten positive sentiments are name Text Applicants roll on roll on alpha tau – bow wow wow wow bow wow wow wow Homer Simpson ill let you try them on right after i decide if these videos are funny or die funny funny die die funny funny but the guy died Chief Wiggum wow-wow-wow-wow-wow thats-thats-thats crazy-crazy woo-woo-woo hello-hello lou-lou Little Homer fun fun fun fun fun fun fun fun Itchy &amp; Scratchy Singers they love they share they share and love and share love love love share share share the itchy scratchy showwww Jinglers come to uncle moes for family fun its good good good good good good good Indian Woman love love love love love im in love with lovely johnny Sideshow Mel cmon they cant all have loved it loved it loved it loved it loved it despite absence of flubber glayvin Marge Simpson boy things are going really good good good good good good good good hmm look at that freckle i should dig that out Melanie Griffith this rooms nice this rooms nice too this rooms nice too this rooms nice this rooms nice this rooms nice too Lisa Simpson my great-great great-great great grandmother was a native american she was a member of the Homer Simpson doh doh doh doh doh doh doh woo woo woo woo woo woo hoo stu stu stu stu-pid flanders Krusty the Clown love love love love love Milhouse Van Houten i hold in my hand another diary that of my great-great-great great-great grandfafa milford van houten Carolers happy happy happy happy hunting happy happy happy happy hunting The scriptlines having top Ten NOT so positive sentiments are name Text Bart Simpson bastard bastard bastard bastard bastard bastard Bart Simpson hell hell hell hell hell hell hell hell hell hell hell hell hell Singers bad cops bad cops bad cops bad cops springfield cops are on the take but what do you expect for the money we make whether in a car or on a bus we dont mind using excessive force bad cops bad cops bad cops bad cops Homer Simpson oh everythings cruel according to you keeping him chained up in the backyard is cruel pulling on his tail is cruel yelling in his ears is cruel everything is cruel so excuse me if im cruel Homer Simpson crap crap crap crap crap crap crap crap Lisa Simpson stop it i cheated cheated cheated cheated cheated cheated Helen Morehouse i wanted maryanne-on-gilligans-island ugly not cornelius-on-the-planet-of-the-apes ugly tv ugly not ugly ugly Grampa Simpson dead dead dead dead dead dead Prisoners kill the rat kill the rat kill the rat kill the rat kill the rat kill the rat kill the rat kill the rat kill the rat Chief Wiggum krusty the clown you are under arrest for armed robbery you have the right to remain silent anything you say blah blah blah blah blah blah blah blah blah Krabappel’s Mouth blah blah blah blah blah blah blah blah blah Krabappel’s Mouth blah blah blah blah blah blah blah blah blah Dr. J. Loren Pyror blah blah blah blah blah blah blah Dr. J. Loren Pyror blah blah blah blah blah blah blah blah blah Homer Simpson so a few people wont get a few letters boo-hoo you know the kinda letters people write dear somebody-you-never-heard-of how is so-and-so blah blah blah blah blah blah blah yours truly some bozo big loss Krusty the Clown blah blah blah blah moses blah blah blah blah some prayer Homer Simpson im mr burns blah blah blah do this do that blah blah blah Marge Simpson legalized gambling is a bad idea you can build a casino over my dead body blah blah blah blah blah blah blah blah blah blah blah blah blah Kent Brockman by the way the spacecraft is still in extreme danger may not make it back attempting risky reentry blah blah blah blah blah well see you after the movie Marge Simpson hey hey over here here bully bully bully bully bully Bart Simpson daddys on fire daddys not on fire daddys on fire daddys not on fire daddys on fire Homer Simpson oh and that dr hibbert was so boring homer weve got to get that lump checked out homer we must discuss your test results homer weve got to find you a donor blah blah blah blah blah blah Marge Simpson ill do it ill do it ill do it ill do it ill do it C. Montgomery Burns youre fired youre all fired fired fired fired TABITHA trouble-istic girl makes your troubulations grow double trouble-istic when you try to tell me no trouble trouble oh yeah trouble and the home of the brave Homer Simpson oh yeah blah blah blah blah blah blah blah blah blah Homer Simpson blah blah blah blah blah blah Other Men penalty penalty penalty penalty penalty penalty Bart Simpson lame lame lame lame have it lame superman dies aquaman dies casper dies caveman robin black robin born-again robin Lisa Simpson sad sad sad sad sad sad sad sad sad cute this one "],
["topic-modelling-1.html", "Chapter 48 Topic Modelling", " Chapter 48 Topic Modelling Topic modeling is a method for unsupervised classification of documents, similar to clustering on numeric data, which finds natural groups of items even when we’re not sure what we’re looking for. Latent Dirichlet allocation (LDA) is a particularly popular method for fitting a topic model. It treats each document as a mixture of topics, and each topic as a mixture of words. This allows documents to “overlap” each other in terms of content, rather than being separated into discrete groups, in a way that mirrors typical use of natural language. ################################################################################################ #Topic Modelling with Simpsons ######################################################################################### SC = ScriptsCharacters %&gt;% select(id,name,normalized_text) corpus = Corpus(VectorSource(SC$normalized_text)) # Pre-process data corpus &lt;- tm_map(corpus, tolower) corpus &lt;- tm_map(corpus, removePunctuation) corpus &lt;- tm_map(corpus, removeWords, stopwords(&quot;english&quot;)) corpus &lt;- tm_map(corpus, removeWords, UniqueLowIDF[1:500]) corpus &lt;- tm_map(corpus, stemDocument) dtm = DocumentTermMatrix(corpus) # Remove sparse terms dtm = removeSparseTerms(dtm, 0.997) # Create data frame labeledTerms = as.data.frame(as.matrix(dtm)) labeledTerms = labeledTerms[rowSums(abs(labeledTerms)) != 0,] ############################################################################## #LDA Modelling Starts ############################################################################### # set a seed so that the output of the model is predictable simpsons_lda &lt;- LDA(labeledTerms, k = 2, control = list(seed = 13)) #The tidytext package provides this method for extracting the per-topic-per-word probabilities, # called β (“beta”), from the model simpsons_topics &lt;- tidy(simpsons_lda, matrix = &quot;beta&quot;) simpsons_top_terms &lt;- simpsons_topics %&gt;% group_by(topic) %&gt;% top_n(10, beta) %&gt;% ungroup() %&gt;% arrange(topic, -beta) simpsons_top_terms %&gt;% mutate(term = reorder(term, beta)) %&gt;% ggplot(aes(term, beta, fill = factor(topic))) + geom_col(show.legend = FALSE) + facet_wrap(~ topic, scales = &quot;free&quot;) + coord_flip() + theme_bw() This visualization lets us understand the two topics that were extracted from the Script Lines. The most common words in topic 1 include mom,homi,milhous which suggests it may represent the Simpson family topics. Those most common in topic 2 include moe,work,hour suggesting that this topic represents work related things. One important observation about the words in each topic is that some words, such as wonder, are common within both topics. This is an advantage of topic modeling as opposed to “hard clustering” methods: topics used in natural language could have some overlap in terms of words. "],
["location-of-characters.html", "Chapter 49 Location of Characters 49.1 Homers Location 49.2 Marge’s Location 49.3 Bart’s Location 49.4 Lisa’s Location", " Chapter 49 Location of Characters LocationCharacters = ScriptsCharacters %&gt;% group_by(name,raw_location_text) %&gt;% tally() locationOfCharacters &lt;- function(nameOfCharacter) { SCLocation = LocationCharacters %&gt;% filter(name == nameOfCharacter) %&gt;% arrange(desc(n)) %&gt;% head(10) titlePlot = paste0(nameOfCharacter,&quot;&#39;s Location&quot;) ggplot(SCLocation, aes(x = reorder(raw_location_text, n), y = n)) + geom_bar(stat=&#39;identity&#39;,colour=&quot;white&quot;, fill = fillColor2) + geom_text(aes(x = raw_location_text, y = 1, label = paste0(&quot;(&quot;,n,&quot;)&quot;,sep=&quot;&quot;)), hjust=0, vjust=.5, size = 4, colour = &#39;black&#39;, fontface = &#39;bold&#39;) + labs(x = &#39;Counts in Locations&#39;, y = &#39;Locations&#39;, title = titlePlot) + coord_flip() + theme_bw() } 49.1 Homers Location locationOfCharacters(&quot;Homer Simpson&quot;) Homer is usually at home , Moe’s Tavern and his work place Springfield Nuclear Power Plant. 49.2 Marge’s Location locationOfCharacters(&quot;Marge Simpson&quot;) 49.3 Bart’s Location locationOfCharacters(&quot;Bart Simpson&quot;) 49.4 Lisa’s Location locationOfCharacters(&quot;Lisa Simpson&quot;) "],
["homer-simpson.html", "Chapter 50 Homer Simpson 50.1 Word Cloud 50.2 Postive and Not So Postive Words of Homer Simpson 50.3 Sentiment Analysis based on location", " Chapter 50 Homer Simpson We examine Homer with the help of the Word Cloud, The Postive and Not so Postive words , and how the postivity differs from one Location to the other. 50.1 Word Cloud SC = ScriptsCharacters %&gt;% select(id,name,normalized_text) SC %&gt;% unnest_tokens(word, normalized_text) %&gt;% filter(name == &quot;Homer Simpson&quot;) %&gt;% dplyr::count(name, word, sort = TRUE) %&gt;% group_by(word) %&gt;% summarize(occurences = n()) %&gt;% with(wordcloud(word, occurences, max.words = 50,colors=brewer.pal(8, &quot;Dark2&quot;))) 50.2 Postive and Not So Postive Words of Homer Simpson The following graph shows the Twenty high positive and the negative words by Homer Simpson. contributions &lt;- SC %&gt;% unnest_tokens(word, normalized_text) %&gt;% filter(name == &quot;Homer Simpson&quot;) %&gt;% dplyr::count(name, word, sort = TRUE) %&gt;% ungroup() %&gt;% inner_join(get_sentiments(&quot;afinn&quot;), by = &quot;word&quot;) %&gt;% group_by(word) %&gt;% summarize(occurences = n(), contribution = sum(score)) contributions %&gt;% top_n(20, abs(contribution)) %&gt;% mutate(word = reorder(word, contribution)) %&gt;% head(20) %&gt;% ggplot(aes(word, contribution, fill = contribution &gt; 0)) + geom_col(show.legend = FALSE) + coord_flip() + theme_bw() 50.3 Sentiment Analysis based on location We wish to analyse the sentiments of Homer based on the location.We want to find out whether Homer’s positivity or negativity changes based on where he is located visualize_sentiments_location &lt;- function(SCWords) { SCWords_sentiments &lt;- SCWords %&gt;% inner_join(get_sentiments(&quot;afinn&quot;), by = &quot;word&quot;) %&gt;% group_by(raw_location_text) %&gt;% summarize(score = sum(score * n) / sum(n)) %&gt;% arrange(desc(score)) SCWords_sentiments %&gt;% mutate(raw_location_text = reorder(raw_location_text, score)) %&gt;% ggplot(aes(raw_location_text, score)) + geom_col(fill = fillColor2) + coord_flip() + ylab(&quot;Average sentiment score&quot;) + theme_bw() } nameOfCharacter = &quot;Homer Simpson&quot; HomerLocation = LocationCharacters %&gt;% filter(name == &quot;Homer Simpson&quot;) %&gt;% arrange(desc(n)) %&gt;% head(10) SCWordsHomerSimpson &lt;- ScriptsCharacters %&gt;% unnest_tokens(word, normalized_text) %&gt;% filter(name == nameOfCharacter) %&gt;% filter(raw_location_text %in% HomerLocation$raw_location_text) %&gt;% dplyr::count(raw_location_text, word, sort = TRUE) %&gt;% ungroup() visualize_sentiments_location(SCWordsHomerSimpson) Homer seems to be positive in the Kitchen, which is expected since the food is there. He has to postive in work and the positivity is shown in his work place Springfield Nuclear Power Plant. "],
["best-and-the-worst-episodes.html", "Chapter 51 Best and the Worst Episodes 51.1 Best Episode 51.2 Worst Episode 51.3 Positive and Not So Positive Characters of the Best Episode 51.4 Positive and Not So Positive Characters of the Worst Episode 51.5 Postive and Not So Postive Words of Best Episode 51.6 Postive and Not So Postive Words of Worst Episode", " Chapter 51 Best and the Worst Episodes Episodes = read_csv(&quot;input/simpsons_episodes.csv&quot;) Episodes$imdb_rating = as.numeric(Episodes$imdb_rating) BestEpisode = Episodes %&gt;% arrange(desc(imdb_rating)) %&gt;% select(id,imdb_rating) %&gt;% head(1) WorstEpisode = Episodes %&gt;% arrange((imdb_rating)) %&gt;% select(id,imdb_rating) %&gt;% head(1) getEpisodeSentimentScore &lt;- function(ScriptsCharacters, ID) { SCBestEpisode = ScriptsCharacters %&gt;% #BestEpisode$id filter(episode_id == ID ) %&gt;% select(id,name,normalized_text) SCWords &lt;- SCBestEpisode %&gt;% unnest_tokens(word, normalized_text) %&gt;% filter(name != &quot;NA&quot;) %&gt;% dplyr::count(name, word, sort = TRUE) %&gt;% ungroup() SCWords_sentiments &lt;- SCWords %&gt;% inner_join(get_sentiments(&quot;afinn&quot;), by = &quot;word&quot;) %&gt;% summarize(score = sum(score * n) / sum(n)) return(SCWords_sentiments$score) } 51.1 Best Episode ## [1] 0.8309179 is the sentiment score for the Best Episode 51.2 Worst Episode ## [1] 0.4871795 is the sentiment score for the Worst Episode 51.3 Positive and Not So Positive Characters of the Best Episode SC = ScriptsCharacters %&gt;% filter(episode_id == BestEpisode$id ) %&gt;% unnest_tokens(word, normalized_text) %&gt;% dplyr::count(name, word, sort = TRUE) %&gt;% ungroup() visualize_sentiments(SC) 51.4 Positive and Not So Positive Characters of the Worst Episode SC = ScriptsCharacters %&gt;% filter(episode_id == WorstEpisode$id ) %&gt;% unnest_tokens(word, normalized_text) %&gt;% dplyr::count(name, word, sort = TRUE) %&gt;% ungroup() visualize_sentiments(SC) 51.5 Postive and Not So Postive Words of Best Episode The following graph shows the Twenty high positive and the negative words by Best Episode. positiveWordsBarGraph &lt;- function(SC) { contributions &lt;- SC %&gt;% unnest_tokens(word, normalized_text) %&gt;% dplyr::count(name, word, sort = TRUE) %&gt;% ungroup() %&gt;% inner_join(get_sentiments(&quot;afinn&quot;), by = &quot;word&quot;) %&gt;% group_by(word) %&gt;% summarize(occurences = n(), contribution = sum(score)) contributions %&gt;% top_n(20, abs(contribution)) %&gt;% mutate(word = reorder(word, contribution)) %&gt;% head(20) %&gt;% ggplot(aes(word, contribution, fill = contribution &gt; 0)) + geom_col(show.legend = FALSE) + coord_flip() + theme_bw() } SC = ScriptsCharacters %&gt;% #BestEpisode$id filter(episode_id == BestEpisode$id ) %&gt;% select(id,name,normalized_text) positiveWordsBarGraph(SC) 51.6 Postive and Not So Postive Words of Worst Episode SC = ScriptsCharacters %&gt;% #BestEpisode$id filter(episode_id == WorstEpisode$id ) %&gt;% select(id,name,normalized_text) positiveWordsBarGraph(SC) "],
["modelling-with-xgboost-1.html", "Chapter 52 Modelling with XGBoost", " Chapter 52 Modelling with XGBoost We try to predict whether the ScriptLines are spoken by Homer or not For ease of execution, we have taken only 5000 samples for the modelling Exercise. We do Cross Validation using Caret package. You can tune the parameters in your own machine for better results. The accuracy obtained through these parameters is quite good 0.7794007. Lastly we wish to examine the feature importance of the variables. This is shown in the flipped bar chart. ScriptsCharactersSample = ScriptsCharacters %&gt;% sample_n(5e3) corpus = Corpus(VectorSource(ScriptsCharactersSample$normalized_text)) # Pre-process data corpus &lt;- tm_map(corpus, tolower) corpus &lt;- tm_map(corpus, removePunctuation) corpus &lt;- tm_map(corpus, removeWords, stopwords(&quot;english&quot;)) corpus &lt;- tm_map(corpus, stemDocument) dtm = DocumentTermMatrix(corpus) # Remove sparse terms dtm = removeSparseTerms(dtm, 0.997) # Create data frame labeledTerms = as.data.frame(as.matrix(dtm)) ScriptsCharactersSample = ScriptsCharactersSample %&gt;% mutate(isHomer = 0) ScriptsCharactersSample = ScriptsCharactersSample %&gt;% mutate(isHomer=replace(isHomer, name == &#39;Homer Simpson&#39;, 1)) %&gt;% as.data.frame() labeledTerms$isHomer = as.factor(ScriptsCharactersSample$isHomer) ## Preparing the features for the XGBoost Model features &lt;- colnames(labeledTerms) for (f in features) { if ((class(labeledTerms[[f]])==&quot;factor&quot;) || (class(labeledTerms[[f]])==&quot;character&quot;)) { levels &lt;- unique(labeledTerms[[f]]) labeledTerms[[f]] &lt;- as.numeric(factor(labeledTerms[[f]], levels=levels)) } } ## Creating the XGBoost Model labeledTerms$isHomer = as.factor(labeledTerms$isHomer) formula = isHomer ~ . fitControl &lt;- trainControl(method=&quot;cv&quot;,number = 3) xgbGrid &lt;- expand.grid(nrounds = 10, max_depth = 3, eta = .05, gamma = 0, colsample_bytree = .8, min_child_weight = 1, subsample = 1) set.seed(13) HomerXGB = train(formula, data = labeledTerms, method = &quot;xgbTree&quot;,trControl = fitControl, tuneGrid = xgbGrid,na.action = na.pass) importance = varImp(HomerXGB) varImportance &lt;- data.frame(Variables = row.names(importance[[1]]), Importance = round(importance[[1]]$Overall,2)) # Create a rank variable based on importance rankImportance &lt;- varImportance %&gt;% mutate(Rank = paste0(&#39;#&#39;,dense_rank(desc(Importance)))) %&gt;% head(20) rankImportancefull = rankImportance ggplot(rankImportance, aes(x = reorder(Variables, Importance), y = Importance)) + geom_bar(stat=&#39;identity&#39;,colour=&quot;white&quot;, fill = fillColor) + geom_text(aes(x = Variables, y = 1, label = Rank), hjust=0, vjust=.5, size = 4, colour = &#39;black&#39;, fontface = &#39;bold&#39;) + labs(x = &#39;Variables&#39;, title = &#39;Relative Variable Importance&#39;) + coord_flip() + theme_bw() All the factors affecting the decision whether the ScriptLines are spoken by Homer or not along with their ranks is provided below Variables Importance Rank marg 100.00 #1 stupid 23.17 #2 woo 20.08 #3 moe 12.32 #4 simpson 10.67 #5 homer 10.25 #6 know 1.13 #7 want 0.57 #8 yeah 0.54 #9 now 0.26 #10 dont 0.07 #11 cut 0.00 #12 aint 0.00 #12 best 0.00 #12 come 0.00 #12 thing 0.00 #12 your 0.00 #12 good 0.00 #12 hes 0.00 #12 man 0.00 #12 "],
["introduction-3.html", "Chapter 53 Introduction", " Chapter 53 Introduction "],
["read-the-data-2.html", "Chapter 54 Read the data", " Chapter 54 Read the data rm(list=ls()) fillColor = &quot;#FFA07A&quot; fillColor2 = &quot;#F1C40F&quot; train &lt;- read_tsv(&quot;input/MoviesSentimentAnalysis/train.tsv&quot;) test &lt;- read_tsv(&quot;input/MoviesSentimentAnalysis/test.tsv&quot;) glimpse(train) ## Observations: 156,060 ## Variables: 4 ## $ PhraseId &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, ... ## $ SentenceId &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ... ## $ Phrase &lt;chr&gt; &quot;A series of escapades demonstrating the adage that... ## $ Sentiment &lt;int&gt; 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ... "],
["tokenisation.html", "Chapter 55 Tokenisation 55.1 Removing the Stop words", " Chapter 55 Tokenisation We break the text into individual tokens which are simply individual words. This process is called tokenisation. This is accomplished through the unnest_tokens function. train &lt;- train %&gt;% dplyr::rename(text = Phrase) test &lt;- test %&gt;% dplyr::rename(text = Phrase) train %&gt;% unnest_tokens(word, text) %&gt;% head(10) ## # A tibble: 10 x 4 ## PhraseId SentenceId Sentiment word ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 1 1 1 a ## 2 1 1 1 series ## 3 1 1 1 of ## 4 1 1 1 escapades ## 5 1 1 1 demonstrating ## 6 1 1 1 the ## 7 1 1 1 adage ## 8 1 1 1 that ## 9 1 1 1 what ## 10 1 1 1 is 55.1 Removing the Stop words movie_stopwords = c(&quot;movie&quot;,&quot;film&quot;) train %&gt;% unnest_tokens(word, text) %&gt;% filter(!word %in% stop_words$word) %&gt;% filter(!word %in% movie_stopwords) %&gt;% head(10) ## # A tibble: 10 x 4 ## PhraseId SentenceId Sentiment word ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 1 1 1 series ## 2 1 1 1 escapades ## 3 1 1 1 demonstrating ## 4 1 1 1 adage ## 5 1 1 1 goose ## 6 1 1 1 gander ## 7 1 1 1 occasionally ## 8 1 1 1 amuses ## 9 1 1 1 amounts ## 10 1 1 1 story "],
["top-ten-most-common-words.html", "Chapter 56 Top Ten most Common Words 56.1 WordCloud of the Common Words 56.2 Word Cloud of Negative Sentiments 56.3 Word Cloud of somewhat negative Sentiments 56.4 Word Cloud of neutral Sentiments 56.5 Word Cloud of somewhat positive Sentiments 56.6 Word Cloud of positive Sentiments", " Chapter 56 Top Ten most Common Words createBarPlotCommonWords = function(train,title) { train %&gt;% unnest_tokens(word, text) %&gt;% filter(!word %in% stop_words$word) %&gt;% filter(!word %in% movie_stopwords) %&gt;% dplyr::count(word,sort = TRUE) %&gt;% ungroup() %&gt;% mutate(word = factor(word, levels = rev(unique(word)))) %&gt;% head(10) %&gt;% ggplot(aes(x = word,y = n)) + geom_bar(stat=&#39;identity&#39;,colour=&quot;white&quot;, fill =fillColor) + geom_text(aes(x = word, y = 1, label = paste0(&quot;(&quot;,n,&quot;)&quot;,sep=&quot;&quot;)), hjust=0, vjust=.5, size = 4, colour = &#39;black&#39;, fontface = &#39;bold&#39;) + labs(x = &#39;Word&#39;, y = &#39;Word Count&#39;, title = title) + coord_flip() + theme_bw() } createBarPlotCommonWords(train,&#39;Top 10 most Common Words&#39;) 56.1 WordCloud of the Common Words A word cloud is a graphical representation of frequently used words in the text. The height of each word in this picture is an indication of frequency of occurrence of the word in the entire text. createWordCloud = function(train) { train %&gt;% unnest_tokens(word, text) %&gt;% filter(!word %in% stop_words$word) %&gt;% filter(!word %in% movie_stopwords) %&gt;% dplyr::count(word,sort = TRUE) %&gt;% ungroup() %&gt;% head(30) %&gt;% with(wordcloud(word, n, max.words = 30,colors=brewer.pal(8, &quot;Dark2&quot;))) } createWordCloud(train) 56.2 Word Cloud of Negative Sentiments #0 = Negative createWordCloud(train %&gt;% filter(Sentiment == 0)) 56.3 Word Cloud of somewhat negative Sentiments #1 = somewhat negative createWordCloud(train %&gt;% filter(Sentiment == 1)) 56.4 Word Cloud of neutral Sentiments #2 = neutral createWordCloud(train %&gt;% filter(Sentiment == 2)) 56.5 Word Cloud of somewhat positive Sentiments #3 = somewhat positive createWordCloud(train %&gt;% filter(Sentiment == 3)) 56.6 Word Cloud of positive Sentiments #4 = positive createWordCloud(train %&gt;% filter(Sentiment == 4)) "],
["tf-idf-2.html", "Chapter 57 TF-IDF 57.1 The Math 57.2 Twenty Most Important Words", " Chapter 57 TF-IDF We wish to find out the important words which are spoken by the characters. Example for your young child , the most important word is mom. Example for a bar tender , important words would be related to drinks. We would explore this using a fascinating concept known as Term Frequency - Inverse Document Frequency. Quite a mouthful, but we will unpack it and clarify each and every term. A document in this case is the set of lines associated with a Sentiment.Therefore we have different documents for each Sentiment. From the book 5 Algorithms Every Web Developer Can Use and Understand TF-IDF computes a weight which represents the importance of a term inside a document. It does this by comparing the frequency of usage inside an individual document as opposed to the entire data set (a collection of documents). The importance increases proportionally to the number of times a word appears in the individual document itself–this is called Term Frequency. However, if multiple documents contain the same word many times then you run into a problem. That’s why TF-IDF also offsets this value by the frequency of the term in the entire document set, a value called Inverse Document Frequency. 57.1 The Math TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document) IDF(t) = log_e(Total number of documents / Number of documents with term t in it). Value = TF * IDF 57.2 Twenty Most Important Words trainWords &lt;- train %&gt;% unnest_tokens(word, text) %&gt;% dplyr::count(Sentiment, word, sort = TRUE) %&gt;% ungroup() total_words &lt;- trainWords %&gt;% group_by(Sentiment) %&gt;% dplyr::summarize(total = sum(n)) trainWords &lt;- dplyr::left_join(trainWords, total_words, by=&quot;Sentiment&quot;) #Now we are ready to use the bind_tf_idf which computes the tf-idf for each term. trainWords &lt;- trainWords %&gt;% filter(!is.na(Sentiment)) %&gt;% bind_tf_idf(word, Sentiment, n) plot_trainWords &lt;- trainWords %&gt;% arrange(desc(tf_idf)) %&gt;% mutate(word = factor(word, levels = rev(unique(word)))) plot_trainWords %&gt;% top_n(20) %&gt;% ggplot(aes(word, tf_idf)) + geom_col(fill = fillColor) + labs(x = NULL, y = &quot;tf-idf&quot;) + coord_flip() + theme_bw() "],
["most-common-bigrams-1.html", "Chapter 58 Most Common Bigrams", " Chapter 58 Most Common Bigrams A Bigram is a collection of Two words. We examine the most common Bigrams and plot them in a bar plot. count_bigrams &lt;- function(dataset) { dataset %&gt;% unnest_tokens(bigram, text, token = &quot;ngrams&quot;, n = 2) %&gt;% separate(bigram, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) %&gt;% filter(!word1 %in% stop_words$word, !word2 %in% stop_words$word) %&gt;% dplyr::count(word1, word2, sort = TRUE) } visualize_bigrams &lt;- function(bigrams) { set.seed(2016) a &lt;- grid::arrow(type = &quot;closed&quot;, length = unit(.15, &quot;inches&quot;)) bigrams %&gt;% graph_from_data_frame() %&gt;% ggraph(layout = &quot;fr&quot;) + geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a) + geom_node_point(color = &quot;lightblue&quot;, size = 5) + geom_node_text(aes(label = name), vjust = 1, hjust = 1) + theme_void() } visualize_bigrams_individual &lt;- function(bigrams) { set.seed(2016) a &lt;- grid::arrow(type = &quot;closed&quot;, length = unit(.15, &quot;inches&quot;)) bigrams %&gt;% graph_from_data_frame() %&gt;% ggraph(layout = &quot;fr&quot;) + geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a,end_cap = circle(.07, &#39;inches&#39;)) + geom_node_point(color = &quot;lightblue&quot;, size = 5) + geom_node_text(aes(label = name), vjust = 1, hjust = 1) + theme_void() } train %&gt;% unnest_tokens(bigram, text, token = &quot;ngrams&quot;, n = 2) %&gt;% separate(bigram, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) %&gt;% filter(!word1 %in% stop_words$word, !word2 %in% stop_words$word) %&gt;% unite(bigramWord, word1, word2, sep = &quot; &quot;) %&gt;% group_by(bigramWord) %&gt;% tally() %&gt;% ungroup() %&gt;% arrange(desc(n)) %&gt;% mutate(bigramWord = reorder(bigramWord,n)) %&gt;% head(10) %&gt;% ggplot(aes(x = bigramWord,y = n)) + geom_bar(stat=&#39;identity&#39;,colour=&quot;white&quot;, fill = fillColor2) + geom_text(aes(x = bigramWord, y = 1, label = paste0(&quot;(&quot;,n,&quot;)&quot;,sep=&quot;&quot;)), hjust=0, vjust=.5, size = 4, colour = &#39;black&#39;, fontface = &#39;bold&#39;) + labs(x = &#39;Bigram&#39;, y = &#39;Count&#39;, title = &#39;Bigram and Count&#39;) + coord_flip() + theme_bw() "],
["most-common-trigrams-1.html", "Chapter 59 Most Common Trigrams", " Chapter 59 Most Common Trigrams A Trigram is a collection of Three words. We examine the most common Trigrams and plot them in a bar plot. train %&gt;% unnest_tokens(trigram, text, token = &quot;ngrams&quot;, n = 3) %&gt;% separate(trigram, c(&quot;word1&quot;, &quot;word2&quot;,&quot;word3&quot;), sep = &quot; &quot;) %&gt;% filter(!word1 %in% stop_words$word, !word2 %in% stop_words$word, !word3 %in% stop_words$word) %&gt;% unite(trigramWord, word1, word2, word3,sep = &quot; &quot;) %&gt;% group_by(trigramWord) %&gt;% tally() %&gt;% ungroup() %&gt;% arrange(desc(n)) %&gt;% mutate(trigramWord = reorder(trigramWord,n)) %&gt;% head(10) %&gt;% ggplot(aes(x = trigramWord,y = n)) + geom_bar(stat=&#39;identity&#39;,colour=&quot;white&quot;, fill = fillColor2) + geom_text(aes(x = trigramWord, y = 1, label = paste0(&quot;(&quot;,n,&quot;)&quot;,sep=&quot;&quot;)), hjust=0, vjust=.5, size = 4, colour = &#39;black&#39;, fontface = &#39;bold&#39;) + labs(x = &#39;Trigram&#39;, y = &#39;Count&#39;, title = &#39;Trigram and Count&#39;) + coord_flip() + theme_bw() "],
["relationship-among-words-3.html", "Chapter 60 Relationship among words", " Chapter 60 Relationship among words Til now, we have explored the most important words for sentiment. Now, we will explore the relationship between words. trainWords &lt;- train %&gt;% count_bigrams() trainWords %&gt;% filter(n &gt; 50) %&gt;% visualize_bigrams() "],
["modelling-using-the-text2vec-package-1.html", "Chapter 61 Modelling using the text2vec package 61.1 Inspect the vocabulary 61.2 Inspect the Document Term Matrix 61.3 TF-IDF 61.4 Build the Multinomial Logistic Regression Model 61.5 Predict using the Multinomial Logistic Regression Model", " Chapter 61 Modelling using the text2vec package We create a vocabulary-based DTM. Here we collect unique terms from all documents and mark each of them with a unique ID using the create_vocabulary() function. We use an iterator to create the vocabulary. We also prune the vocabulary to reduce the terms in the matrix. prep_fun = function(x) { stringr::str_replace_all(tolower(x), &quot;[^[:alpha:]]&quot;, &quot; &quot;) } tok_fun = word_tokenizer it_train = itoken(train$text, preprocessor = prep_fun, tokenizer = tok_fun, ids = train$id, progressbar = FALSE) it_test = test$text %&gt;% prep_fun %&gt;% tok_fun %&gt;% itoken(ids = test$id, progressbar = FALSE) NFOLDS = 4 vocab = create_vocabulary(it_train, ngram = c(1L, 3L)) vocab = vocab %&gt;% prune_vocabulary(term_count_min = 10, doc_proportion_max = 0.5, doc_proportion_min = 0.01,vocab_term_max = 5000) trigram_vectorizer = vocab_vectorizer(vocab) dtm_train = create_dtm(it_train, trigram_vectorizer) 61.1 Inspect the vocabulary vocab 61.2 Inspect the Document Term Matrix dim(dtm_train) 61.3 TF-IDF # define tfidf model tfidf = TfIdf$new() # fit model to train data and transform train data with fitted model dtm_train_tfidf = fit_transform(dtm_train, tfidf) # tfidf modified by fit_transform() call! # apply pre-trained tf-idf transformation to test data dtm_test_tfidf = create_dtm(it_test, trigram_vectorizer) dtm_test_tfidf = transform(dtm_test_tfidf, tfidf) 61.4 Build the Multinomial Logistic Regression Model glmnet_classifier = cv.glmnet(x = dtm_train_tfidf, y = train[[&#39;Sentiment&#39;]], family = &#39;multinomial&#39;, alpha = 1, type.measure = &quot;class&quot;, nfolds = NFOLDS, thresh = 1e-3, maxit = 1e3) 61.5 Predict using the Multinomial Logistic Regression Model predictions = data.frame(PhraseId=test$PhraseId,Sentiment = predict(glmnet_classifier, dtm_test_tfidf,type=&quot;class&quot;)) options(scipen = 999) predictions &lt;- predictions %&gt;% rename(Sentiment = X1) predictions$PhraseId = as.numeric(predictions$PhraseId) predictions$Sentiment = as.numeric(predictions$Sentiment) write.csv(predictions, &#39;glmnet.csv&#39;, row.names = F) "],
["introduction-4.html", "Chapter 62 Introduction", " Chapter 62 Introduction Restaurant inspections ensure that food served to the public at licensed food establishments follows food safety guidelines. The Food Protection Division of the Chicago Department of Public Health (CDPH) is committed to maintaining the safety of food bought, sold, or prepared for public consumption in Chicago by carrying out science-based inspections of all retail food establishments. These inspections promote public health in areas of food safety and sanitation and prevent the occurrence of food-borne illness. CDPH’s licensed, accredited sanitarians inspect retail food establishments such as restaurants, grocery stores, bakeries, convenience stores, hospitals, nursing homes, day care facilities, shelters, schools, and temporary food service events. Inspections focus on food handling practices, product temperatures, personal hygiene, facility maintenance, and pest control. All restaurants are subject to certain recurring inspections. Each year a restaurant is subject to annual inspections to ensure continued compliance with City ordinances and regulations. In addition to recurring inspections, restaurants may also be inspected in response to a complaint. Some of these recurring inspections, such as the inspection by the Buildings Department, will be scheduled, while others will not. Generally inspections are conducted by the Health Department for sanitation and safe food handling practices, the Buildings Department to ensure the safety of the structure, and the Fire Department to ensure safe fire exits.The City’s Dumpster Task Force, a collaborative effort between the Health Department and Streets and Sanitation Department, also inspects restaurants to ensure compliance with sanitation regulations. library(DT) # table format display of data library(leaflet) # maps library(lubridate) rm(list=ls()) fillColor = &quot;#FFA07A&quot; fillColor2 = &quot;#F1C40F&quot; FoodInspections = read_csv(&quot;input/Food_Inspections.csv&quot;) "],
["analysis-of-data-having-values-percentage.html", "Chapter 63 Analysis of Data ( Having Values Percentage)", " Chapter 63 Analysis of Data ( Having Values Percentage) PercentageOfNonNullValues = ( colSums(!is.na(FoodInspections)) / nrow(FoodInspections) ) * 100 PercentageOfNonNullValues = as.data.frame(PercentageOfNonNullValues) PercentageOfNonNullValues = rownames_to_column(PercentageOfNonNullValues,&quot;VariableName&quot;) PercentageOfNonNullValues = PercentageOfNonNullValues %&gt;% arrange(desc(PercentageOfNonNullValues)) PercentageOfNonNullValues = PercentageOfNonNullValues %&gt;% mutate(VariableName = reorder(VariableName,PercentageOfNonNullValues)) ggplot(PercentageOfNonNullValues, aes(x = VariableName,y = PercentageOfNonNullValues)) + geom_bar(stat=&#39;identity&#39;,colour=&quot;white&quot;, fill =fillColor2) + geom_text(aes(x = VariableName, y = 1, label = paste0(&quot;(&quot;,round(PercentageOfNonNullValues,2),&quot;)&quot;,sep=&quot;&quot;)), hjust=0, vjust=.5, size = 4, colour = &#39;black&#39;, fontface = &#39;bold&#39;) + labs(x = &#39;VariableName&#39;, y = &#39;PercentageOfNonNullValues&#39;, title = &#39;Variable Name and PercentageOfNonNullValues&#39;) + coord_flip() + theme_bw() We observe all the variables except one have more than 97% values. Therefore very few missing values, looking forward for an interesting analysis. "],
["top-twenty-facility-types.html", "Chapter 64 Top Twenty Facility Types", " Chapter 64 Top Twenty Facility Types TopFacilityTypes = FoodInspections %&gt;% mutate(FacilityType = as.factor(`Facility Type`)) %&gt;% select(-(`Facility Type`)) %&gt;% filter(!is.na(FacilityType)) %&gt;% group_by(FacilityType) %&gt;% dplyr::summarise(CountResults = n()) %&gt;% arrange(desc(CountResults)) %&gt;% mutate(FacilityType = reorder(FacilityType,CountResults)) %&gt;% head(20) TopFacilityTypes %&gt;% ggplot(aes(x = FacilityType,y = CountResults)) + geom_bar(stat=&#39;identity&#39;,colour=&quot;white&quot;, fill =fillColor2) + geom_text(aes(x = FacilityType, y = 1, label = paste0(&quot;(&quot;,CountResults,&quot;)&quot;,sep=&quot;&quot;)), hjust=0, vjust=.5, size = 4, colour = &#39;black&#39;, fontface = &#39;bold&#39;) + labs(x = &#39;Results&#39;, y = &#39;CountResults&#39;, title = &#39;Results Analysis&#39;) + coord_flip() + theme_bw() Restaurant is the most common Facility type. "],
["inspection-results-for-top-twenty-facility-types.html", "Chapter 65 Inspection Results for Top Twenty Facility Types", " Chapter 65 Inspection Results for Top Twenty Facility Types FoodInspections %&gt;% mutate(FacilityType = as.factor(`Facility Type`)) %&gt;% filter(FacilityType %in% TopFacilityTypes$FacilityType) %&gt;% group_by(FacilityType,Results) %&gt;% dplyr::summarise(CountResults = n()) %&gt;% ggplot(aes(x = FacilityType,y = CountResults, fill =Results)) + geom_bar(stat=&#39;identity&#39;,colour=&quot;white&quot;) + labs(x = &#39;FacilityType&#39;, y = &#39;CountResults&#39;, title = &#39;Facilities with Different Results Analysis&#39;) + coord_flip() + theme_bw() "],
["trend-of-inspections-yearwise.html", "Chapter 66 Trend of Inspections YearWise 66.1 All Results 66.2 Each Result Category", " Chapter 66 Trend of Inspections YearWise 66.1 All Results FoodInspections %&gt;% select(Results,`Inspection Date`) %&gt;% mutate(Results = as.factor(Results)) %&gt;% dplyr::rename(InspectionDate = `Inspection Date`) %&gt;% mutate(yr = year(mdy(InspectionDate))) %&gt;% group_by(yr,Results) %&gt;% dplyr::summarise(InspectionsCount = n()) %&gt;% ggplot(aes(x=yr,y=InspectionsCount, fill=Results)) + geom_bar(stat = &#39;identity&#39;) + labs(x = &#39;Year&#39;, y = &#39;Count of Inspections&#39;, title = &#39;Trend of Inspections&#39;) + theme_bw() + theme(legend.position=&quot;top&quot;) The number of inspections show a very slight increasing trend. 66.2 Each Result Category We examine the inspections in each of the Years for all Results in seperate bar plots representing each Result Type.The Out of Business is very high in 2013. FoodInspections %&gt;% select(Results,`Inspection Date`) %&gt;% mutate(Results = as.factor(Results)) %&gt;% dplyr::rename(InspectionDate = `Inspection Date`) %&gt;% mutate(yr = year(mdy(InspectionDate))) %&gt;% group_by(yr,Results) %&gt;% dplyr::summarise(InspectionsCount = n()) %&gt;% ggplot(aes(x=yr,y=InspectionsCount, fill=Results)) + geom_bar(stat = &#39;identity&#39;) + facet_wrap(~Results, ncol = 2, scales = &quot;free_y&quot;) + labs(x = &#39;Month&#39;, y = &#39;Count of Inspections&#39;, title = &#39;Trend of Inspections&#39;) + theme_bw() + theme(legend.position=&quot;top&quot;) "],
["trend-of-inspections-monthwise.html", "Chapter 67 Trend of Inspections Monthwise 67.1 All Results 67.2 Each Result category 67.3 Out of Business Result", " Chapter 67 Trend of Inspections Monthwise We examine the inspections Monthwise in Three different perspectives Trend Monthwise for all Results Trend Monthwise for all Results in seperate bar plots representing each Result Type Trend Monthwise for Out of Business Result 67.1 All Results We examine the inspections in each of the months.Inspections dip in Nov, Dec which is expected because of the holiday seasons. I do not understand why the inspections dip ubruptly in July. breaks = seq(1,12,1) FoodInspections %&gt;% select(Results,`Inspection Date`) %&gt;% mutate(Results = as.factor(Results)) %&gt;% dplyr::rename(InspectionDate = `Inspection Date`) %&gt;% mutate(month_of_year = month(mdy(InspectionDate))) %&gt;% group_by(month_of_year,Results) %&gt;% dplyr::summarise(InspectionsCount = n()) %&gt;% ggplot(aes(x=month_of_year,y=InspectionsCount, fill=Results)) + geom_bar(stat = &#39;identity&#39;) + scale_x_continuous(limits = c(0,13),breaks=breaks ) + labs(x = &#39;Month&#39;, y = &#39;Count of Inspections&#39;, title = &#39;Trend of Inspections&#39;) + theme_bw() + theme(legend.position=&quot;top&quot;) 67.2 Each Result category The Out Of Business categories increase in Nov and Dec, though the number of inspections decrease in Nov and Dec. In the next section, lets plot only the Out of Business categories. FoodInspections %&gt;% select(Results,`Inspection Date`) %&gt;% mutate(Results = as.factor(Results)) %&gt;% dplyr::rename(InspectionDate = `Inspection Date`) %&gt;% mutate(month_of_year = month(mdy(InspectionDate))) %&gt;% group_by(month_of_year,Results) %&gt;% dplyr::summarise(InspectionsCount = n()) %&gt;% ggplot(aes(x=month_of_year,y=InspectionsCount, fill=Results)) + geom_bar(stat = &#39;identity&#39;) + scale_x_continuous(limits = c(0,13),breaks=breaks ) + facet_wrap(~Results, ncol = 2, scales = &quot;free_y&quot;) + labs(x = &#39;Month&#39;, y = &#39;Count of Inspections&#39;, title = &#39;Trend of Inspections&#39;) + theme_bw() + theme(legend.position=&quot;top&quot;) 67.3 Out of Business Result FoodInspections %&gt;% select(Results,`Inspection Date`) %&gt;% filter(Results == &quot;Out of Business&quot;) %&gt;% dplyr::rename(InspectionDate = `Inspection Date`) %&gt;% mutate(month_of_year = month(mdy(InspectionDate))) %&gt;% group_by(month_of_year) %&gt;% dplyr::summarise(InspectionsCount = n()) %&gt;% ggplot(aes(x=month_of_year,y=InspectionsCount)) + geom_bar(stat = &#39;identity&#39; ,colour=&quot;white&quot;, fill =fillColor) + scale_x_continuous(limits = c(0,13),breaks=breaks ) + labs(x = &#39;Month&#39;, y = &#39;Count of Inspections&#39;, title = &#39;Trend of Out of Business Inspections&#39;) + theme_bw() + theme(legend.position=&quot;top&quot;) "],
["trend-of-inspections-daywise.html", "Chapter 68 Trend of Inspections Daywise", " Chapter 68 Trend of Inspections Daywise We examine the Inspections from the day of week perspective. Wednesday is the day which has the lowest inspections. FoodInspections %&gt;% select(Results,`Inspection Date`) %&gt;% mutate(Results = as.factor(Results)) %&gt;% dplyr::rename(InspectionDate = `Inspection Date`) %&gt;% mutate(dayOfWeek = wday(mdy(InspectionDate),label = TRUE)) %&gt;% group_by(dayOfWeek,Results) %&gt;% dplyr::summarise(InspectionsCount = n()) %&gt;% ggplot(aes(x=dayOfWeek,y=InspectionsCount, fill=Results)) + geom_bar(stat = &#39;identity&#39;) + labs(x = &#39;Day of Week&#39;, y = &#39;Count of Inspections&#39;, title = &#39;Trend of Inspections Day Wise&#39;) + theme_bw() + theme(legend.position=&quot;top&quot;) "],
["maps-of-food-places.html", "Chapter 69 Maps of Food Places 69.1 Pass or Fail Spots 69.2 Out of Business Spots", " Chapter 69 Maps of Food Places 69.1 Pass or Fail Spots ResultsPassORFail = c(&quot;Pass&quot;,&quot;Fail&quot;) factpal &lt;- colorFactor(c(&quot;gray&quot;,&quot;red&quot;,&quot;purple&quot;,&quot;yellow&quot;,&quot;orange&quot;,&quot;green&quot;,&quot;blue&quot;), FoodInspections$Results) FoodInspectionsSubSet = FoodInspections %&gt;% sample_n(8e3) %&gt;% filter(Results %in% ResultsPassORFail) leaflet(FoodInspectionsSubSet) %&gt;% addProviderTiles(&quot;Esri.NatGeoWorldMap&quot;) %&gt;% addCircles(lng = ~Longitude, lat = ~Latitude,radius = 1, color = ~factpal(Results)) %&gt;% addLegend(&quot;bottomright&quot;, pal = factpal, values = ~Results, title = &quot;Locations of Food Places in Chicago&quot;, opacity = 1) The plot shows more Greens i.e. Pass across the shoreline than the Reds i.e. Fail. You may want to zoom in and zoom out and play with the map to find out more. 69.2 Out of Business Spots ResultsOOB = c(&quot;Out of Business&quot;) factpal &lt;- colorFactor(c(&quot;gray&quot;,&quot;red&quot;,&quot;purple&quot;,&quot;yellow&quot;,&quot;orange&quot;,&quot;green&quot;,&quot;blue&quot;), FoodInspections$Results) FoodInspectionsSubSet = FoodInspections %&gt;% sample_n(8e3) %&gt;% filter(Results %in% ResultsOOB) leaflet(FoodInspectionsSubSet) %&gt;% addProviderTiles(&quot;Esri.NatGeoWorldMap&quot;) %&gt;% addCircles(lng = ~Longitude, lat = ~Latitude,radius = 1, color = ~factpal(Results)) %&gt;% addLegend(&quot;bottomright&quot;, pal = factpal, values = ~Results, title = &quot;Locations of Food Places in Chicago&quot;, opacity = 1) FoodInspectionsReduced = FoodInspections %&gt;% mutate(InspectionID = `Inspection ID`) %&gt;% select(InspectionID,Violations,Results) FoodInspectionWords &lt;- FoodInspectionsReduced %&gt;% unnest_tokens(word, Violations) %&gt;% filter(!word %in% stop_words$word) %&gt;% dplyr::count(Results, word, sort = TRUE) %&gt;% ungroup() "],
["top-twenty-most-common-words-2.html", "Chapter 70 Top Twenty most Common Words 70.1 WordCloud of the Common Words", " Chapter 70 Top Twenty most Common Words We examine the Top Twenty Most Common words and show them in a bar graph. FoodInspectionsReduced %&gt;% unnest_tokens(word, Violations) %&gt;% filter(!word %in% stop_words$word) %&gt;% dplyr::count(word,sort = TRUE) %&gt;% ungroup() %&gt;% mutate(word = factor(word, levels = rev(unique(word)))) %&gt;% head(20) %&gt;% ggplot(aes(x = word,y = n)) + geom_bar(stat=&#39;identity&#39;,colour=&quot;white&quot;, fill =fillColor) + geom_text(aes(x = word, y = 1, label = paste0(&quot;(&quot;,n,&quot;)&quot;,sep=&quot;&quot;)), hjust=0, vjust=.5, size = 4, colour = &#39;black&#39;, fontface = &#39;bold&#39;) + labs(x = &#39;Word&#39;, y = &#39;Word Count&#39;, title = &#39;Top 20 most Common Words&#39;) + coord_flip() + theme_bw() 70.1 WordCloud of the Common Words A word cloud is a graphical representation of frequently used words in the Violations text. The height of each word in this picture is an indication of frequency of occurrence of the word in the entire text.Comments , equipment , clean and food are some of the most commonly occuring terms. FoodInspectionsReduced %&gt;% unnest_tokens(word, Violations) %&gt;% filter(!word %in% stop_words$word) %&gt;% dplyr::count(word,sort = TRUE) %&gt;% ungroup() %&gt;% mutate(word = factor(word, levels = rev(unique(word)))) %&gt;% head(50) %&gt;% with(wordcloud(word, n, max.words = 50,colors=brewer.pal(8, &quot;Dark2&quot;))) "],
["tf-idf-theory.html", "Chapter 71 TF - IDF Theory 71.1 The Math", " Chapter 71 TF - IDF Theory Unigrams are words of length 1 , therefore they are single words.BiGrams are words of length 2. We wish to find out the important UniGrams and BiGrams which are present in the Violations Text. We would explore this using a fascinating concept known as Term Frequency - Inverse Document Frequency. Quite a mouthful, but we will unpack it and clarify each and every term. A document in this case is the set of Violation Text present in Results Type. E.g. The Violation Text present in Results Type Pass is a single document.The Violation Text present in Result Type Fail is a single document Therefore we have different documents for each Results Type. From the book 5 Algorithms Every Web Developer Can Use and Understand TF-IDF computes a weight which represents the importance of a term inside a document. It does this by comparing the frequency of usage inside an individual document as opposed to the entire data set (a collection of documents). The importance increases proportionally to the number of times a word appears in the individual document itself–this is called Term Frequency. However, if multiple documents contain the same word many times then you run into a problem. That’s why TF-IDF also offsets this value by the frequency of the term in the entire document set, a value called Inverse Document Frequency. 71.1 The Math TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document) IDF(t) = log_e(Total number of documents / Number of documents with term t in it). Value = TF * IDF "],
["term-frequency-of-words-tf.html", "Chapter 72 Term Frequency of Words (TF)", " Chapter 72 Term Frequency of Words (TF) Here we examine the Term Frequency of the words. The commonly occuring Terms are not considered in the bar plot. The commonly occurring terms would be present in the Tail of the plot. FoodInspectionResultsWords &lt;- FoodInspectionWords TotalWordsPerResult &lt;- FoodInspectionResultsWords %&gt;% group_by(Results) %&gt;% dplyr::summarize(total = sum(n)) FoodInspectionResultsWords &lt;- left_join(FoodInspectionResultsWords, TotalWordsPerResult) FoodInspectionResultsWords = FoodInspectionResultsWords %&gt;% filter (!is.na(Results)) ggplot(FoodInspectionResultsWords, aes(n/total, fill = Results)) + geom_histogram(bins = 30, show.legend = FALSE) + xlim(NA, 0.0009) + facet_wrap(~Results, ncol = 2, scales = &quot;free_y&quot;) + theme_bw() "],
["tf-idf-of-unigrams-one-word.html", "Chapter 73 TF- IDF of Unigrams (One Word )", " Chapter 73 TF- IDF of Unigrams (One Word ) We examine the TF-IDF of single words in the following bar graph.This shows the Top Twenty most Important words. FoodInspectionWords_TF_IDF &lt;- FoodInspectionWords %&gt;% bind_tf_idf(word, Results, n) #Choose words with low IDF LowIDF = FoodInspectionWords_TF_IDF %&gt;% arrange((idf)) %&gt;% select(word,idf) #Get the Unique Words with LowIDF UniqueLowIDF = unique(LowIDF$word) plot_FoodInspectionWords_TF_IDF &lt;- FoodInspectionWords_TF_IDF %&gt;% arrange(desc(tf_idf)) %&gt;% mutate(word = factor(word, levels = rev(unique(word)))) plot_FoodInspectionWords_TF_IDF %&gt;% top_n(20) %&gt;% ggplot(aes(word, tf_idf, fill = Results)) + geom_col() + labs(x = NULL, y = &quot;tf-idf&quot;) + coord_flip() + theme_bw() "],
["word-cloud-for-unigrams.html", "Chapter 74 Word Cloud for Unigrams", " Chapter 74 Word Cloud for Unigrams We show the Fifty most important words. This Word Cloud is based on the TF- IDF scores. Higher the score, bigger is the size of the text. plot_FoodInspectionWords_TF_IDF2 = plot_FoodInspectionWords_TF_IDF %&gt;% top_n(100) plot_FoodInspectionWords_TF_IDF2 %&gt;% with(wordcloud(word, tf_idf, max.words = 50,colors=brewer.pal(8, &quot;Dark2&quot;))) "],
["tf-idf-bigrams.html", "Chapter 75 TF - IDF Bigrams", " Chapter 75 TF - IDF Bigrams FoodInspectionWordsBiGram &lt;- FoodInspectionsReduced %&gt;% unnest_tokens(bigram, Violations, token = &quot;ngrams&quot;, n = 2) bigrams_separated &lt;- FoodInspectionWordsBiGram %&gt;% separate(bigram, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) bigrams_filtered &lt;- bigrams_separated %&gt;% filter(!word1 %in% stop_words$word) %&gt;% filter(!word2 %in% stop_words$word) # new bigram counts: bigram_counts &lt;- bigrams_filtered %&gt;% dplyr::count(word1, word2, sort = TRUE) bigrams_united &lt;- bigrams_filtered %&gt;% unite(bigram, word1, word2, sep = &quot; &quot;) bigram_tf_idf &lt;- bigrams_united %&gt;% dplyr::count(Results, bigram) %&gt;% bind_tf_idf(bigram, Results, n) plot_FoodInspectionWords_TF_IDF &lt;- bigram_tf_idf %&gt;% arrange(desc(tf_idf)) %&gt;% mutate(bigram = factor(bigram, levels = rev(unique(bigram)))) plot_FoodInspectionWords_TF_IDF %&gt;% top_n(15) %&gt;% ggplot(aes(bigram, tf_idf, fill = Results)) + geom_col() + labs(x = NULL, y = &quot;tf-idf&quot;) + coord_flip() + theme_bw() Therefore for Out of Business, the most important words are as follows:: “comments reflected”, “license 34”, “licensee inspection”, “license 38” "],
["verification-of-important-words-for-out-of-business.html", "Chapter 76 Verification of important words for “Out of Business”", " Chapter 76 Verification of important words for “Out of Business” OOBList = c(&quot;comments reflected&quot;, &quot;license 34&quot;, &quot;licensee inspection&quot;, &quot;license 38&quot;) FoodInspectionWordsFiltered = FoodInspectionWordsBiGram %&gt;% filter(bigram %in% OOBList) table(FoodInspectionWordsFiltered$Results) ## ## Fail Out of Business ## 1 18 We show that if we look for the following bigrams “comments reflected”, “license 34”, “licensee inspection”, “license 38” then most commonly occuring Result is Out of Business. "],
["word-cloud-for-bigrams.html", "Chapter 77 Word Cloud for Bigrams", " Chapter 77 Word Cloud for Bigrams We show the Fifty most important bigrams. This Word Cloud is based on the TF- IDF scores. Higher the score, bigger is the size of the text. bigrams_united_withSeparator &lt;- bigrams_filtered %&gt;% unite(bigram, word1, word2, sep = &quot;_&quot;) bigram_tf_idf_withSeparator &lt;- bigrams_united_withSeparator %&gt;% dplyr::count(Results, bigram) %&gt;% bind_tf_idf(bigram, Results, n) %&gt;% arrange(desc(tf_idf)) bigram_tf_idf_withSeparator %&gt;% with(wordcloud(bigram, tf_idf, max.words = 50,colors=brewer.pal(8, &quot;Dark2&quot;))) The bigrams shown are more informative than the single words. Example hazardous food , licensee 34 refer to some specific Results. "],
["relationship-among-words-4.html", "Chapter 78 Relationship among words", " Chapter 78 Relationship among words Til now, we have explored the most important words for a character. Now, we will explore the relationship between words which occur more than 30,000 times. #Relationship among words count_bigrams &lt;- function(dataset) { dataset %&gt;% unnest_tokens(bigram, Violations, token = &quot;ngrams&quot;, n = 2) %&gt;% separate(bigram, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) %&gt;% filter(!word1 %in% stop_words$word, !word2 %in% stop_words$word) %&gt;% dplyr::count(word1, word2, sort = TRUE) } visualize_bigrams &lt;- function(bigrams) { set.seed(2016) a &lt;- grid::arrow(type = &quot;closed&quot;, length = unit(.15, &quot;inches&quot;)) bigrams %&gt;% graph_from_data_frame() %&gt;% ggraph(layout = &quot;fr&quot;) + geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a) + geom_node_point(color = &quot;lightblue&quot;, size = 5) + geom_node_text(aes(label = name), vjust = 1, hjust = 1) + theme_void() } visualize_bigrams_individual &lt;- function(bigrams) { set.seed(2016) a &lt;- grid::arrow(type = &quot;closed&quot;, length = unit(.15, &quot;inches&quot;)) bigrams %&gt;% graph_from_data_frame() %&gt;% ggraph(layout = &quot;fr&quot;) + geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a,end_cap = circle(.07, &#39;inches&#39;)) + geom_node_point(color = &quot;lightblue&quot;, size = 5) + geom_node_text(aes(label = name), vjust = 1, hjust = 1) + theme_void() } FoodInspectionsReduced_Bigrams &lt;- FoodInspectionsReduced %&gt;% count_bigrams() FoodInspectionsReduced_Bigrams %&gt;% filter(n &gt; 40000) %&gt;% visualize_bigrams() "],
["sentiment-analysis-for-results.html", "Chapter 79 Sentiment Analysis for Results", " Chapter 79 Sentiment Analysis for Results We investigate how often positive and negative words occurred in these Violation text. Which Results (e.g. Pass , Fail , Out of Business ) were the most positive or negative overall? We will use the AFINN sentiment lexicon, which provides numeric positivity scores for each word, and visualize it with a bar plot. FoodInspectionWords_sentiments &lt;- FoodInspectionWords %&gt;% inner_join(get_sentiments(&quot;afinn&quot;), by = &quot;word&quot;) %&gt;% group_by(Results) %&gt;% dplyr::summarize(score = sum(score * n) / sum(n)) %&gt;% arrange(desc(score)) FoodInspectionWords_sentiments %&gt;% mutate(Results = reorder(Results, score)) %&gt;% ggplot(aes(Results, score, fill = Results)) + geom_col(show.legend = TRUE) + coord_flip() + ylab(&quot;Average sentiment score&quot;) + theme_bw() As expected, the sentiments are ordered in the following order Pass has the MOST postive sentiment Out of Business , Pass w/ Conditions, No Entry follows Next Fail Not Ready "],
["sentiment-analysis-by-word.html", "Chapter 80 Sentiment analysis by word", " Chapter 80 Sentiment analysis by word We examine the total positive and negative contributions of each word.We display the words with the greatest contributions to positive/negative sentiment scores in the Violations text. contributions = FoodInspectionsReduced %&gt;% unnest_tokens(word, Violations) %&gt;% filter(!word %in% stop_words$word) %&gt;% dplyr::count(word, sort = TRUE) %&gt;% ungroup() %&gt;% inner_join(get_sentiments(&quot;afinn&quot;), by = &quot;word&quot;) %&gt;% group_by(word) %&gt;% dplyr::summarize(occurences = n(), contribution = sum(score)) contributions %&gt;% top_n(10, abs(contribution)) %&gt;% mutate(word = reorder(word, contribution)) %&gt;% ggplot(aes(word, contribution, fill = contribution &gt; 0)) + geom_col(show.legend = FALSE) + coord_flip() + theme_bw() "],
["sentiment-analysis-by-word-for-each-result-type.html", "Chapter 81 Sentiment analysis by word for Each Result Type", " Chapter 81 Sentiment analysis by word for Each Result Type We visualize the most important contributors within each Result Type here. FoodInspectionWords %&gt;% inner_join(get_sentiments(&quot;afinn&quot;), by = &quot;word&quot;) %&gt;% mutate(contribution = score * n / sum(n)) %&gt;% top_n(20, abs(contribution)) %&gt;% mutate(word = reorder(word, contribution)) %&gt;% ggplot(aes(word, contribution, fill = contribution &gt; 0)) + geom_col(show.legend = FALSE) + facet_wrap(~Results, ncol = 2, scales = &quot;free_y&quot;) + coord_flip() + theme_bw() "],
["sentiment-analysis-by-inspection-text.html", "Chapter 82 Sentiment analysis by Inspection Text", " Chapter 82 Sentiment analysis by Inspection Text We examine the positive and the negative Text.We filtered out messages that had fewer than five words that contributed to sentiment. sentiment_messages &lt;- FoodInspectionsReduced %&gt;% unnest_tokens(word, Violations) %&gt;% filter(!word %in% stop_words$word) %&gt;% inner_join(get_sentiments(&quot;afinn&quot;), by = &quot;word&quot;) %&gt;% group_by(Results, InspectionID) %&gt;% dplyr::summarize(sentiment = mean(score), words = n()) %&gt;% ungroup() %&gt;% filter(words &gt;= 5) FoodInspectionsReduced2 = FoodInspections %&gt;% mutate(InspectionID = `Inspection ID`) %&gt;% mutate(DBAName = `DBA Name`) %&gt;% select(InspectionID,Violations,Results,DBAName) The Messages having top Ten positive sentiments are DBAName Results.x Text LA CENTRAL BAKERY V Pass WALLS, CEILINGS, ATTACHED EQUIPMENT CONSTRUCTED PER CODE: GOOD REPAIR, SURFACES CLEAN AND DUST-LESS CLEANING METHODS - Comments: MUST CLEAN WALL AT PREP TABLE NEXT TO OVEN IN KITCHEN. REMOVE DUST FROM FAN COVERS IN WALK-IN COOLER. SEAL HOLES AROUND PIPES UNDERNEATH HANDSINKS IN WASHROOMS. | 36. LIGHTING: REQUIRED MINIMUM FOOT-CANDLES OF LIGHT PROVIDED, FIXTURES SHIELDED - Comments: ONE SET OF LIGHTS IN KITCHEN HAVE SHIELDS BUT NO END CAPS; PROVIDE END CAPS FOR LIGHTS. | 38. VENTILATION: ROOMS AND EQUIPMENT VENTED AS REQUIRED: PLUMBING: INSTALLED AND MAINTAINED - Comments: VENTILATION FAN IN WOMENS WASHROOM NOT WORKING; REPAIR FAN. WHITE CASTLE SYSTEM, INC Pass VENTILATION: ROOMS AND EQUIPMENT VENTED AS REQUIRED: PLUMBING: INSTALLED AND MAINTAINED - Comments: INSTRUCTED TO REPAIR THE VENTILATION INSIDE THE BATHROOMS. ALSO INSTRUCTED TO CLEAN FAN INSIDE THE WALK IN COOLER. | 43. FOOD (ICE) DISPENSING UTENSILS, WASH CLOTHS PROPERLY STORED - Comments: NOTED ICE SCOOP ON TOP OF ICE MACHINE NOT PROPERLY STORED. INSTRUCTED TO STORE ICE SCOOP IN A CLEAN SCOOP HOLDER OR CONTAINER. JAMBA JUICE Pass FOOD AND NON-FOOD CONTACT SURFACES PROPERLY DESIGNED, CONSTRUCTED AND MAINTAINED - Comments: CLEAN INTERIOR OF FRONT REFRIGERATOR. CLEAN FAN GUARD COVER IN REAR PREP REFRIGERATOR TO REMOVE DUST OBSERVED. | 34. FLOORS: CONSTRUCTED PER CODE, CLEANED, GOOD REPAIR, COVING INSTALLED, DUST-LESS CLEANING METHODS USED - Comments: SOME AREAS OF FLOOR STICKY; MOP AREAS. | 35. WALLS, CEILINGS, ATTACHED EQUIPMENT CONSTRUCTED PER CODE: GOOD REPAIR, SURFACES CLEAN AND DUST-LESS CLEANING METHODS - Comments: SOME DUST ON CEILING VENTILATION FAN IN WASHROOM; REMOVE. REMOVE DUST ON FAN COVERS IN WALK-IN COOLER. TUSCAN HEN FOODS Pass FOOD AND NON-FOOD CONTACT EQUIPMENT UTENSILS CLEAN, FREE OF ABRASIVE DETERGENTS - Comments: OBSERVED SLIGHT CALCIUM BUILD UP INSIDE ICE MACHINE. MUST CLEAN AND MAINTAIN INTERIOR OF ICE MACHINE. | 43. FOOD (ICE) DISPENSING UTENSILS, WASH CLOTHS PROPERLY STORED - Comments: OBSERVED ICE SCOOP IMPROPERLY STORED INSIDE ICE MACHINE WITH ICE SCOOP HANDLE NOT UPWARD. INSTRUCTED MANAGER TO PROVIDE CLEAN CONTAINER FOR ICE SCOOP. | 40. REFRIGERATION AND METAL STEM THERMOMETERS PROVIDED AND CONSPICUOUS - Comments: NO THERMOMETER INSIDE BASEMENT REFRIGERATOR. INSTRUCTED MANAGER TO PROVIDE WORKING THERMOMETER FOR SAID COOLER. | 38. VENTILATION: ROOMS AND EQUIPMENT VENTED AS REQUIRED: PLUMBING: INSTALLED AND MAINTAINED - Comments: BASEMENT EMPLOYEES WASHROOM CEILING VENTILATION FAN NOT WORKING. INSTRUCTED MANAGER TO PROVIDE WORKING WASHROOM CEILING VENTILATION FAN. TAQUERIA LA OAXAQUENA Pass FOOD AND NON-FOOD CONTACT SURFACES PROPERLY DESIGNED, CONSTRUCTED AND MAINTAINED - Comments: DRAIN RACK OVER THREE COMPARTMENT SINK HAVE BUILD UP OF GRIME. MUST CLEAN. | 43. FOOD (ICE) DISPENSING UTENSILS, WASH CLOTHS PROPERLY STORED - Comments: ICE DISPENSING SCOOP INSIDE ICE MACHINE. MUST PROVE CONTAINER FOOD ICE SCOOP. | 34. FLOORS: CONSTRUCTED PER CODE, CLEANED, GOOD REPAIR, COVING INSTALLED, DUST-LESS CLEANING METHODS USED - Comments: FLOORS AROUND BASEBOARDS/COVE HAVE BUILD OF GRIME/GREASE MUST DETAIL CLEAN. | 37. TOILET ROOM DOORS SELF CLOSING: DRESSING ROOMS WITH LOCKERS PROVIDED: COMPLETE SEPARATION FROM LIVING/SLEEPING QUARTERS - Comments: WOMEN TOILET DOOR NEED SELF CLOSING DEVICE . | 38. VENTILATION: ROOMS AND EQUIPMENT VENTED AS REQUIRED: PLUMBING: INSTALLED AND MAINTAINED - Comments: VENTILATION COVERS IN DING AREA HAVE BUILD UP OF DUST. MUST CLEAN. BUZZ KILLER ESPRESSO Pass FOOD AND NON-FOOD CONTACT SURFACES PROPERLY DESIGNED, CONSTRUCTED AND MAINTAINED - Comments: CLEAN INTERIOR OF REFRIGERATION UNIT BY THE THREE COMP SINK. CLEAN INTERIOR OF ICE MACHINE AND MAINTAIN. | 35. WALLS, CEILINGS, ATTACHED EQUIPMENT CONSTRUCTED PER CODE: GOOD REPAIR, SURFACES CLEAN AND DUST-LESS CLEANING METHODS - Comments: A COVER IS NEEDED FOR THE VENTILATION FAN IN THE FIRST FLOOR WASHROOM. REMOVE DUST FROM VENTILATION FAN COVER IN THE SECOND FLOOR WASHROOM. BUDACKIS DRIVE-IN HOT DOGS Pass LIGHTING: REQUIRED MINIMUM FOOT-CANDLES OF LIGHT PROVIDED, FIXTURES SHIELDED - Comments: Light not provided in dining room front storage room. Instructed to install bulb and/or light fixture. | 38. VENTILATION: ROOMS AND EQUIPMENT VENTED AS REQUIRED: PLUMBING: INSTALLED AND MAINTAINED - Comments: Exhaust fan in toilet room not working. Instructed to repair exhaust fan. | 34. FLOORS: CONSTRUCTED PER CODE, CLEANED, GOOD REPAIR, COVING INSTALLED, DUST-LESS CLEANING METHODS USED - Comments: Floor along wallbase and behind heavy equipment not cleaned. Instructed to detail clean daily. | 35. WALLS, CEILINGS, ATTACHED EQUIPMENT CONSTRUCTED PER CODE: GOOD REPAIR, SURFACES CLEAN AND DUST-LESS CLEANING METHODS - Comments: Light and ventilation covers not cleaned. Instructed to detail clean. | 37. TOILET ROOM DOORS SELF CLOSING: DRESSING ROOMS WITH LOCKERS PROVIDED: COMPLETE SEPARATION FROM LIVING/SLEEPING QUARTERS - Comments: Toilet room door not self closing. Instructed to replace self closing device or keep door closed. O’DONNELLS Pass FLOORS: CONSTRUCTED PER CODE, CLEANED, GOOD REPAIR, COVING INSTALLED, DUST-LESS CLEANING METHODS USED - Comments: OBSERVED LITTER/BOTTLE CAPS ON FLOOR AT BAR AND STANDING WATER ON BASEMENT FLOOR. INSTRUCTED MANAGER TO CLEAN/KEEP DRY FLOOR. | 32. FOOD AND NON-FOOD CONTACT SURFACES PROPERLY DESIGNED, CONSTRUCTED AND MAINTAINED - Comments: BAR 2-DOOR COOLER RUBBER GASKET RIPPED. INSTRUCTED MANAGER TO REPAIR/REPLACE DOOR GASKET. | 38. VENTILATION: ROOMS AND EQUIPMENT VENTED AS REQUIRED: PLUMBING: INSTALLED AND MAINTAINED - Comments: BAR 3-COMPARTMENT SINK HOT/COLD WATER LEAKING AT BASE OF FAUCET ALSO NO BACKFLOW PREVENTION DEVICE AT ICE MACHINE. INSTRUCTED MANAGER TO REPAIR SINK ALSO MUST PROVIDE/MAKE VISIBLE BACK FLOW PREVENTION DEVICE FOR ICE MACHINE. | 43. FOOD (ICE) DISPENSING UTENSILS, WASH CLOTHS PROPERLY STORED - Comments: OBSERVED ICE SCOOP IMPROPERLY STORED ON TOP OF ICE MACHINE. INSTRUCTED MANAGER TO PROVIDE CLEAN CONTAINER FOR ICE SCOOP. ASIAN OUTPOST Pass WALLS, CEILINGS, ATTACHED EQUIPMENT CONSTRUCTED PER CODE: GOOD REPAIR, SURFACES CLEAN AND DUST-LESS CLEANING METHODS - Comments: OBSERVED HOOD VENTS WITH ACCUMULATED GREASE. INSTRUCTED TO CLEAN AND MAINTAIN THE HOOD VENTS. | 31. CLEAN MULTI-USE UTENSILS AND SINGLE SERVICE ARTICLES PROPERLY STORED: NO REUSE OF SINGLE SERVICE ARTICLES - Comments: OBSERVED SINGLE SERVICE FOOD TRAYS STORED IMPROPERLY. INSTRUCTED TO STORE SINGLE SERVICE ITEMS COVERED OR INVERTED. | 43. FOOD (ICE) DISPENSING UTENSILS, WASH CLOTHS PROPERLY STORED - Comments: OBSERVED BOWLS USED TO SCOOP FOOD PRODUCTS. INSTRUCTED TO USE A SCOOP WITH A HANDLE AND HAVE THE HANDLE UP. STARS FOOD MARKET INC Pass FLOORS: CONSTRUCTED PER CODE, CLEANED, GOOD REPAIR, COVING INSTALLED, DUST-LESS CLEANING METHODS USED - Comments: MUST ELEVATE ALL FOOD ITEMS FROM THE FLOOR IN THE WALK-IN COOLER. CLEAN FLOORS IN CORNER AREAS OF PREP AREA, STORAGE ROOM, WASHROOM AND WALK-IN COOLER. | 35. WALLS, CEILINGS, ATTACHED EQUIPMENT CONSTRUCTED PER CODE: GOOD REPAIR, SURFACES CLEAN AND DUST-LESS CLEANING METHODS - Comments: OBSERVED HEAVY DUST ON THE VENTILATION FAN ON THE CEILING OF THE WASHROOM; CLEAN FAN TO REMOVE DUST OBSERVED. OBSERVED WATER STAINED AND DAMAGED CEILING TILES IN THE WASHROOM AND THE SELLING FLOOR OF THE STORE; MUST REPLACE ALL CEILING TILES WHERE NEEDED. | 38. VENTILATION: ROOMS AND EQUIPMENT VENTED AS REQUIRED: PLUMBING: INSTALLED AND MAINTAINED - Comments: OBSERVED LEAKING FROM THE FIRST AND SECOND COMPARTMENTS (UNDERNEATH) OF THE THREE COMP SINK; MUST REPAIR LEAKS FROM THE SINK. MUST PROVIDE THREE METAL DRAIN STOPPERS FOR THE THREE COMP SINK TO PROPERLY SET UP SINK FOR WASH RINSE AND SANITIZING. DAILY FOOD &amp; LIQUOR Pass WALLS, CEILINGS, ATTACHED EQUIPMENT CONSTRUCTED PER CODE: GOOD REPAIR, SURFACES CLEAN AND DUST-LESS CLEANING METHODS - Comments: MUST CLEAN THE FAN COVERS IN THE WALK-IN COOLER WHERE THE BEER IS STORED TO REMOVE DUST OBSERVED. MUST CLEAN THE INTERIOR OF THE VENTILATION FAN IN THE WASHROOM ON THE CEILING TO REMOVE DUST OBSERVED. | 38. VENTILATION: ROOMS AND EQUIPMENT VENTED AS REQUIRED: PLUMBING: INSTALLED AND MAINTAINED - Comments: NO BACKFLOW PREVENTOR DEVICE FOUND AT THE UTILITY SINK; MUST INSTALL A BACKFLOW PREVENTOR DEVICE. | 37. TOILET ROOM DOORS SELF CLOSING: DRESSING ROOMS WITH LOCKERS PROVIDED: COMPLETE SEPARATION FROM LIVING/SLEEPING QUARTERS - Comments: NO SELF CLOSING DEVICE FOUND ON THE WASHROOM DOOR; MUST INSTALL A SELF CLOSING DEVICE. Benjyehuda Pass WALLS, CEILINGS, ATTACHED EQUIPMENT CONSTRUCTED PER CODE: GOOD REPAIR, SURFACES CLEAN AND DUST-LESS CLEANING METHODS - Comments: VENTILATION FAN NOT CLEAN,(DUSTY) ABOVE MOP SINK AND IN WASHROOM.MUST CLEAN/MAINTAIN. | 38. VENTILATION: ROOMS AND EQUIPMENT VENTED AS REQUIRED: PLUMBING: INSTALLED AND MAINTAINED - Comments: VENTILATION FAN IN WASHROOM NOT WORKING.MUST REPAIR. HOTEL CHICAGO Pass FLOORS: CONSTRUCTED PER CODE, CLEANED, GOOD REPAIR, COVING INSTALLED, DUST-LESS CLEANING METHODS USED - Comments: OBSERVED GREASE/FOOD BUILD UP ON FLOOR UNDER COOKING EQUIPMENT. MUST CLEAN FLOOR UNDER AND AROUND COOKING EQUIPMENT AND MAINTAIN. | 35. WALLS, CEILINGS, ATTACHED EQUIPMENT CONSTRUCTED PER CODE: GOOD REPAIR, SURFACES CLEAN AND DUST-LESS CLEANING METHODS - Comments: OBSERVED DUST/FOOD DEBRIS ON WALLS CEILING TILES AT DISHWASH AND PREP AREA. MUST CLEAN WALLS AND CEILING TILES. | 43. FOOD (ICE) DISPENSING UTENSILS, WASH CLOTHS PROPERLY STORED - Comments: METAL ICE SCOOP CRACKED BY HANDLE. MUST REPLACE DAMAGED ICE SCOOP. The Messages having top Ten NOT so positive sentiments are DBAName Results.x Text DePriest Elementary Pass VENTILATION: ROOMS AND EQUIPMENT VENTED AS REQUIRED: PLUMBING: INSTALLED AND MAINTAINED - Comments: CORRECTED. | 32. FOOD AND NON-FOOD CONTACT SURFACES PROPERLY DESIGNED, CONSTRUCTED AND MAINTAINED - Comments: OBSERVED THE GASKET AROUND THE INTERIOR OF THE ICE MACHINE DOOR IS IN POOR REPAIR; REPLACE GASKET. OBSERVED TWO OF THE THREE GASKETS AT THE THREE DOOR COOLER NEXT TO THE ICE MACHINE LOOSE; REATTACH LOOSE GASKETS OR REPLACE TO AVOID ESCAPING AIR. TORTOISE CLUB Fail FOOD AND NON-FOOD CONTACT EQUIPMENT UTENSILS CLEAN, FREE OF ABRASIVE DETERGENTS - Comments: OBSERVED SMALL LOW TEMP DISH MAHCINE AT BAR WITH HEAVY LIME BUILD UP UNDERNEATH. DELIME AND MAINTAIN. | 22. DISH MACHINES: PROVIDED WITH ACCURATE THERMOMETERS, CHEMICAL TEST KITS AND SUITABLE GAUGE COCK - Comments: OBSERVED GAUGE COCK ON HOT WATER DISH MACHINE READING 160F. HIGH TEMP TEST STRIPS WERE USED TO INDICATE HOT WATER DISH MACHINE IS SANITIZING AT 180F. NEW GUAGE COCK MUST BE INSTALLED AND MAINTAINED TO MONITOR DISH MACHINE. SERIOUS VIOALTION 7-38-030 | 41. PREMISES MAINTAINED FREE OF LITTER, UNNECESSARY ARTICLES, CLEANING EQUIPMENT PROPERLY STORED - Comments: OBSERVED 6 PACK OF SODA ON FLOOR OF BAR AND BUCKET OF LIMES AND LEMONS ON FLOOR OF BAR. ALL FOOD ITEMS NEED TO BE STORED AT LEAST 6 INCHES OFF FLOOR AT ALL TIMES. NORTH LAWNDALE SCHOOL COLLEGE PREP Fail FACILITIES TO MAINTAIN PROPER TEMPERATURE - Comments: OBSERVED WALK-IN COOLER IN POOR REPAIR, NOT HOLD AT 40’F AND BELOW, COOLER READING 60’F, INSTRUCTED TO RESTRICT USE, COOLER TAGGED, MUST FAX LETTER FOR RE-INSPECTION, ONCE REPAIR, CRITICAL VIOLATION: 7-38-005A | 3. POTENTIALLY HAZARDOUS FOOD MEETS TEMPERATURE REQUIREMENT DURING STORAGE, PREPARATION DISPLAY AND SERVICE - Comments: OBSERVED POTENTIALLY HAZARDOUS FOODS STORED IN WALK-IN COOLER AT IMPROPER TEMPS, (2) ROLLS OF GROUND BEEF, COOKED CHICKEN, (4) BAGS OF ROAST BEEF, AND (1) BAG OF YOGURT, ALL READING BETWEEN 50’F-58’F, ALL PRODUCT DUMPED AND DENATURED AT A COST OF ABOUT $1,000.00 AND ABOUT 30 POUNDS, CRITICAL VIOLATION: 7-38-005A | 29. PREVIOUS MINOR VIOLATION(S) CORRECTED 7-42-090 - Comments: CONTINUED NON COMPLIANCE ON PREVIOUS ORDER ISSUED, DATED 05/12/2016, REPORT #1770612, VIOLATION LISTED BELOW: 34 FLOORS IN POOR REPAIR IN KITCHEN AREA, CRACKED FLOOR TILE NEEDS REPLACED, 35 WALLS IN POOR REPAIR IN DINING AREA, HOLES AND BROKEN PLASTER, INSTRUCTED TO REPAIR, AND BETTER MAINTAIN, SERIOUS VIOLATION: 7-42-090 COOPER ELEMENTARY Pass FOOD AND NON-FOOD CONTACT SURFACES PROPERLY DESIGNED, CONSTRUCTED AND MAINTAINED - Comments: REPAIR LOOSE RUBBER GASKET AT THREE DOOR COOLER IN KITCHEN WHERE LOOSE (UNIT HOLDING AT PROPER TEMPERATURE AT THIS TIME). | 34. FLOORS: CONSTRUCTED PER CODE, CLEANED, GOOD REPAIR, COVING INSTALLED, DUST-LESS CLEANING METHODS USED - Comments: REPLACE CRACKED FLOOR TILE IN BATHROOM OF ROOM 106. REPLACE CRACKED FLOOR TILE IN BASEMENT OUTSIDE OF ROOM LEADING TO BOYS BATHROOM. | 35. WALLS, CEILINGS, ATTACHED EQUIPMENT CONSTRUCTED PER CODE: GOOD REPAIR, SURFACES CLEAN AND DUST-LESS CLEANING METHODS - Comments: PEELING PAINT OBSERVED ON WALLS OF LUNCHROOM STAFF WASHROOM, IN HALLWAYS OF BUILDING ON WALLS AND CEILINGS; REMOVE PEELING PAINT AND PAINT AREAS WHERE NEEDED. WALLBASES IN BASEMENT HALLWAY IN POOR REPAIR; REPAIR WHERE NEEDED. REPLACE COVER ON WALL OF GIRLS WASHROOM IN STALL ONE OVER FLUSH BUTTON FOR TOILET; PANEL LOOSE AND HANGING FROM WALL. SEAL AROUND TOILETS AND URINALS TO SEAL HOLES OBSERVED IN BOYS AND GIRLS WASHROOM IN BASEMENT AND THIRD FLOOR WHERE NEEDED. MARQUETTE Pass FOOD AND NON-FOOD CONTACT SURFACES PROPERLY DESIGNED, CONSTRUCTED AND MAINTAINED - Comments: INTERIOR DOORS OF ALL MILK COOLERS AT SERVING LINES IN POOR REPAIR, MISSING AND LOOSE GASKETS. REPLACE SAME. UNUSED, NON-WORKING HOT HOLDING UNITS ON SITE WITH MISSING GAUGES. INSTD TO REPAIR SAME OR REMOVE. | 35. WALLS, CEILINGS, ATTACHED EQUIPMENT CONSTRUCTED PER CODE: GOOD REPAIR, SURFACES CLEAN AND DUST-LESS CLEANING METHODS - Comments: LOOSE THRESHOLD AT CAFETERIA EXIT DOOR A. REPAIR SAME AND HAVE TIGHT FITTING. SUPERMERCADO LA FIESTA #2, INC. Pass FOOD AND NON-FOOD CONTACT SURFACES PROPERLY DESIGNED, CONSTRUCTED AND MAINTAINED - Comments: INTERIOR PANEL OF ICE MACHINE WITH CALCIUM BUILD-UP. INSTD TO CLEAN SAME. WALK-IN COOLER DOOR GASKETS IN POOR REPAIR, MISSING IN SECTIONS, LOOSE. REPAIR SAME AND HAVE TIGHT FITTING. IMPROPER LARGE PLASTIC CONTAINERS BEING USED TO STORE CHICKEN. INSTD TO USE PROPER FOOD GRADE CONTAINERS. | 38. VENTILATION: ROOMS AND EQUIPMENT VENTED AS REQUIRED: PLUMBING: INSTALLED AND MAINTAINED - Comments: URINAL IN TOILET ROOM IN POOR REPAIR. REPAIR SAME. REPAIR LOOSE PIPE UNDER UTILITY SINK. CERMAK PRODUCE #5 LTD Pass FLOORS: CONSTRUCTED PER CODE, CLEANED, GOOD REPAIR, COVING INSTALLED, DUST-LESS CLEANING METHODS USED - Comments: BASEMENT WALK IN COOLER FLOOR HAS BUILD UP OF DEBRIS. MUST DETAIL CLEAN BASEMENT WALK-IN COOLER FLOOR. TILES IN FRON OF PRODUCE DISPLAY COUNTER ARE MISSING OR DAMAGE . MUST REPLACE MISSING AND DAMAGE TILES. | 36. LIGHTING: REQUIRED MINIMUM FOOT-CANDLES OF LIGHT PROVIDED, FIXTURES SHIELDED - Comments: LIGHT SHIELDS ARE MISSING ON DISPLAY COLD HOLD UNITS THUR OUT THE STORE. MUST INSTALL LIGHT SHIELDS ON COLD HOLD DISPLAY UNITS. "],
["plot-of-results-and-risks.html", "Chapter 83 Plot of Results and Risks", " Chapter 83 Plot of Results and Risks ggplot(FoodInspections, aes(Results, ..count..)) + geom_bar(aes(fill = Risk), position = &quot;dodge&quot;) +theme_bw() + theme(legend.position=&quot;top&quot;) "],
["modelling-with-xgboost-2.html", "Chapter 84 Modelling with XGBoost", " Chapter 84 Modelling with XGBoost We try to predict whether the Inspection Result would result in Out Of Business or not. For this we use the following features * Curated list of words in the Violations Text. Each word is a feature for the Model. * Latitude * Longitude * Year of Inspection * Month of Inspection For ease of execution, we have taken only 5000 samples for the modelling Exercise. We do Cross Validation using Caret package. You can tune the parameters in your own machine for better results. The accuracy obtained through these parameters is quite good 0.9166024. Lastly we wish to examine the feature importance of the variables. This is shown in the flipped bar chart. FoodInspectionsSample = FoodInspections %&gt;% sample_n(5e3) corpus = Corpus(VectorSource(FoodInspectionsSample$Violations)) # Pre-process data corpus &lt;- tm_map(corpus, tolower) corpus &lt;- tm_map(corpus, removePunctuation) corpus &lt;- tm_map(corpus, removeWords, c(stopwords(&quot;english&quot;),&quot;comments&quot;)) corpus &lt;- tm_map(corpus, removeWords, UniqueLowIDF[1:50]) corpus &lt;- tm_map(corpus, stemDocument) dtm = DocumentTermMatrix(corpus) # Remove sparse terms dtm = removeSparseTerms(dtm, 0.997) # Create data frame labeledTerms = as.data.frame(as.matrix(dtm)) FoodInspectionsSample = FoodInspectionsSample %&gt;% mutate(isOOB = 0) FoodInspectionsSample = FoodInspectionsSample %&gt;% dplyr::rename(InspectionDate = `Inspection Date`) %&gt;% mutate(yr = year(mdy(InspectionDate))) %&gt;% mutate(month_of_year = month(mdy(InspectionDate))) FoodInspectionsSample = FoodInspectionsSample %&gt;% mutate(isOOB=replace(isOOB, Results == &quot;Out of Business&quot;, 1)) %&gt;% as.data.frame() labeledTerms$isOOB = as.factor(FoodInspectionsSample$isOOB) labeledTerms$Latitude = as.numeric(FoodInspectionsSample$Latitude) labeledTerms$Longitude = as.numeric(FoodInspectionsSample$Longitude) labeledTerms$Year = as.numeric(FoodInspectionsSample$yr) labeledTerms$Month = as.numeric(FoodInspectionsSample$month_of_year) ## Preparing the features for the XGBoost Model features &lt;- colnames(labeledTerms) for (f in features) { if ((class(labeledTerms[[f]])==&quot;factor&quot;) || (class(labeledTerms[[f]])==&quot;character&quot;)) { levels &lt;- unique(labeledTerms[[f]]) labeledTerms[[f]] &lt;- as.numeric(factor(labeledTerms[[f]], levels=levels)) } } ## Creating the XGBoost Model labeledTerms$isOOB = as.factor(labeledTerms$isOOB) formula = isOOB ~ . fitControl &lt;- trainControl(method=&quot;cv&quot;,number = 3) xgbGrid &lt;- expand.grid(nrounds = 10, max_depth = 3, eta = .05, gamma = 0, colsample_bytree = .8, min_child_weight = 1, subsample = 1) set.seed(13) OOBXGB = train(formula, data = labeledTerms, method = &quot;xgbTree&quot;,trControl = fitControl, tuneGrid = xgbGrid,na.action = na.pass) importance = varImp(OOBXGB) varImportance &lt;- data.frame(Variables = row.names(importance[[1]]), Importance = round(importance[[1]]$Overall,2)) # Create a rank variable based on importance rankImportance &lt;- varImportance %&gt;% mutate(Rank = paste0(&#39;#&#39;,dense_rank(desc(Importance)))) %&gt;% head(20) rankImportancefull = rankImportance ggplot(rankImportance, aes(x = reorder(Variables, Importance), y = Importance)) + geom_bar(stat=&#39;identity&#39;,colour=&quot;white&quot;, fill = fillColor) + geom_text(aes(x = Variables, y = 1, label = Rank), hjust=0, vjust=.5, size = 4, colour = &#39;black&#39;, fontface = &#39;bold&#39;) + labs(x = &#39;Variables&#39;, title = &#39;Relative Variable Importance&#39;) + coord_flip() + theme_bw() All the factors affecting the decision of Inspection Result along with their ranks is provided below "],
["references.html", "Chapter 85 References", " Chapter 85 References An awesome book Text Mining with R - A Tidy Approach - Julia Silge and David Robinson - 2017-05-07 5 Algorithms Every Web Developer Can Use and Understand . This book is used for the TF-IDF explanation. "]
]

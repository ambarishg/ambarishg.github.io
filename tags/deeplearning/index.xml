<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>deeplearning on Thoughts - Ambarish</title>
    <link>/tags/deeplearning/</link>
    <description>Recent content in deeplearning on Thoughts - Ambarish</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Â© Copyright notice</copyright>
    <lastBuildDate>Mon, 26 Apr 2021 00:00:00 +0030</lastBuildDate><atom:link href="/tags/deeplearning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Convolutions in 1 dimension</title>
      <link>/posts/conv1d-pytorch/</link>
      <pubDate>Mon, 26 Apr 2021 00:00:00 +0030</pubDate>
      
      <guid>/posts/conv1d-pytorch/</guid>
      <description>Basics The Convolutional block is one of the basic building blocks used in deep learning. We go in-depth with Convolution in 1 dimension and understand the basics of convolution, strides, and padding. We explain visually and also through PyTorch code to verify our concepts.
The Kernel takes an Input and provides an output which is sometimes referred to as a feature map
The Kernel is made up of many things .</description>
    </item>
    
    <item>
      <title>Red Wine Quality prediction using AzureML, AKS with TensorFlow Keras</title>
      <link>/posts/red-wine-quality-azureml-keras/</link>
      <pubDate>Sat, 24 Apr 2021 00:00:00 +0030</pubDate>
      
      <guid>/posts/red-wine-quality-azureml-keras/</guid>
      <description>What are we trying to do Predict the Quality of Red Wine using Tensorflow Keras deep learning framework given certain attributes such as fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates, and alcohol
We divide our approach into 2 major blocks:
 Building the Model in Azure ML Inference from the Model in Azure ML  Building the model in Azure ML has the following steps:</description>
    </item>
    
    <item>
      <title>AdaptiveAvgPool2d in PyTorch</title>
      <link>/posts/adaptiveavgpool2d-pytorch/</link>
      <pubDate>Sun, 18 Apr 2021 00:00:00 +0030</pubDate>
      
      <guid>/posts/adaptiveavgpool2d-pytorch/</guid>
      <description>I had trouble understanding the AdaptiveAvgPool2d function in PyTorch. The following examples helped me to teach myself better. Hopefully, somebody may benefit from this.
Example 1 import torch import torch.nn as nn import numpy as np m = nn.AdaptiveAvgPool2d((1)) x = np.array( [ [ 2. , 3.], [ 4. , 1.], ]) input = torch.tensor(x) print(input) output = m(input) print(output) print(torch.mean(input)) The output will be equal to torch.mean(input)
Example 2 with a 3 x 3 x 3 tensor x = np.</description>
    </item>
    
    <item>
      <title>Bees Health detection using Azure Custom Vision Service</title>
      <link>/posts/bees-health-detection-azure-cognitive-services/</link>
      <pubDate>Wed, 31 Mar 2021 00:00:00 +0030</pubDate>
      
      <guid>/posts/bees-health-detection-azure-cognitive-services/</guid>
      <description>Every third bite of food relies on pollination by bees. Honey beehive losses are quite prevalent due to the diseased bees.
While many indications of hive strength and health are visible on the inside of the hive, frequent check-ups on the hive are time-consuming and disruptive to the bees&#39; workflow and hive in general. By investigating the bees that leave the hive, we can gain a more complete understanding of the hive itself.</description>
    </item>
    
    <item>
      <title>Cross Entropy Loss</title>
      <link>/posts/2021-03-27-cross-entropy-loss/</link>
      <pubDate>Sat, 27 Mar 2021 00:00:00 +0030</pubDate>
      
      <guid>/posts/2021-03-27-cross-entropy-loss/</guid>
      <description>Cross Entropy Loss In a supervised learning problem for predicting classes, we predict probabilities for the classes. To determine how successful we are predicting the classes, we require a loss function.
Cross-Entropy loss function provides a loss function to calculate the loss between the actual classes and the predicted probabilities
This can be represented as
 Get the predicted probability of the class Get the actual label of the class ( 1 or 0 ) Multiply the actual label with the log of the predicted probability of the class Sum the values obtained from each of the classes Multiply by -1 the value obtained in Step 4  This is the loss for a single observation.</description>
    </item>
    
    <item>
      <title>activation functions</title>
      <link>/posts/2017-07-06-activation-functions/</link>
      <pubDate>Thu, 06 Jul 2017 00:00:00 +0030</pubDate>
      
      <guid>/posts/2017-07-06-activation-functions/</guid>
      <description>Sigmoid function takes values between 0 and 1
$ {sigmoid} = \frac {e^x} {( 1+ e^x )} $
This is used in logistic regression.
The sigmoid graph is generated by the R code provided below.
Softmax function is used to calculate the probablities in multiple classes. Suppose we have a image of a digit. We want to calculate whether it is a 1, 2, 3, and so on. Softmax would provide the probablity for each of the classes.</description>
    </item>
    
  </channel>
</rss>

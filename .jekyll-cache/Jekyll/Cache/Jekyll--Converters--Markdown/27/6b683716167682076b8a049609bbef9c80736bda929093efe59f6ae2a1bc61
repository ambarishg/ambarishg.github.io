I"öT<p>I will tell you about 2 of my experiences with machine learning in data science competitions and writing a research paper. This will give us enough motivation to understand why platforms such as AzureML are required.</p>

<h1 id="story-1----a-data-science-competition">Story 1 -  A data science competition</h1>

<blockquote>
  <p>I participated in a data science competition that required predicting time series data. For this, I tried 5 different algorithms <code class="language-plaintext highlighter-rouge">Arima, ETS, Facebook Prophet, XGBoost, and Random Forest</code>. I also used several imputation techniques which also involved a number of hyperparameters. The competition ran for around 2 months and I worked on it continuously. It was very difficult and cumbersome to maintain the various metrics of the model runs and their hyperparameters.It would have been easier if I could log the metrics and the hyperparameters in a more systematic manner. This would have helped me ease the model management to a great extent.</p>
</blockquote>

<h1 id="story-2----writing-a-research-paper">Story 2 -  Writing a research paper</h1>

<blockquote>
  <p>This involved classification of <strong>breast cancer</strong> images. I used 13 different models, which included deep learning architectures and classical computer vision models. The complexity here is more since the <code class="language-plaintext highlighter-rouge">experimentation</code> is with different architectures which also have different hyperparameters. This was an unbalanced dataset, therefore metrics such as <code class="language-plaintext highlighter-rouge">precision</code>, <code class="language-plaintext highlighter-rouge">recall</code>, <code class="language-plaintext highlighter-rouge">fscore</code> , <code class="language-plaintext highlighter-rouge">balanced accuracy</code> were used. The model management and hyperparameter management becomes very difficult if we do not have a very good platform for such quick experimentation and also for the analysis of the results.</p>
</blockquote>

<p>These 2 stories illustrate that in Machine Learning, model development is important but there are many other factors such as model management, hyperparameter management, reproducibility are very important factors for the success of a machine learning project.</p>

<p>Azure ML provides a robust framework for deploying, maintaining models.  We are going to build a Machine Learning Model and deploy it to Azure ML. In the process, we not only make use of AzureML but also use various other services such as <code class="language-plaintext highlighter-rouge">AKS(Azure Kubernetes Service) and Azure Storage</code> to name a few. Through this blog, we are going to learn various Machine Learning concepts and how they are implemented in Azure ML.</p>

<p>We divide into <strong>2 major blocks</strong>:</p>
<ol>
  <li>Building the Model in Azure ML</li>
  <li>Inference from the Model in Azure ML</li>
</ol>

<p><strong>Building the model in Azure ML</strong> has the following steps:</p>
<ol>
  <li>Create the Azure ML workspace</li>
  <li>Upload data into the Azure ML Workspace</li>
  <li>Create the code folder</li>
  <li>Create the Compute Cluster</li>
  <li>Create the Model</li>
  <li>Create the Compute Environment</li>
  <li>Create the Estimator</li>
  <li>Create the Experiment and Run</li>
  <li>Register the Model</li>
</ol>

<p><strong>Inferencing from the model</strong> in Azure ML has the following steps:</p>
<ol>
  <li>Create the Inference Script</li>
  <li>Create the Inference Dependencies</li>
  <li>Create the Inference Config</li>
  <li>Create the Inference Clusters</li>
  <li>Deploy the Model in the Inference Cluster</li>
  <li>Get the predictions</li>
</ol>

<blockquote>
  <p>The post generously uses code from the MS Learn Data Scientist Associate Learning path. The post will help you in understanding the different Azure ML concepts from conceptualizing a problem to deployment in a production environment. You may have to change certain settings though. Moreover, if you are interested in the DP-100 Microsoft Data Scientist Associate Exam, you will find this post helpful.</p>
</blockquote>

<p><strong>Letâ€™s begin our journey:</strong></p>

<h1 id="create-the-azure-ml-workspace">Create the Azure ML workspace</h1>

<p>We need to create an Azure ML workspace that contains the experiments, runs, models, and everything. It is a house for everything. Letâ€™s create one</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import azureml.core
print(azureml.core.VERSION)
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from azureml.core import Workspace
from azureml.core.authentication import InteractiveLoginAuthentication

sid = &lt;your-subscription-id&gt;
forced_interactive_auth = InteractiveLoginAuthentication(tenant_id=&lt;your-tenant-id&gt;)
ws = Workspace.create(name='azureml_workspace',
            subscription_id= sid, 
            resource_group='rgazureml',
            create_resource_group = True,
            location='centralus'
            )
</code></pre></div></div>

<p>What does this code segment actually do:</p>
<ul>
  <li>Deployed an Azure ML workspace <code class="language-plaintext highlighter-rouge">azureml_workspace</code></li>
  <li>Deployed an AppInsights</li>
  <li>Deployed KeyVault</li>
  <li>Deployed StorageAccount</li>
</ul>

<p>in the resource group <strong>rgazureml</strong>. You can navigate to the resource group to view these features.</p>

<p>I have attached my screenshot. Please navigate to your workspace to see the results.</p>

<p><img src="https://dev-to-uploads.s3.amazonaws.com/uploads/articles/0ml6s7upgzwm1gvb469v.png" alt="image" /></p>

<h1 id="upload-data-into-the-azure-ml-workspace">Upload data into the Azure ML Workspace</h1>

<p>Machine Learning is about data. We need data to build our models. The data which we are going to use is the Red Wine Quality dataset. Let us store the data in the Azure ML workspace.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Upload data into the Azure ML Workspace

#upload data by using get_default_datastore()
ds = ws.get_default_datastore()
ds.upload(src_dir='./winedata', target_path='winedata', overwrite=True, show_progress=True)

print('Done')
</code></pre></div></div>

<p>We upload the data to the workspace default store. The default store is associated with the <strong>storage account</strong> we had created earlier.</p>

<p>The screenshot of my files in the datastore is attached here
<img src="https://dev-to-uploads.s3.amazonaws.com/uploads/articles/hd7zcmzyy74bobz9b1xc.png" alt="image" /></p>

<h1 id="create-the-code-folder">Create the code folder</h1>

<p>We store the code files in a directory</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import os

# create the folder
folder_training_script = './winecode'
os.makedirs(folder_training_script, exist_ok=True)

print('Done')
</code></pre></div></div>
<p>Till this point, we have uploaded the data for machine learning. We need to identify the computing environment to process the machine learning models. Letâ€™s create a <strong>compute cluster</strong> using the Azure ML SDK.</p>

<h1 id="create-the-compute-cluster">Create the Compute Cluster</h1>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from azureml.core.compute import AmlCompute
from azureml.core.compute import ComputeTarget
import os

# Step 1: name the cluster and set the minimal and maximal number of nodes 
compute_name = os.environ.get("AML_COMPUTE_CLUSTER_NAME", "cpucluster")
min_nodes = os.environ.get("AML_COMPUTE_CLUSTER_MIN_NODES", 0)
max_nodes = os.environ.get("AML_COMPUTE_CLUSTER_MAX_NODES", 1)

# Step 2: choose environment variables 
vm_size = os.environ.get("AML_COMPUTE_CLUSTER_SKU", "STANDARD_D2_V2")

provisioning_config = AmlCompute.provisioning_configuration(
    vm_size = vm_size, min_nodes = min_nodes, max_nodes = max_nodes)

# create the cluster
compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)

print('Compute target created')
</code></pre></div></div>
<p><img src="https://dev-to-uploads.s3.amazonaws.com/uploads/articles/0bojaz128y0hq504zc3m.png" alt="image" /></p>

<h1 id="create-the-model">Create the Model</h1>

<p>We now create the model and store it in a script. The model uses the following</p>
<ul>
  <li>Decision Trees algorithm</li>
  <li>Cross-validation</li>
  <li>Logs the Decision Trees <code class="language-plaintext highlighter-rouge">max_depth</code> parameter and <code class="language-plaintext highlighter-rouge">rmse</code> parameter</li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>%%writefile $folder_training_script/train.py

import argparse
import os
import numpy as np
import pandas as pd
import glob

from azureml.core import Run
# from utils import load_data

import joblib

from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score

# let user feed in 2 parameters, the dataset to mount or download, and the regularization rate of the logistic regression model
parser = argparse.ArgumentParser()
parser.add_argument('--data-folder', type=str, dest='data_folder', help='data folder mounting point')
parser.add_argument('--max-depth', type=float, dest='max_depth', default=5, help='max depth')
args = parser.parse_args()

###
data_folder = os.path.join(args.data_folder, 'winedata')
print('Data folder:', data_folder)

wine_data = pd.read_csv(os.path.join(data_folder, 'winequality_red.csv'))

                        
X = wine_data.drop(columns =["quality"])
y = wine_data["quality"]

clf = DecisionTreeRegressor(random_state=0,max_depth = args.max_depth)
rmse= np.mean(np.sqrt(-cross_val_score(clf, X, y, scoring="neg_mean_squared_error", cv = 5)))
print('RMSE is', rmse)

# Get the experiment run context
run = Run.get_context()

run.log('max depth', np.float(args.max_depth))
run.log('rmse', np.float(rmse))

os.makedirs('outputs', exist_ok=True)

clf.fit(X,y)
# note file saved in the outputs folder is automatically uploaded into experiment record
joblib.dump(value=clf, filename='outputs/wine_model.pkl')

run.complete()
</code></pre></div></div>
<h1 id="create-the-compute-environment">Create the Compute Environment</h1>

<p>Till this point, we have done the following things</p>
<ul>
  <li>Uploaded the data into Azure</li>
  <li>Created the compute resources for the machine learning model</li>
  <li>Created the model in a file</li>
</ul>

<p>We need certain packages so that we can perform machine learning. Usually, we would require packages such as <code class="language-plaintext highlighter-rouge">sklearn</code>. This is a small machine learning model, but ideally, we would require more packages. In the next step, we create an <strong>Environment</strong> which houses the packages for the machine learning model</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from azureml.core import Environment
from azureml.core.conda_dependencies import CondaDependencies

# Create a Python environment for the experiment
wine_env = Environment("wine-experiment-env")
wine_env.python.user_managed_dependencies = False # Let Azure ML manage dependencies
wine_env.docker.enabled = False # Use a docker container

# Create a set of package dependencies (conda or pip as required)
wine_packages = CondaDependencies.create(conda_packages=['scikit-learn'])

# Add the dependencies to the environment
wine_env.python.conda_dependencies = wine_packages

print(wine_env.name, 'defined.')

# Register the environment
wine_env.register(workspace=ws)
</code></pre></div></div>

<h1 id="create-the-estimator">Create the Estimator</h1>

<p>Till this point, we have done the following things</p>
<ul>
  <li>Uploaded the data into Azure</li>
  <li>Created the compute resources for the machine learning model</li>
  <li>Created the model in a file</li>
  <li>Created the environment for the compute clusters</li>
</ul>

<p>We need to bind all this together and we would create an <strong>Estimator</strong> which binds the source code, hyperparameters required for the model, compute clusters, and the compute environment together. To run the model, we would also pass the hyperparameters required for the model.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from azureml.train.estimator import Estimator

script_params = {
    '--data-folder': ds.as_mount(),
    '--max-depth': 10
}

registered_env = Environment.get(ws, 'wine-experiment-env')

# Create an estimator
estimator = Estimator(source_directory=folder_training_script,
                      script_params=script_params,
                      compute_target = compute_target, # Run the experiment on the remote compute target
                      environment_definition = registered_env,
                      entry_script='train.py')
</code></pre></div></div>
<p>We are now ready for an AzureML <strong>Experiment</strong> which would house a number of <strong>Runs</strong>. The following code segment creates an experiment and also creates a <strong>Run</strong> with the created <strong>Estimator</strong></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Create the Experiment and Run               

from azureml.core import Experiment

#Create an experiment
experiment = Experiment(workspace = ws, name = "wine_expt")

print('Experiment created')
run = experiment.submit(config=estimator)
run
</code></pre></div></div>
<p>A snapshot of the experiment and its associated run is provided below. The experiment is <code class="language-plaintext highlighter-rouge">wine_expt</code> and the Runs associated with it.</p>

<p><img src="https://dev-to-uploads.s3.amazonaws.com/uploads/articles/crpzixx2hwutgu3lvlnu.png" alt="image" /></p>

<p>The latest completed Run snapshot is provided below
<img src="https://dev-to-uploads.s3.amazonaws.com/uploads/articles/2f4e1lswpsm9k7tefyhp.png" alt="image" /></p>

<p>In the latest run, we see the metrics <strong>rmse</strong> and the hyperparameters <strong>max_depth</strong> being shown.</p>

<p>We print the model metrics here using the following code segment</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#get the result
print(run.get_metrics())
</code></pre></div></div>
<h1 id="register-the-model">Register the Model</h1>

<p>We register the model in the workspace using the following code segment</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>model = run.register_model(model_name='wine_model',
                           model_path='outputs/wine_model.pkl',
                           tags = {'area': "wine", 'type': "sklearn"},
                           description = "quality wine")

print(model.name, model.id, model.version, sep='\t')
</code></pre></div></div>

<p>You can see this in your workspace. I have attached my screenshot
<img src="https://dev-to-uploads.s3.amazonaws.com/uploads/articles/5rli8x3uoq7bk270kj4m.png" alt="image" /></p>

<h1 id="create-the-inference-script">Create the Inference Script</h1>

<p>Till this point , we have built the model and registered in our workspace. We will now use this model to predict on new data. The first step is to create an <strong>inference script</strong>.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>%%writefile $folder_training_script/score.py
import json
import joblib
import numpy as np
from azureml.core.model import Model

# Called when the service is loaded
def init():
    global model
    # Get the path to the registered model file and load it
    model_path = Model.get_model_path('wine_model')
    model = joblib.load(model_path)

# Called when a request is received
def run(raw_data):
    # Get the input data as a numpy array
    data = np.array(json.loads(raw_data)['data'])
    # Get a prediction from the model
    predictions = model.predict(data)
    # Return the predictions as any JSON serializable format
    return predictions.tolist()
</code></pre></div></div>
<p>We also create the environment for the <code class="language-plaintext highlighter-rouge">packages</code> which are required for the inference script through the script</p>

<h1 id="create-the-inference-dependencies">Create the Inference Dependencies</h1>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from azureml.core.conda_dependencies import CondaDependencies

# Add the dependencies for your model
myenv = CondaDependencies()
myenv.add_conda_package("scikit-learn")

# Save the environment config as a .yml file
env_file = './winecode/env.yml'
with open(env_file,"w") as f:
    f.write(myenv.serialize_to_string())
print("Saved dependency info in", env_file)
</code></pre></div></div>
<p>Till this point, we have the following</p>
<ul>
  <li>Inference script</li>
  <li>Dependencies required for the inference script</li>
</ul>

<p>We need to combine this together and the <strong>InferenceConfig</strong> will help us to do this</p>

<h1 id="create-the-inference-config">Create the Inference Config</h1>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from azureml.core.model import InferenceConfig

classifier_inference_config = InferenceConfig(runtime= "python",
                                              source_directory = './winecode',
                                              entry_script="score.py",
                                              conda_file="env.yml")
</code></pre></div></div>

<h1 id="create-the-inference-clusters">Create the Inference Clusters</h1>

<p>We create the <strong>Inference Clusters</strong>, the AKS clusters required for the inference</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from azureml.core.compute import ComputeTarget, AksCompute

cluster_name = 'aks-cluster'
compute_config = AksCompute.provisioning_configuration(cluster_purpose = AksCompute.ClusterPurpose.DEV_TEST)
production_cluster = ComputeTarget.create(ws, cluster_name, compute_config)
production_cluster.wait_for_completion(show_output=True)
</code></pre></div></div>

<p>We can see the inference clusters in the workspace. I have attached my screenshot here
<img src="https://dev-to-uploads.s3.amazonaws.com/uploads/articles/eyuvjj45l0tg99bjsx8p.png" alt="image" /></p>

<h1 id="deploy-the-model-in-the-inference-cluster">Deploy the Model in the Inference Cluster</h1>

<p>We create the deployment config for the AKS Web Service</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from azureml.core.webservice import AksWebservice

classifier_deploy_config = AksWebservice.deploy_configuration(cpu_cores = 1,
                                                              memory_gb = 1)
</code></pre></div></div>
<p>The final step is to combine the</p>
<ul>
  <li>Inference config</li>
  <li>Deployment config</li>
  <li>Inference Cluster to create an endpoint which can be used for predicting new data</li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from azureml.core.model import Model

model = ws.models['wine_model']
service = Model.deploy(workspace=ws,
                       name = 'wine-service',
                       models = [model],
                       inference_config = classifier_inference_config,
                       deployment_config = classifier_deploy_config,
                       deployment_target = production_cluster)
service.wait_for_deployment(show_output = True)
</code></pre></div></div>
<p>We print the endpoint for the model</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>endpoint = service.scoring_uri
print(endpoint)
</code></pre></div></div>
<p>You can see the endpoint of the model in your workspace. Here is my screenshot</p>

<p><img src="https://dev-to-uploads.s3.amazonaws.com/uploads/articles/j1ejillo1cunfk8ffiuw.png" alt="image" /></p>

<h1 id="get-the-predictions">Get the predictions</h1>

<p>We get the <strong>Service Keys</strong> which we would require for prediction</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>primary_key, secondary_key = service.get_keys()
</code></pre></div></div>

<p>The final step is creating predictions from the endpoint which is illustrated below</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import requests
import json

# An array of new data cases
x_new = [[7.3,0.65,0,1.2,0.065,15,21,0.9946,3.39,0.47,10]]

# Convert the array to a serializable list in a JSON document
json_data = json.dumps({"data": x_new})

# Set the content type in the request headers
request_headers = { "Content-Type":"application/json",
                    "Authorization":"Bearer " + primary_key }

# Call the service
response = requests.post(url = endpoint,
                         data = json_data,
                         headers = request_headers)

print(response)
print(response.json)
</code></pre></div></div>
:ET